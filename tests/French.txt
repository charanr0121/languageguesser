INTRODUCTION


L’ebook - appelé aussi livre numérique - a tout juste quarante ans.
Après des débuts timides, il est maintenant solidement implanté à côté
du livre imprimé. On peut désormais lire un livre sur son ordinateur,
son PDA, son téléphone mobile, son smartphone ou sa tablette de
lecture.

«L’ebook a 40 ans» se présente sous la forme d’une chronologie en 60
épisodes de 1971 à 2011. Sauf mention contraire, les citations sont des
extraits des Entretiens du NEF <www.etudes-francaises.net/entretiens/>,
Université de Toronto, et des entretiens qui ont suivi pour les
compléter. Merci à tous ceux qui sont cités ici, pour leur temps et
pour leur amitié.

Une partie de ce livre a été publié dans le magazine en ligne
ActuaLitté <www.actualitte.com> en mai et juin 2011, sous la forme
d’une série d’articles, pour célébrer le 40e anniversaire du Projet
Gutenberg le 4 juillet 2011. Une autre partie de ce livre émane d’une
série d’articles en anglais publiée dans Project Gutenberg News
<www.gutenbergnews.org> en juillet 2011, ces articles ayant ensuite été
traduits en français.

Ce livre marque la fin d'un projet de recherche ayant duré douze ans,
avec une centaine de participants de par le monde.

Marie Lebert, chercheuse et journaliste, s'intéresse aux technologies
pour le livre et les langues. Ses livres sont librement disponibles
dans le Projet Gutenberg <www.gutenberg.org>, dans divers formats
permettant leur lecture sur tout appareil électronique.

Copyright © 2011 Marie Lebert



TABLE DES MATIÈRES


1971 > Le Projet Gutenberg, un projet visionnaire
1974 > Les débuts de l’internet
1986 > Des extensions pour l’ASCII
1990 > Le web booste l’internet
1991 > L’Unicode, système d’encodage universel
1992 > Des répertoires de textes électroniques
1993 > L’Online Books Page, liste de livres en ligne
1993 > Le format PDF, lancé par Adobe
1994 > L’internet comme outil de marketing
1994 > Athena, bibliothèque numérique
1995 > Éditel, éditeur littéraire né sur la toile
1995 > La presse imprimée se met en ligne
1995 > Amazon, pionnier du cybercommerce
1996 > L’Internet Archive, pour les générations futures
1996 > CyLibris, éditeur électronique
1996 > Vers un savoir numérique
1996 > Le projet @folio, baladeur de textes
1996 > Les éditions du Choucas sur la toile
1997 > La convergence multimédia
1997 > Un portail pour les bibliothèques nationales européennes
1997 > E Ink, technologie d’encre électronique
1997 > oVosite, espace d’écriture hypermédia
1997 > NON, roman multimédia
1997 > Gallica, bibliothèque numérique
1998 > Des livres numérisés en quantité
1998 > L’Encyclopédie de Diderot en ligne
1998 > 00h00, éditeur en ligne
1998 > Un prolongement sur le web pour les livres
1998 > Un durcissement du copyright
1998 > Les premières tablettes de lecture
1999 > Du bibliothécaire au cyberthécaire
1999 > La librairie Ulysse sur le web
1999 > L’internet, personnage de roman
2000 > Encyclopédies et dictionnaires en ligne
2000 > Les aventures de Stephen King
2000 > Des auteurs de best-sellers
2000 > Cotres.net, site de littérature hypermédia
2000 > Un format standard pour le livre numérique
2000 > Numilog, librairie numérique
2000 > La Bible de Gutenberg en ligne
2001 > Le web au service des auteurs
2001 > De nouveaux genres littéraires
2001 > Wikipédia, encyclopédie collaborative
2001 > D’autres tablettes de lecture
2001 > Une meilleure bande passante
2001 > Creative Commons, le copyright revisité
2003 > La Public Library of Science
2003 > Handicapzéro, l’internet pour tous
2003 > Le matériel d’enseignement du MIT
2004 > Le web 2.0, communauté et partage
2005 > Du PDA au smartphone
2005 > De Google Print à Google Books
2005 > L’Open Content Alliance, bibliothèque planétaire
2006 > Le catalogue collectif WorldCat en ligne
2007 > Quel avenir pour l’ebook?
2007 > Citizendium, encyclopédie expérimentale
2007 > L’Encyclopedia of Life, projet global
2009 > Indiscripts, laboratoire de scripts InDesign
2010 > Du Librié à l’iPad
2011 > L’ebook en dix points



1971 > LE PROJET GUTENBERG, UN PROJET VISIONNAIRE


[Résumé]
Le premier livre numérique est l’eText #1 du Projet Gutenberg, un
projet visionnaire fondé en juillet 1971 par Michael Hart pour créer
des versions électroniques gratuites d'oeuvres littéraires et les
diffuser dans le monde entier. Au 16e siècle, Gutenberg avait permis à
chacun d'avoir des livres imprimés pour un prix relativement modique.
Au 21e siècle, le Projet Gutenberg permettrait à chacun d'avoir une
bibliothèque numérique gratuite. D'abord considéré comme complètement
irréaliste, ce projet trouve un nouveau souffle et un rayonnement
international avec l'apparition du web en 1990, ce qui facilite la
circulation des livres, puis la création de Distributed Proofreaders en
2000, pour partager la relecture des livres entre des milliers de
volontaires. En juillet 2011, pour son 40e anniversaire, le Projet
Gutenberg compte 36.000 livres numériques, des dizaines de milliers de
téléchargements par jour, des sites web aux États-Unis, en Australie,
en Europe et au Canada et 40 sites miroirs répartis sur toute la
planète.

***

Le premier livre numérique est l’eText #1 du Projet Gutenberg, fondé en
juillet 1971 par Michael Hart pour créer des versions électroniques
d'oeuvres littéraires et les diffuser gratuitement dans le monde
entier.

Au 16e siècle, Gutenberg avait permis à chacun d'avoir des livres
imprimés pour un prix relativement modique. Au 21e siècle, le Projet
Gutenberg permettrait à chacun d'avoir une bibliothèque numérique
gratuite.

# Les débuts du projet

Comment le projet débute-t-il? Alors étudiant à l’Université de
l'Illinois (États-Unis), Michael Hart se voit attribuer quelques
millions de dollars de «temps machine» dans le laboratoire informatique
(Materials Research Lab) de son université.

Le 4 juillet 1971, jour de la fête nationale, il saisit «The United
States Declaration of Independence» (La Déclaration de l’indépendance
des États-Unis, signée le 4 juillet 1776) sur le clavier de son
ordinateur. En caractères majuscules, puisque les caractères minuscules
n’existent pas encore. Le texte électronique représente 5 Ko (kilo-
octets).

Michael diffuse un message à la centaine de personnes que représente le
réseau de l’époque pour indiquer où le texte est stocké - sans lien
hypertexte toutefois, puisque le web ne voit le jour que vingt ans
après - suite à quoi le fichier est téléchargé par six personnes.

Dans la foulée, Michael décide de consacrer ce crédit-temps de quelques
millions de dollars à la recherche d’oeuvres littéraires disponibles en
bibliothèque, à la numérisation de celles-ci et au stockage des textes
électroniques.

Peu après, il définit la mission du Projet Gutenberg, à savoir mettre à
la disposition de tous, par voie électronique, le plus grand nombre
possible d’oeuvres littéraires.

Ce projet trouve un rayonnement international avec l’apparition du web
en 1990, ce qui facilite la circulation des textes électroniques et les
échanges avec les volontaires.

Michael explique plus tard, en août 1998 : «Nous considérons le texte
électronique comme un nouveau médium, sans véritable relation avec le
papier. Le seul point commun est que nous diffusons les mêmes oeuvres,
mais je ne vois pas comment le papier peut concurrencer le texte
électronique une fois que les gens y sont habitués, particulièrement
dans les établissements d'enseignement.»

Au lieu d’être un ensemble de pages reliées, le livre devient un texte
électronique que l’on peut dérouler en continu, au format ASCII
(American Standard Code for Information Interchange), à savoir le
format le plus simple et le plus répandu, avec des lettres capitales
pour les termes en italique, en gras et soulignés de la version
imprimée, pour que le texte du livre puisse être lu sans problème quels
que soient la machine, la plateforme et le logiciel utilisés.

# Distributed Proofreaders

Le Projet Gutenberg trouve un nouveau souffle avec la création de
Distributed Proofreaders en 2000, pour partager la relecture des livres
entre des milliers de volontaires.

Conçu en octobre 2000 par Charles Franks pour aider à la numérisation
des livres du domaine public, Distributed Proofreaders (DP) devient
rapidement la principale source du Projet Gutenberg. Le concept est de
permettre la correction partagée de livres du domaine public scannés à
partir d'une version imprimée puis convertis au format texte par un
logiciel OCR (fiable à 99% dans le meilleur des cas, ce qui représente
donc quelques erreurs par page), en fragmentant ces livres en pages
pouvant être relues par des correcteurs différents. Les volontaires
n'ont aucun quota à respecter. À titre indicatif, il est suggéré de
relire une page par jour.

Distributed Proofreaders est officiellement affilié au Projet Gutenberg
en 2002, puis devient une entité séparée en mai 2006 tout en conservant
des liens étroits avec le projet. Distributed Proofreaders comptabilise
10.000 livres numérisés et relus par ses volontaires en décembre 2006
et 20.000 livres en avril 2011. Distributed Proofreaders Europe (DP
Europe) voit le jour début 2004, et Distributed Proofreaders Canada (DP
Canada) en décembre 2007.

# La philosophie du projet

La structure administrative et financière du Projet Gutenberg se limite
au strict minimum, avec une devise qui tient en trois mots: «Less is
more.» Le but est d’assurer la pérennité du projet indépendamment des
crédits, des coupures de crédits et des priorités culturelles,
financières et politiques du moment. Pas de pression possible donc par
le pouvoir et par l’argent. Et respect à l’égard des volontaires, qui
sont assurés de voir leur travail utilisé pendant de nombreuses années,
si ce n’est pour plusieurs générations. Le suivi régulier du projet est
assuré grâce à une lettre d’information hebdomadaire et mensuelle, des
forums de discussion, des wikis et des blogs.

En juillet 2011, pour son quarantième anniversaire, le Projet Gutenberg
compte 36.000 livres numériques, des dizaines de milliers de
téléchargements par jour, des sites web aux États-Unis, en Australie,
en Europe et au Canada et 40 sites miroirs répartis sur toute la
planète.

Quarante ans après les débuts du Projet Gutenberg, Michael Hart se
définit toujours comme un fou de travail dédiant toute sa vie à son
projet, qu’il voit comme étant à l’origine d’une révolution néo-
industrielle. Il se définit aussi comme altruiste, pragmatique et
visionnaire. Après avoir été traité de toqué pendant de nombreuses
années, il force maintenant le respect.

Michael précise souvent dans ses écrits que, si Gutenberg a permis à
chacun d'avoir ses propres livres - jusque-là réservés à une élite -
pour un coût relativement modique, le Projet Gutenberg permet à chacun
d'avoir une bibliothèque complète gratuite - jusque-là réservée à une
collectivité -, sur un support qu'on peut glisser dans sa poche (ou
porter en pendentif autour du cou). Les collections du Projet Gutenberg
ont la taille d'une bibliothèque publique de quartier, mais cette fois
disponible sur le web et téléchargeable par tous.

Au fil des ans, la mission du Projet Gutenberg reste la même, à savoir
changer le monde par le biais de l’ebook gratuit indéfiniment
reproductible, et favoriser ainsi la lecture et la culture pour tous à
moindres frais.



1974 > LES DÉBUTS DE L'INTERNET


[Résumé]
L'internet, embryonnaire en 1971, naît en 1974, quinze ans avant le
web. Vinton Cerf est souvent appelé le père de l'internet parce qu'il
est le co-auteur en 1974 avec Bob Kahn du protocole TCP/IP
(Transmission Control Protocol / Internet Protocol) nécessaire au bon
fonctionnement du réseau. L’internet est d’abord mis en place aux
États-Unis pour relier les agences gouvernementales, les universités et
les centres de recherche, avant de débuter sa progression mondiale en
1983. L’internet trouve ensuite un nouveau souffle avec l'invention du
web par Tim Berners-Lee en 1990 puis le lancement du premier navigateur
Mosaic en 1993. Vinton Cerf fonde l'Internet Society (ISOC) en 1992
pour promouvoir le développement du réseau. Il explique en janvier 1998
lors d’un entretien avec le quotidien Libération: «Le réseau fait deux
choses (...): comme les livres, il permet d'accumuler de la
connaissance. Mais, surtout, il la présente sous une forme qui la met
en relation avec d'autres informations. Alors que, dans un livre,
l'information est maintenue isolée.»

***

L'internet, embryonnaire en 1971, naît en 1974 suite à l’invention du
protocole TCP/IP (Transmission Control Protocol / Internet Protocol)
par Vinton Cerf et Bob Kahn pour les échanges de données, quinze ans
avant l’invention du web.

# Les premiers pas

Vinton Cerf est souvent appelé le père de l'internet parce qu'il est le
co-auteur en 1974 (avec Bob Kahn) du protocole TCP/IP (Transmission
Control Protocol / Internet Protocol) nécessaire au bon fonctionnement
du réseau. L’internet est d’abord mis en place aux États-Unis pour
relier les agences gouvernementales, les universités et les centre de
recherche, avant de débuter sa progression mondiale en 1983. Il trouve
ensuite un nouveau souffle avec l'invention du web par Tim Berners-Lee
en 1990 puis le lancement du premier navigateur Mosaic en 1993.

Vinton Cerf fonde l'Internet Society (ISOC) en 1992 pour promouvoir le
développement du réseau. Il explique en janvier 1998 lors d’un
entretien avec le quotidien Libération: «Le réseau fait deux choses
(...): comme les livres, il permet d'accumuler de la connaissance.
Mais, surtout, il la présente sous une forme qui la met en relation
avec d'autres informations. Alors que, dans un livre, l'information est
maintenue isolée.»

Le web étant facile d’utilisation grâce aux liens hypertextes reliant
les documents entre eux, l’internet peut enfin être utilisé par le
grand public dans les années 1990, et pas seulement par les usagers
versés dans l’informatique. On compte 100 millions d’usagers en
décembre 1997, avec un million de nouveaux usagers par mois, et 300
millions d’usagers en décembre 2000.

# La situation en Europe

En ce qui concerne la connexion à l’internet, les choses sont moins
faciles en Europe qu’en Amérique du Nord. La connexion est d'abord
tarifée à la durée, avec un tarif de jour très élevé et un tarif de
nuit plus intéressant, d’où l’obligation de travailler la nuit pour
éviter les factures trop élevées. Des mouvements de grève sont lancés
fin 1998 et début 1999 en France, en Italie et en Allemagne dans le but
de faire pression sur les sociétés prestataires pour qu'elles baissent
leurs prix et qu'elles proposent des forfaits internet, avec gain de
cause les mois suivants.

Quelques années plus tard, le haut débit se généralise. Jean-Paul,
webmestre du site hypermédia cotres.net, résume la situation en janvier
2007: «J’ai l’impression que nous vivons une période "flottante", entre
les temps héroïques, où il s’agissait d’avancer en attendant que la
technologie nous rattrape, et le futur, où le très haut débit va
libérer les forces qui commencent à bouger, pour l’instant dans les
seuls jeux.»

# L’internet du futur

L’internet du futur pourrait être un réseau pervasif permettant de se
connecter en tout lieu et à tout moment sur tout type d’appareil à
travers un réseau unique et omniprésent.

Le concept de réseau pervasif est développé par Rafi Haladjian,
fondateur de la société Ozone. Comme expliqué sur le site web en 2007,
«la nouvelle vague touchera notre monde physique, notre environnement
réel, notre vie quotidienne dans tous les instants. Nous n’accéderons
plus au réseau, nous l’habiterons. Les composantes futures de ce réseau
(parties filiaires, parties non filiaires, opérateurs) seront
transparentes à l’utilisateur final. Il sera toujours ouvert, assurant
une permanence de la connexion en tout lieu. Il sera également
agnostique en terme d’application(s), puisque fondé sur les protocoles
mêmes de l’internet.» Nous attendons cela avec impatience.

Quant au contenu de l’internet, Timothy Leary, philosophe visionnaire,
le décrit ainsi dans son livre «Chaos et cyberculture?», publié en
1994: «Toute l’information du monde est à l’intérieur. Et grâce au
cyberespace, tout le monde peut y avoir accès. Tous les signaux humains
contenus jusque-là dans les livres ont été numérisés. Ils sont
enregistrés et disponibles dans ces banques de données, sans compter
tous les tableaux, tous les films, toutes les émissions de télé, tout,
absolument tout.» En 2011, nous n’en sommes pas encore là, mais les
choses sont en bonne voie.



1986 > DES EXTENSIONS POUR L'ASCII


[Résumé]
Avec le développement de l’internet hors de la sphère anglophone,
communiquer uniquement en anglais devient insuffisant, d’où la
nécessité de prendre en compte les caractères accentués d’autres
langues européennes. Publié par l'American National Standards Institute
(ANSI) en 1963, l'ASCII (American Standard Code for Information
Interchange) est le premier système d'encodage. Il s'agit d'un code
standard de 128 caractères traduits en langage binaire sur sept bits (A
est traduit par «1000001», B est traduit par «1000010», etc.). L'ASCII
permet uniquement la lecture de l'anglais (et du latin). Des variantes
de l'ASCII sur huit bits sont publiées à partir de 1986 pour prendre en
compte les caractères accentués de quelques langues européennes. La
variante pour le français, l’espagnol et l’allemand (entre autres) est
la norme ISO 8859-1 (Latin-1). Mais les problèmes sont loin d’être
résolus. Pour cela, il faudra attendre l’Unicode, nouveau système
d’encodage universel dont la première version est publiée en janvier
1991.

***

Avec le développement de l’internet hors de la sphère anglophone,
communiquer uniquement en anglais devient insuffisant, d’où la
nécessité de prendre en compte les caractères accentués de plusieurs
langues européennes.

#  L’ASCII sur 7 bits

Le premier système d'encodage informatique est l’ASCII (American
Standard Code for Information Interchange). Publié en 1963 aux États-
Unis par l’American National Standards Institute (ANSI), l'ASCII est un
code standard de 128 caractères traduits en langage binaire sur sept
bits (A est traduit par «1000001», B est traduit par «1000010», etc.).
Les 128 caractères comprennent 33 caractères de contrôle (qui ne
représentent donc pas de symbole écrit) et 95 caractères imprimables:
les 26 lettres sans accent en majuscules (A-Z) et minuscules (a-z), les
chiffres, les signes de ponctuation et quelques caractères spéciaux, le
tout correspondant aux touches du clavier anglophone.

# L’ASCII sur 8 bits

L'ASCII permet uniquement la lecture de l’anglais (et du latin).
L’ASCII ne permet donc pas de prendre en compte les lettres accentuées
présentes dans bon nombre de langues européennes (français, espagnol,
allemand, etc.), tout comme les langues disposant d’autres alphabets
(arabe, grec, russe, etc.) et à plus forte raison les langues non
alphabétiques (chinois, coréen, japonais, etc.). Ceci ne pose pas de
problème majeur les premières années, tant que l’échange de fichiers
électroniques se limite surtout à l’Amérique du Nord. Mais le
multilinguisme devient bientôt une nécessité vitale. Des variantes de
l’ASCII sur huit bits sont publiées à partir de 1986 pour prendre en
compte les caractères accentués de quelques langues européennes. La
variante pour le français, l’espagnol et l’allemand (entre autres) est
la norme ISO 8859-1 (ISO Latin-1).

# Un casse-tête

Avec le développement de l’internet, l’échange des données
s’internationalise encore davantage. Même avec des variantes de
l’ASCII, on ne peut décidément plus se limiter à l’utilisation d’un
système d’encodage datant des débuts de l’informatique. De plus, le
passage de l’ASCII original à ses différentes variantes devient vite un
véritable casse-tête, y compris au sein de l’Union européenne, les
problèmes étant entre autres la multiplication des variantes, la
corruption des données dans les échanges informatiques ou encore
l’incompatibilité des systèmes, les pages ne pouvant être affichées que
dans une seule langue à la fois.

Olivier Gainon, fondateur de CyLibris et pionnier de l’édition
électronique littéraire, écrit à ce sujet en décembre 2000: «Il faut
que le réseau respecte les lettres accentuées, les lettres spécifiques,
etc. Je crois très important que les futurs protocoles permettent une
transmission parfaite de ces aspects - ce qui n’est pas forcément
simple (dans les futures évolutions de l’HTML ou des protocoles IP,
etc.). Donc il faut que chacun puisse se sentir à l’aise avec
l’internet et que ce ne soit pas simplement réservé à des (plus ou
moins) anglophones. Il est anormal aujourd’hui que la transmission
d’accents puisse poser problème dans les courriers électroniques. La
première démarche me semble donc une démarche technique. Si on arrive à
faire cela, le reste en découle: la représentation des langues se fera
en fonction du nombre de connectés, et il faudra envisager à terme des
moteurs de recherche multilingues.»

# L’Unicode

Publié pour la première fois en janvier 1991, l’Unicode est un système
d'encodage universel sur 16 bits spécifiant un nombre unique pour
chaque caractère. Ce nombre est lisible quels que soient la plateforme,
le logiciel et la langue utilisés. L’Unicode peut traiter 65.000
caractères uniques et prendre en compte tous les systèmes d’écriture de
la planète. L’Unicode est progressivement adopté à partir de 1998. Un
énorme travail est en effet nécessaire pour sa prise en compte par tous
les logiciels et navigateurs web. Il faudra attendre décembre 2007 pour
que l’Unicode supplante l’ASCII sur l’internet.



1990 > LE WEB BOOSTE L’INTERNET


[Résumé]
Le World Wide Web est inventé en 1990 par Tim Berners-Lee, alors
chercheur au CERN (Centre européen pour la recherche nucléaire) à
Genève, en Suisse. En 1989, il met au point l'hypertexte pour relier
des documents entre eux. En 1990, il met au point le premier serveur
HTTP (HyperText Transfer Protocol) et le premier navigateur web. En
1991, le web est opérationnel et rend l'internet (qui existe depuis
1974) accessible à tous et pas seulement aux usagers versés dans
l’informatique. Des liens hypertextes permettent désormais de passer
d'un document textuel ou visuel à un autre au moyen d'un simple clic de
souris. Plus tard, cette interactivité est encore accrue avec la
possibilité de liens hypermédias permettant de lier des textes et des
images à des vidéos ou bandes sonores. Le World Wide Web Consortium
(W3C) est fondé en octobre 1994 pour développer les protocoles communs
du web.

***

Le World Wide Web est inventé en 1990 par Tim Berners-Lee, chercheur au
CERN (Centre européen pour la recherche nucléaire) à Genève, en Suisse.
Le web rend l’internet accessible à tous et lui permet une progression
exponentielle.

# Les débuts du web

En 1989, Tim Berners-Lee met au point l’hypertexte pour relier des
documents entre eux. En 1990, il met au point le premier serveur HTTP
(HyperText Transfer Protocol) et le premier navigateur web. En 1991, le
World Wide Web est opérationnel et rend l'internet (qui existe depuis
1974) accessible à tous et pas seulement aux usagers versés dans
l’informatique. Des liens hypertextes permettent désormais de passer
d'un document textuel à un autre au moyen d'un clic de souris. Plus
tard, cette interactivité est encore accrue avec la possibilité de
liens hypermédias permettant de lier textes et images fixes à des
vidéos ou bandes sonores.

Mosaic est le premier navigateur destiné au grand public. Développé par
le NSCA (National Center for Supercomputing Applications) à
l'Université de l'Illinois (États-Unis) et distribué gratuitement en
novembre 1993, il contribue largement au développement rapide du web.
Début 1994, une partie de l'équipe de Mosaic émigre dans la Netscape
Communications Corporation pour développer un nouveau logiciel sous le
nom de Netscape Navigator. En 1995, Microsoft lance son propre
navigateur, l'Internet Explorer. Viennent ensuite d'autres navigateurs,
comme Opera ou Safari, le navigateur d'Apple.

Un consortium industriel international est fondé en octobre 1994 pour
développer les protocoles communs du web, sous le nom de World Wide
Consortium (W3C) et sous l’égide de Tim Berners-Lee. En 1997, une
section Internationalization / Localization regroupe les protocoles
utilisés pour créer un site web multilingue: HTML (HyperText Markup
Language), jeux (de base) de caractères, nouveaux attributs, HTTP
(HyperText Transfer Protocol), négociation de la langue, URL (Uniform
Resource Locator) et autres identificateurs incluant des caractères non
ASCII, conseils divers.

# Le rêve de Tim Berners-Lee

À la question de Pierre Ruetschi, journaliste à la Tribune de Genève,
quotidien suisse: «Sept ans plus tard, êtes-vous satisfait de la façon
dont le web a évolué?», Tim Berners-Lee répond en décembre 1997 que,
s’il est heureux de la richesse et de la variété de l’information
disponible, le web n’a pas encore la puissance prévue dans sa
conception d’origine. Il aimerait «que le web soit plus interactif, que
les gens puissent créer de l’information ensemble», et pas seulement
consommer celle qui leur est proposée. Le web doit devenir «un média de
collaboration, un monde de connaissance que nous partageons».

Dans un essai publié en avril 1998 sur sa propre page web (sur le site
du World Wide Web Consortium), Tim Berners-Lee explique que «le rêve
derrière le web est un espace d'information commun dans lequel nous
communiquons en partageant l'information. Son universalité est
essentielle, à savoir le fait qu'un lien hypertexte puisse pointer sur
quoi que ce soit, quelque chose de personnel, de local ou de global,
aussi bien une ébauche qu'une réalisation très sophistiquée. Deuxième
partie de ce rêve, le web deviendrait d'une utilisation tellement
courante qu'il serait un miroir réaliste (sinon la principale
incarnation) de la manière dont nous travaillons, jouons et nouons des
relations sociales. Une fois que ces interactions seraient en ligne,
nous pourrions utiliser nos ordinateurs pour nous aider à les analyser,
donner un sens à ce que nous faisons, et voir comment chacun trouve sa
place et comment nous pouvons mieux travailler ensemble.» (extrait de
«The World Wide Web: a very short personal history»)

# Le web 2.0

Selon Netcraft, société spécialisée dans les mesures d’audience, le
nombre de sites web passe d’un million de sites en avril 1997 à dix
millions de sites en février 2000, 20 millions de sites en septembre
2000, 30 millions de sites en juillet 2001, 40 millions de sites en
avril 2003, 50 millions de sites en mai 2004, 60 millions de sites en
mars 2005, 70 millions de sites en août 2005, 80 millions de sites en
avril 2006, 90 millions de sites en août 2006 et 100 millions de sites
en novembre 2006, une augmentation rapide qui s’explique par
l’explosion des sites personnels et des blogs.

Le web 2.0, termé lancé en 2004 par Tim O’Reilly, éditeur de livres
informatiques, apporte peut-être un début de réponse au rêve de Tim
Berners-Lee puisqu’il est basé sur les notions de communauté et de
partage.

Quinze ans après la création du web, le magazine Wired constate dans
son numéro d'août 2005 que «moins de la moitié du web est commercial,
le reste fonctionne avec la passion». Quant à l'internet, d'après le
quotidien Le Monde du 19 août 2005, «ses trois pouvoirs - l'ubiquité,
la variété et l'interactivité - rendent son potentiel d'usages quasi
infini».

Robert Beard, professeur de langues et créateur du site A Web of Online
Dictionaries en 1995, écrivait de manière prémonitoire dès septembre
1998: «Le web sera une encyclopédie du monde faite par le monde pour le
monde. Il n'y aura plus d'informations ni de connaissances utiles qui
ne soient pas disponibles, si bien que l'obstacle principal à la
compréhension internationale et interpersonnelle et au développement
personnel et institutionnel sera levé. Il faudrait une imagination plus
débordante que la mienne pour prédire l'effet de ce développement sur
l'humanité.»



1991 > L'UNICODE, SYSTÈME D'ENCODAGE UNIVERSEL


[Résumé]
L’ASCII, premier système d’encodage datant des débuts de
l’informatique, n’est plus suffisant avec l’internationalisation de
l’internet, d’où l’intérêt de l’Unicode, nouveau système d’encodage
universel, dont la première version est publiée en janvier 1991.
L'Unicode spécifie un nombre sur 16 bits unique à chaque caractère (ou
idéogramme) et lisible quels que soient la plateforme, le logiciel et
la langue utilisés. L'Unicode peut traiter 65.000 caractères et prendre
en compte tous les systèmes d'écriture de la planète. Il devient une
composante des spécifications du World Wide Web Consortium (W3C),
l'organisme international chargé du développement du web. L’utilisation
de l’Unicode se généralise à partir de 1998, par exemple pour les
fichiers texte sous plateforme Windows (Windows NT, Windows 2000,
Windows XP et versions suivantes), qui étaient jusque-là en ASCII.
L’Unicode supplante définitivement l’ASCII en décembre 2007.

***

L’ASCII n’est plus suffisant avec l’internationalisation de l’internet,
d’où l’intérêt de l’Unicode, nouveau système d’encodage universel, dont
la première version est publiée en janvier 1991.

Contrairement à l’ASCII conçu pour l’anglais (et le latin), avec des
variantes pour quelques langues supplémentaires, l’Unicode prend en
compte toutes les langues de la planète.

# De l’ASCII à l’Unicode

Pour mémoire, le premier système d'encodage informatique est l’ASCII
(American Standard Code for Information Interchange), publié en 1963
aux États-Unis par l’American National Standards Institute (ANSI) pour
encoder des informations en anglais.

Mais le multilinguisme devient bientôt une nécessité vitale. Des
variantes de l’ASCII prennent en compte d’autres langues à partir de
1986. Avec le développement de l’internet, l’échange des données
s’internationalise de plus en plus, si bien qu’il n’est plus possible
de se limiter à un système d’encodage datant des débuts de
l’informatique, même avec ses variantes.

Publié pour la première fois en janvier 1991, l’Unicode est un système
d'encodage universel sur 16 bits spécifiant un nombre unique pour
chaque caractère (ou idéogramme). Ce nombre est lisible quels que
soient la plateforme, le logiciel et la langue utilisés. L’Unicode peut
traiter 65.000 caractères uniques et prendre en compte tous les
systèmes d’écriture de la planète. À la grande satisfaction des
linguistes, il remplace progressivement l’ASCII, avec des variantes
UTF-8, UTF-16 et UTF-32 (UTF: Unicode Transformation Format) selon le
nombre de bits utilisés pour l’encodage.

L'Unicode est maintenu par l'Unicode Consortium. Il devient une
composante des spécifications du World Wide Web Consortium (W3C), fondé
en octobre 1994 pour promouvoir le développement du web. L’utilisation
de l’Unicode se généralise à partir de 1998, par exemple pour les
fichiers texte sous plateforme Windows (Windows NT, Windows 2000,
Windows XP et versions suivantes), qui étaient jusque-là en ASCII.

# Une tâche énorme

Mais la tâche s’annonce rude. Patrick Rebollar, professeur de français
et de littérature française au Japon et modérateur de la liste de
diffusion LITOR (Littérature et ordinateur), précise en janvier 2000:
«Il s'agit d'abord d'un problème logiciel. Comme on le voit avec
Netscape ou Internet Explorer, la possibilité d'affichage multilingue
existe. La compatibilité entre ces logiciels et les autres (de la suite
Office de Microsoft, par exemple) n'est cependant pas acquise.
L'adoption de la table Unicode devrait résoudre une grande partie des
problèmes, mais il faut pour cela réécrire la plupart des logiciels, ce
à quoi les producteurs de logiciels rechignent du fait de la dépense,
pour une rentabilité qui n'est pas évidente car ces logiciels
entièrement multilingues intéressent moins de clients que les logiciels
de navigation.»

Luc Dall’Armellina, co-auteur et webmestre d’oVosite, un espace
d’écriture hypermédia, écrit en juin 2000: «Les systèmes d’exploitation
se dotent peu à peu des kits de langues et bientôt peut-être de polices
de caractères Unicode à même de représenter toutes les langues du
monde; reste que chaque application, du traitement de texte au
navigateur web, emboîte ce pas. Les difficultés sont immenses: notre
clavier avec ses ± 250 touches avoue ses manques dès lors qu’il faille
saisir des Katakana ou Hiragana japonais, pire encore avec la langue
chinoise. La grande variété des systèmes d’écriture de par le monde et
le nombre de leurs signes font barrage. Mais les écueils culturels ne
sont pas moins importants, liés aux codes et modalités de
représentation propres à chaque culture ou ethnie.» Un sentiment
prémonitoire puisque l’Unicode ne supplantera l’ASCII qu’en décembre
2007.



1992 > DES RÉPERTOIRES DE TEXTES ÉLECTRONIQUES


[Résumé]
Les premiers textes électroniques sont recensés dans les Etext
Archives, répertoire créé en 1992 par Paul Southworth, et dans l’E-
Zine-List, liste créée en 1993 par John Labovitz. Les premiers titres
purement électroniques sont des textes courts de tous ordres, souvent
politiques au début, auxquels succèdent les e-zines (electronic zines),
rédigés par une personne ou un petit groupe sur des sujets souvent
culturels, sans publicité ni profit commercial. Les Etext Archives sont
créées en 1992 par Paul Southworth, et hébergées sur le site web de
l’Université du Michigan (États-Unis). Elles sont “un lieu d’accueil
pour les textes électroniques de tout genre”, sans juger de leur
contenu. L’E-Zine-List est créée en été 1993 par John Labovitz. En cinq
ans, de 1993 à 1998, les quelques dizaines d'e-zines deviennent
plusieurs centaines (3.045 e-zines recensés en novembre 1998). Le champ
de l’e-zine s'élargit pour recouvrir tout type de publication publiée
par voie électronique.

***

Les premiers textes électroniques sont recensés dans les Etext
Archives, répertoire créé en 1992 par Paul Southworth, et dans l’E-
Zine-List, liste créée en 1993 par John Labovitz.

Les premiers titres purement électroniques sont des textes courts de
tous ordres, souvent politiques au début. Viennent ensuite les e-zines
(zines électroniques), rédigés par une personne ou un petit groupe sur
des sujets souvent culturels.

Qu’est-ce exactement qu’un zine? John explique sur le site: «Pour ceux
d’entre vous qui ne connaissent pas le monde du zine, “zine" est
l’abrégé de "fanzine" ou "magazine" selon votre point de vue. Les zines
sont en général l’oeuvre d’une personne ou d’un petit groupe, souvent
rédigée pour le plaisir ou pour des raisons personnelles, et sont le
plus souvent irrévérencieux, bizarres et/ou ésotériques. Les zines ne
sont pas des publications grand public - le plus souvent ils ne
contiennent pas de publicité (sauf parfois des publicités pour d’autres
zines), ils ne sont pas dirigés vers une audience de masse et ils ne
visent pas un profit commercial. Un "e-zine" est un zine qui est
distribué en partie ou uniquement sur des réseaux électroniques tels
que l’internet.»

# Les Etext Archives

Les Etext Archives sont créées en 1992 par Paul Southworth, et
hébergées par le site web de l’Université du Michigan (États-Unis).
Elles sont «un lieu d’accueil pour les textes électroniques de tout
genre, du sacré au profane, et du politique au personnel», sans juger
de leur contenu.

Cinq ans plus tard, elles comportent six sections: (a) une section «E-
zines», qui regroupe des textes électroniques périodiques qui vont du
professionnel au personnel; (b) une section «Politics», qui regroupe
des zines politiques, des essais et des pages de groupes politiques;
(c) une section «Fiction», qui regroupe des publications d’auteurs
amateurs; (d) une section «Religion», qui regroupe des textes religieux
grand public ou non; (e) une section «Poetry», qui est un mélange
éclectique de poésie surtout amateur; et enfin (f) une section
«Quartz», qui comprend les archives auparavant hébergées à
quartz.rutgers.edu.

Comme indiqué à l’époque sur le site, “le web venait de débuter [en
1992], le gopher était la nouvelle technologie de pointe et le FTP
était encore le protocole standard d’extraction de l’information pour
la grande majorité des utilisateurs. L’origine du projet a incité de
nombreuses personnes à l’associer avec l’Université du Michigan, bien
qu’il n’ait existé aucune relation officielle et que le projet soit
entièrement le fait du travail des volontaires et de dons personnels.
Le matériel est la propriété exclusive des responsables du projet. Le
projet a été lancé en réponse à l’absence d’archivage organisé de
documents politiques, de périodiques et de discussions diffusées par le
biais de Usenet sur des newsgroups tels que alt.activism,
misc.activism.progressive et alt.society.anarchy. Le groupe
alt.politics.radical-left a rejoint le projet plus tard et il était
aussi une source importante de documents et de contributeurs réguliers.
Peu de temps après, les zines électroniques (e-zines) ont débuté leur
prolifération rapide sur l’internet, et il était clair que ces
publications souffraient de la même absence de collecte coordonnée et
de préservation, sans parler du fait que la frontière était floue entre
les e-zines (qui à l’époque était surtout liés au hacking, au phreaking
et à l’anarchisme internet) et les documents politiques présents sur
l’internet, si bien que la plupart des e-zines étaient en phase avec
l’objectif original des Etext Archives. Une chose en amenant une autre,
des e-zines de toutes sortes - dont de nombreux titres sur divers
sujets culturels non liés à la politique - ont fini par envahir nos
archives en volume significatif.”

# L’E-Zine-List

L’E-Zine-List est créée en été 1993 par John Labovitz pour recenser les
e-zines circulant dans le monde entier et accessibles par FTP (File
Transfer Protocol), gopher (système d’information à base de menus
textuels à plusieurs niveaux), courriel, le web ou d’autres services.
La liste est actualisée une fois par mois.

Comment l’E-Zine-List débute-t-elle? Dans l’historique présent sur le
site, John relate qu’à l’origine son intention est de faire connaître
Crash, un zine imprimé dont il souhaite proposer une version
électronique. À la recherche de répertoires, il ne trouve que le groupe
de discussion alt.zines et des archives comme The Well et les Etext
Archives. Lui vient alors l’idée d’un répertoire organisé. Il débute
avec douze titres classés manuellement sur un traitement de texte. Puis
il écrit sa propre base de données.

En cinq ans, de 1993 à 1998, les quelques dizaines d'e-zines deviennent
plusieurs centaines, et la signification même d’e-zine s’élargit pour
recouvrir tout type de publication publiée par voie électronique, même
si, selon John, «il subsiste toujours un groupe original et indépendant
désormais minoritaire qui continue de publier suivant son coeur ou de
repousser les frontières de ce que nous appelons un zine.» L’E-Zine-
List recense 3.045 titres en novembre 1998. John poursuit encore la
liste pendant quelques années avant de passer le relais à d’autres.



1993 > L'ONLINE BOOKS PAGE, LISTE DE LIVRES EN LIGNE


[Résumé]
Alors que certains numérisent les oeuvres littéraires du domaine
public, comme le Projet Gutenberg et des projets connexes, d'autres se
donnent pour tâche de répertorier celles qui sont en accès libre sur le
web, en offrant au lecteur un point d’accès commun. C’est le cas de
John Mark Ockerbloom, doctorant à l’Université Carnegie Mellon
(Pittsburgh, Pennsylvanie, États-Unis), qui crée l’Online Books Page en
janvier 1993 afin de recenser les livres anglophones du domaine public
en accès libre sur le web. En 1999, il rejoint l’Université de
Pennsylvanie pour travailler à la R&D (recherche et développement) de
la bibliothèque numérique. À la même époque, il y transfère l'Online
Books Page tout en gardant la même présentation, très sobre, et tout en
poursuivant son travail d’inventaire dans le même esprit. Ce répertoire
recense 12.000 titres en 1999, 20.000 titres en 2003 (dont 4.000 titres
publiés par des femmes), 25.000 titres en 2006, 30.000 titres en 2007
(dont 7.000 titres du Projet Gutenberg) et 35.000 titres en 2010.

***

L’Online Books Page est une page web créée en janvier 1993 pour
recenser les livres anglophones du domaine public en accès libre sur le
web.

Alors que certains numérisent les oeuvres littéraires du domaine
public, comme le Projet Gutenberg et des projets connexes, d'autres se
donnent pour tâche de répertorier celles qui sont en accès libre sur le
web, en offrant au lecteur un point d’accès commun. C’est le cas de
John Mark Ockerbloom, doctorant à l’Université Carnegie Mellon
(Pittsburgh, Pennsylvanie, États-Unis), auteur de l’Online Books Page.

Cinq ans plus tard, en septembre 1998, John Mark relate: «J’étais
webmestre ici pour la section informatique de la CMU [Carnegie Mellon
University], et j’ai débuté notre site local en 1993. Il comprenait des
pages avec des liens vers des ressources disponibles localement, et à
l’origine l’Online Books Page était l’une de ces pages, avec des liens
vers des livres mis en ligne par des collègues de notre département
(par exemple Robert Stockton, qui a fait des versions web de certains
textes du Projet Gutenberg). Ensuite les gens ont commencé à demander
des liens vers des livres disponibles sur d’autres sites. J’ai remarqué
que de nombreux sites (et pas seulement le Projet Gutenberg ou Wiretap)
proposaient des livres en ligne, et qu’il serait utile d’en avoir une
liste complète qui permette de télécharger ou de lire des livres où
qu’ils soient sur l’internet. C’est ainsi que mon index a débuté.

J’ai quitté mes fonctions de webmestre en 1996, mais j’ai gardé la
gestion de l’Online Books Page, parce qu’entre temps je m’étais
passionné pour l’énorme potentiel qu’a l’internet de rendre la
littérature accessible au plus grand nombre. Maintenant il y a tant de
livres mis en ligne que j’ai du mal à rester à jour. Je pense pourtant
poursuivre cette activité d’une manière ou d’une autre. Je suis très
intéressé par le développement de l’internet en tant que médium de
communication de masse dans les prochaines années. J’aimerais aussi
rester impliqué dans la mise à disposition gratuite de livres sur
l’internet, que ceci fasse partie intégrante de mon activité
professionnelle, ou que ceci soit une activité bénévole menée sur mon
temps libre.»

En 1998, un index de 7.000 livres en ligne est disponible par auteur,
par titre et par sujet. On trouve aussi une liste de répertoires et
d’archives de textes en ligne, tout comme une liste de répertoires de
publications périodiques (magazines, journaux, revues, périodiques
scientifiques).

Fin 1998, John Mark obtient son doctorat en informatique. En 1999, il
rejoint l’Université de Pennsylvanie, où il travaille à la R&D
(recherche et développement) de la bibliothèque numérique. À la même
époque, il y transfère l’Online Books Page tout en gardant la même
présentation, très sobre, et tout en poursuivant son travail
d’inventaire dans le même esprit. Ce répertoire recense 12.000 titres
en 1999, 20.000 titres en 2003 (dont 4.000 titres publiés par des
femmes), 25.000 titres en 2006, 30.000 titres en 2007 (dont 7.000
titres du Projet Gutenberg) et 35.000 titres en 2010.



1993 >  LE FORMAT PDF, LANCÉ PAR ADOBE


[Résumé]
La société Adobe lance en juin 1993 le format PDF (Portable Document
Format), l’Acrobat Reader (gratuit, pour lire les PDF) et l’Adobe
Acrobat (payant, pour créer des PDF). Le but du format PDF est de figer
les documents numériques dans une présentation donnée, pour conserver
la présentation originale du document source, quelle que soit la
plateforme utilisée pour le créer et pour le lire. Au fil des ans, le
format PDF devient un standard de diffusion des documents
électroniques. L'Acrobat Reader est progressivement disponible dans
plusieurs langues, pour diverses plateformes (Windows, Mac, Linux) et
pour divers supports (ordinateur, PDA, smartphone). En mai 2003,
l'Acrobat Reader (5e version) fusionne avec l'Acrobat eBook Reader (2e
version) pour devenir l'Adobe Reader, qui débute à la version 6 et qui
permet de lire aussi bien les fichiers PDF standard que les fichiers
PDF sécurisés des livres numériques sous droits. De format
propriétaire, le format PDF devient un standard ouvert en juillet 2008,
tout comme une norme ISO (ISO 32000-1:2008).

***

De la côte californienne, la société Adobe lance en juin 1993 le format
PDF (Portable Document Format), tout comme l’Acrobat Reader (gratuit,
pour lire les PDF) et l’Adobe Acrobat (payant, pour créer des PDF).

# Un standard de diffusion

Le but du format PDF est de figer les documents numériques dans une
présentation donnée, pour conserver la présentation originale du
document source, quelle que soit la plateforme utilisée pour le créer
et pour le lire. Au fil des ans, le format PDF devient un standard de
diffusion des documents électroniques. L’Acrobat Reader est
progressivement disponible dans plusieurs langues et pour diverses
plateformes (Windows, Mac, Linux).

Adobe annonce en août 2000 l’acquisition de la société Glassbook,
spécialisée dans les logiciels de distribution de livres numériques à
l'intention des éditeurs, libraires, diffuseurs et bibliothèques. À la
même date, Adobe passe un partenariat avec les grandes librairies en
ligne Amazon.com et Barnes  & Noble.com pour que celles-ci proposent
des titres lisibles sur l’Acrobat Reader et le Glassbook Reader dans
leur eBookStore, lancé en août 2000 par Barnes & Noble et en novembre
2000 par Amazon.

# Deux nouveaux logiciels

En janvier 2001, Adobe lance deux nouveaux logiciels.

Le premier logiciel, gratuit, est l’Acrobat eBook Reader, qui permet de
lire les fichiers PDF de livres numériques sous droits, avec gestion
des droits par l’Adobe Content Server. Le logiciel permet d’ajouter des
notes et des signets, de choisir l’orientation de lecture des livres
(paysage ou portrait), ou encore de visualiser leur couverture dans une
bibliothèque personnelle. Il utilise la technique d’affichage CoolType
et comporte un dictionnaire intégré.

Le deuxième logiciel, payant, est l’Adobe Content Server, destiné aux
éditeurs et distributeurs. Il s’agit d’un logiciel serveur de contenu
assurant le conditionnement, la protection, la distribution et la vente
sécurisée de livres numériques au format PDF. Ce système de gestion des
droits numériques (ou système DRM: Digital Rights Management) permet de
contrôler l’accès aux livres numériques sous droits, et donc de gérer
les droits d’un livre selon les consignes données par le gestionnaire
des droits, qui est souvent l’éditeur, par exemple en autorisant ou non
l’impression ou le prêt. L’Adobe Content Server sera remplacé par
l’Adobe LiveCycle Policy Server en novembre 2004.

En avril 2001, Adobe passe un deuxième partenariat avec Amazon, qui met
en vente 2.000 livres numériques lisibles sur l’Acrobat eBook Reader:
titres de grands éditeurs, guides de voyages, livres pour enfants, etc.

L'Acrobat Reader s'enrichit d'une version PDA, disponible pour le Palm
Pilot en mai 2001 puis pour le Pocket PC en décembre 2001, puisque le
public commence à lire sur PDA, suscitant l’inquiétude de certains
professionnels du livre (et de certains ophtalmologues) qui trouvent
que l’écran est vraiment trop petit, alors que les adeptes de la
lecture sur PDA tentent de les convaincre du contraire.

# L’Adobe Reader

En dix ans, entre 1993 et 2003, l’Acrobat Reader aurait été téléchargé
500 millions de fois. En 2003, ce logiciel est désormais disponible
dans de nombreuses langues et pour toute plateforme (Windows, Mac,
Linux, Palm OS, Pocket PC, Symbian OS, etc.). 10% des documents
présents sur l'internet seraient au format PDF. Des millions de
fichiers PDF sont présents sur le web pour lecture et téléchargement ou
bien transitent par courriel. Le format PDF est également le format de
livre numérique le plus répandu.

En mai 2003, l’Acrobat Reader (version 5) fusionne avec l’Acrobat eBook
Reader (version 2) pour devenir l’Adobe Reader, qui débute à la version
6 et permet de lire aussi bien les fichiers PDF standard que les
fichiers PDF sécurisés des livres numériques sous droits.

Fin 2003, Adobe ouvre sa librairie en ligne, le Digital Media Store,
avec les titres au format PDF de grands éditeurs tels que HarperCollins
Publishers, Random House et Simon & Schuster, ainsi que les versions
électroniques de journaux et magazines comme le New York Times et
Popular Science. Adobe lance aussi Adobe eBooks Central, un service
permettant de lire, publier, vendre et prêter des livres numériques, et
l’Adobe eBook Library, qui se veut un prototype de bibliothèque de
livres numériques.

Après avoir été un format propriétaire, le format PDF devient un
standard ouvert en juillet 2008. Il est publié en tant que norme ISO
(Organisation internationale de normalisation) sous l’appellation ISO
32000-1:2008.



1994 > L’INTERNET COMME OUTIL DE MARKETING


[Résumé]
Aussi bizarre que cela puisse paraître, des livres numériques en accès
libre favorisent la vente des mêmes livres imprimés. La National
Academy Press (NAP) décide en 1994 de mettre en accès libre sur le web
le texte intégral de plusieurs centaines de livres, avec l'accord de
leurs auteurs, afin que les lecteurs puissent les «feuilleter» à
l'écran, comme ils l'auraient fait dans une librairie. L'éditeur
utilise l’internet comme nouvel outil de marketing, avec un pari gagné,
puisque la présence de ces livres sur le web entraîne une augmentation
de la vente des mêmes livres imprimés. La solution choisie par la NAP
est adoptée dès 1995 par la MIT Press (MIT: Massachusetts Institute of
Technology), avec un succès similaire. Les autres maisons d'édition
hésitent à se lancer dans l'aventure, pour trois raisons: le coût
excessif qu'entraîne la mise en ligne de milliers de pages, les
problèmes liés au droit d'auteur, et enfin la peur d'une «concurrence»
entre les versions numériques gratuites et les versions imprimées
payantes, qu'ils estiment nuisible aux ventes.

***

Aussi bizarre que cela puisse paraître, des livres numériques en accès
libre favorisent la vente des mêmes livres imprimés. La National
Academy Press (NAP) est la première à tenter l’expérience, dès 1994,
avec un pari gagné.

La publication en ligne d’un livre à titre gratuit nuit-elle aux ventes
de la version imprimée ou non? «À première vue, cela paraît illogique»,
écrit Beth Berselli, journaliste au Washington Post, dans un article
repris par le Courrier international de novembre 1997. «Un éditeur de
Washington, la National Academy Press (NAP), qui a publié sur internet
700 titres de son catalogue actuel, permettant ainsi à tout un chacun
de lire gratuitement ses livres, a vu ses ventes augmenter de 17%
l’année suivante. Qui a dit que personne n’achèterait la vache si on
pouvait avoir le lait gratuitement?»

# La politique atypique de la NAP

Une politique atypique porte donc ses fruits. Éditeur universitaire, la
National Academy Press (qui devient ensuite la National Academies
Press) publie environ 200 livres par an, surtout des ouvrages
scientifiques et techniques et des ouvrages médicaux. En 1994,
l'éditeur choisit de mettre en accès libre sur le web le texte intégral
de plusieurs centaines de livres, afin que les lecteurs puissent les
«feuilleter» à l’écran, comme ils l’auraient fait dans une librairie,
avant de les acheter ensuite si utile.

Ce sont les auteurs eux-mêmes qui, pour mieux faire connaître leurs
livres, demandent que ceux-ci soient mis en ligne sur le site, avec
succès, puisque les ventes augmentent pour leurs correspondants
imprimés.

Pour l’éditeur, l’internet est un nouvel outil de marketing face aux
50.000 ouvrages publiés chaque année aux États-Unis. Une réduction de
20% est accordée pour toute commande effectuée en ligne. La présence de
ces livres sur le web entraîne aussi une augmentation des ventes par
téléphone. En 1998, le site de la NAP propose le texte intégral d’un
millier de titres.

# La MIT Press lui emboîte le pas

La solution choisie par la NAP est adoptée en 1995 par la MIT Press
(MIT: Massachusetts Institute of Technology). À cette date, la MIT
Press publie 200 livres par an et 40 périodiques, dans divers domaines:
sciences et technologies, architecture, sciences sociales, économie,
sciences cognitives et informatique. Nombre de livres sont mis en ligne
gratuitement sur le site, afin de marquer «un engagement à long terme
pour une utilisation efficace et créative des nouvelles technologies».
La MIT Press voit rapidement les ventes de livres imprimés augmenter
pour les titres disponibles gratuitement en version intégrale sur le
web.

Ces initiatives sont saluées par d'autres maisons d'édition, qui
hésitent cependant à se lancer dans l'aventure, pour trois raisons: le
coût excessif qu'entraîne la mise en ligne de milliers de pages, les
problèmes liés au droit d'auteur, et enfin la peur d'une «concurrence»
entre les versions numériques gratuites et les versions imprimées
payantes, concurrence qu'ils estiment nuisible aux ventes, même si les
expériences menées par la NAP et la MIT Press démontrent le contraire.



1994 > ATHENA, BIBLIOTHÈQUE NUMÉRIQUE


[Résumé]
Les premières bibliothèques numériques francophones débutent avec la
saisie patiente de livres imprimés ligne après ligne sur le clavier
d’un ordinateur. C’est le cas d’Athena (Genève), précédée par ABU
(Paris) et suivie de la Bibliothèque électronique de Lisieux
(Normandie), entre autres. ABU: la bibliothèque universelle (ABU:
Association des bibliophiles universels) voit le jour en avril 1993, à
l'initiative de l'association du même nom. Ses membres  bénévoles
dactylographient eux-mêmes des oeuvres francophones du domaine public
ou bien les scannent. Athena est une bibliothèque numérique à la fois
francophone et multilingue créée en 1994 par Pierre Perroud, professeur
à Genève. La Bibliothèque électronique de Lisieux est créée en juin
1996 par Olivier Bogros, directeur de la médiathèque municipale de
Lisieux, avec 370 textes courts numérisés en juillet 1999.

***

Les premières bibliothèques numériques francophones débutent avec la
saisie patiente de livres imprimés ligne après ligne sur le clavier
d’un ordinateur.

C’est le cas d’Athena (Genève), précédée par ABU (Paris) et suivie de
la Bibliothèque électronique de Lisieux (Normandie), entre autres.

# ABU: la bibliothèque universelle

La toute première bibliothèque numérique française à voir le jour est
ABU: la bibliothèque universelle. Elle est créée en juin 1993 à
l'initiative de l’Association des bibliophiles universels (ABU) et
hébergée sur le site du CNAM (Conservatoire national des arts et
métiers) à Paris. Ses membres bénévoles dactylographient eux-mêmes des
oeuvres francophones du domaine public ou bien les scannent. En janvier
2002, les collections comprennent 288 textes de 101 auteurs. Il ne
semble pas que d'autres textes aient été ajoutés depuis.

# Athena, francophone et plurilingue

Athena est une bibliothèque numérique fondée en 1994 par Pierre
Perroud, professeur au collège Voltaire à Genève (Suisse), et hébergée
sur le site de l'Université de Genève. Elle propose à la fois des
oeuvres numérisées par Athena (200 oeuvres depuis 1994) et des liens
vers des oeuvres en accès libre sur le web.

En 1997, le site bilingue français-anglais donne accès à 3.500 textes
électroniques dans des domaines aussi variés que la philosophie, les
sciences, la période classique, la littérature, l'histoire, l'économie,
etc. En décembre 1998, la bibliothèque offre des liens vers 8.000
textes électroniques en plusieurs langues.

Un des objectifs d'Athena est de mettre en ligne des textes de langue
française (French Authors and Texts) puisque Genève est la capitale de
la Suisse francophone. Une section spécifique regroupe les auteurs et
textes suisses (Swiss Authors and Texts). On trouve aussi un répertoire
mondial de ressources littéraires en ligne (Athena Literature
Resources). Par ailleurs, Athena propose une table de minéralogie qui
est l'oeuvre de Pierre Perroud et qui est consultée dans le monde
entier.

Dans un article de la revue Informatique-Informations (Genève) daté de
février 1997, Pierre Perroud insiste sur la complémentarité du texte
électronique et du livre imprimé. Selon lui, «les textes électroniques
représentent un encouragement à la lecture et une participation
conviviale à la diffusion de la culture», notamment pour l’étude de ces
textes et la recherche textuelle. Ces textes électroniques «sont un bon
complément du livre imprimé - celui-ci restant irremplaçable lorsqu’il
s’agit de lire.»  Mais le livre imprimé reste «un compagnon
mystérieusement sacré vers lequel convergent de profonds symboles: on
le serre dans la main, on le porte contre soi, on le regarde avec
admiration; sa petitesse nous rassure autant que son contenu nous
impressionne; sa fragilité renferme une densité qui nous fascine; comme
l’homme il craint l’eau et le feu, mais il a le pouvoir de mettre la
pensée de celui-là à l’abri du Temps.»

# La Bibliothèque électronique de Lisieux

La Bibliothèque électronique de Lisieux est créée en juin 1996 par
Olivier Bogros, directeur de la médiathèque municipale de Lisieux
(Normandie), qui l'héberge pendant deux ans sur les pages de son compte
personnel CompuServe avant d'enregistrer un nom de domaine en juin
1998.

En juillet 1999, la bibliothèque électronique comprend 370 textes
courts, numérisés en mode texte à partir des collections de la
médiathèque. On y trouve des oeuvres littéraires, des brochures et des
opuscules documentaires, ainsi que des manuscrits, livres et brochures
sur la Normandie.

Lancé en août 2000, LexoTor est une base de données fonctionnant avec
le logiciel TACTweb (TACT: Text Analysis Computing Tools) et permettant
l'interrogation en ligne des oeuvres de la bibliothèque, ainsi que des
analyses et comparaisons textuelles. Les collections comprennent 930
oeuvres et 20 galeries d'images en décembre 2006.



1995 > ÉDITEL, ÉDITEUR LITTÉRAIRE NÉ SUR LA TOILE


[Résumé]
Éditel est le premier éditeur électronique francophone non commercial.
Le site est lancé en avril 1995 sous la houlette de Pierre François
Gagnon, poète et essayiste québécois. Pierre François relate en juillet
2000: «En fait, tout le monde et son père savent ou devraient savoir
que le premier site d’édition en ligne commercial fut CyLibris [créé en
août 1996 à Paris par Olivier Gainon, ndlr], précédé de loin lui-même,
au printemps de 1995, par nul autre qu’Éditel, le pionnier d’entre les
pionniers du domaine, bien que nous fûmes confinés à l’action
symbolique collective, faute d’avoir les moyens de déboucher jusqu’ici
sur une formule de commerce en ligne vraiment viable et abordable.»
D’abord site pionnier de l'édition littéraire francophone, puis premier
site web d'auto-édition collective de langue française, Éditel devient
au fil des ans un site de cyberédition non commerciale, en partenariat
avec quelques auteurs «maison», ainsi qu'un webzine littéraire. Un blog
lui succède quelques années plus tard.

***

Éditel, premier éditeur électronique francophone non commercial,
apparaît sur la toile en avril 1995 sous la houlette de Pierre François
Gagnon, poète et essayiste québécois.

Pierre François décide d’utiliser le numérique pour la réception des
textes, leur archivage et leur diffusion. Il relate en juillet 2000:
«En fait, tout le monde et son père savent ou devraient savoir que le
premier site d’édition en ligne commercial fut CyLibris [créé en août
1996 à Paris par Olivier Gainon, ndlr], précédé de loin lui-même, au
printemps de 1995, par nul autre qu’Éditel, le pionnier d’entre les
pionniers du domaine, bien que nous fûmes confinés à l’action
symbolique collective, faute d’avoir les moyens de déboucher jusqu’ici
sur une formule de commerce en ligne vraiment viable et abordable
(...). Nous sommes actuellement trois mousquetaires [Pierre François
Gagnon, Jacques Massacrier et Mostafa Benhamza, ndlr] à développer le
contenu original et inédit du webzine littéraire qui continuera de
servir de façade d’animation gratuite, offerte personnellement par les
auteurs maison à leur lectorat, à d’éventuelles activités d’édition en
ligne payantes, dès que possible au point de vue technico-financier.
Est-il encore réaliste de rêver à la démocratie économique?»

Quant à l’avenir, «tout ce que j'espère de mieux pour le petit éditeur
indépendant issu, comme Éditel, directement du net et qui cherche à y
émerger enfin, c'est que les nouveaux supports de lecture, ouverts et
compatibles grâce au standard OeB (Open eBook), s'imposeront d'emblée
comme des objets usuels indispensables, c'est-à-dire multifonctionnels
et ultramobiles, intégrant à la fois l'informatique, l'électronique
grand public et les télécommunications, et pas plus dispendieux qu'une
console de jeux vidéo.»

Quel est son meilleur souvenir lié à l'internet? «La découverte de
quelques amitiés affinitaires, indéfectibles, m'enchante encore, tandis
que l'étroitesse de vision, le scepticisme négatif qu'affichait la
vaste majorité des auteurs de science-fiction et de fantastique vis-à-
vis du caractère pourtant immanent et inéluctable de ce qui n'est après
tout qu'un fantasme à la Star Trek, qui hante depuis longtemps
l'imaginaire collectif, soit l'e-book tout communicant qui tienne dans
le creux de la paume, ne cesse pas de m'étonner et de me laisser
pantois rétrospectivement.»

Une conclusion? «Je dirai, pour conclure, que je me trouve vraiment
fait pour être "éditeur en ligne, poète et essayiste, et peut-être même
un jour, romancier"! Fait à noter, c'est curieusement de la part des
poètes, toujours visionnaires quand ils sont authentiques, que le
concept de livre numérique a reçu le meilleur accueil!»

Après avoir été le premier site web d'auto-édition collective de langue
française, Éditel devient un site de cyberédition non commerciale, en
partenariat avec quelques auteurs «maison», ainsi qu'un webzine
littéraire. Un blog lui succède quelques années plus tard. Le blog
prend ensuite la forme d’une vitrine de diffusion web pour quelques
livres.



1995 > LA PRESSE IMPRIMÉE SE MET EN LIGNE


[Résumé]
La mise en ligne de la presse imprimée à partir de 1995 préfigure la
mise en ligne des livres imprimés quelques années plus tard, d'où
l'intérêt de ce chapitre. Au début des années 1990, les premières
éditions électroniques de journaux sont d’abord disponibles par le
biais de services commerciaux tels que America OnLine (AOL) ou
CompuServe. Les grands titres de la presse imprimée lancent ensuite
leurs propres sites web. En février 1995 est mis en ligne le site web
du mensuel Le Monde diplomatique, premier site d'un périodique imprimé
français, suivi des sites web de Libération fin 1995 et du Monde et de
L'Humanité en 1996. Au Royaume-Uni, le Times et le Sunday Times font
web commun sur un site dénommé Times Online. Aux États-Unis, la version
en ligne du Wall Street Journal est payante tandis que celle du New
York Times est disponible sur abonnement gratuit. Le Washington Post
est librement disponible en ligne, tout comme le mensuel Wired.

***

La mise en ligne de la presse imprimée à partir de 1995 préfigure la
mise en ligne des livres imprimés quelques années plus tard, d'où
l'intérêt de ce chapitre.

Au début des années 1990, les premières éditions électroniques de
journaux sont disponibles par le biais de services commerciaux tels que
America OnLine (AOL) ou CompuServe. Suite à l'apparition du premier
navigateur fin 1993 et à la croissance rapide du web qui s'ensuit, les
grands titres de la presse imprimée lancent leurs propres sites web en
1995 et 1996.

# Aux États-Unis

Aux États-Unis, la version en ligne du Wall Street Journal est payante,
avec 100.000 abonnés en 1998. Celle du New York Times est disponible
sur abonnement gratuit. Le Washington Post propose l’actualité
quotidienne en accès libre ainsi que de nombreux articles archivés, le
tout avec images, sons et vidéos. Pathfinder (rebaptisé ensuite Time)
est le site web du groupe Time-Warner, éditeur de Time Magazine, Sports
Illustrated, Fortune, People, Southern Living, Money, Sunset, etc. On
peut y lire les articles de ces magazines et les rechercher par date ou
par sujet. Lancé en 1992 en Californie, Wired, premier magazine imprimé
entièrement consacré à la culture cyber, est bien évidemment présent
sur le web.

Au Royaume-Uni, le Times et le Sunday Times font web commun sur un site
dénommé Times Online, avec possibilité de créer une édition
personnalisée.

# En France

Mis en ligne en février 1995, le site web du mensuel Le Monde
diplomatique est le premier site d’un périodique imprimé français.
Monté dans le cadre d’un projet expérimental avec l’Institut national
de l’audiovisuel (INA), ce site est inauguré lors du forum des images
Imagina. Il donne accès à l’ensemble des articles depuis janvier 1994,
par date, par sujet et par pays. L’intégralité du mensuel en cours est
consultable gratuitement pendant deux semaines suivant sa parution. Un
forum de discussion permet au journal de discuter avec ses lecteurs.

Fin 1995, le quotidien Libération met en ligne son site web, peu après
le lancement du Cahier Multimédia, un cahier imprimé hebdomadaire
inclus dans l’édition du jeudi. Le site propose la Une du quotidien, la
rubrique Multimédia (qui regroupe les articles du Cahier Multimédia et
les archives des cahiers précédents), le Cahier Livres complété par
Chapitre Un (le premier chapitre des nouveautés retenues par le
quotidien) et bien d’autres rubriques. La rubrique Multimédia est
ensuite rebaptisée Numériques.

Le site du quotidien Le Monde est lancé en 1996. On y trouve des
dossiers en ligne, la Une en version graphique à partir de 13 heures,
l’intégralité du journal avant 17 heures, l’actualité en liaison avec
l’AFP (Agence France-Presse) et des rubriques sur la Bourse, les
livres, le multimédia et le sport. En 1998, le journal complet en ligne
coûte 5 FF (0,76 euros) alors que le journal imprimé coûte 7,50 FF
(1,15 euros). S’ils concernent le multimédia, les articles du
supplément imprimé hebdomadaire Télévision-Radio-Multimédia sont
disponibles gratuitement en ligne dans la rubrique Multimédia,
rebaptisée ensuite Nouvelles technologies.

L’Humanité est le premier quotidien français à proposer la version
intégrale du journal en accès libre. Classés par rubriques, les
articles sont disponibles entre 10 heures et 11 heures du matin, à
l’exception de L’Humanité du samedi, disponible en ligne le lundi
suivant. Tous les articles sont archivés sur le site.

# L’internet, «à la fois une menace et une chance»

Quelles sont les retombées de l’internet pour les journalistes? Bernard
Boudic, le responsable éditorial du site web du quotidien Ouest-France
(site lancé en juillet 1996), explique en juin 1998: «Elles sont encore
minces. Nous commençons seulement à offrir un accès internet à chacun
(rédaction d’Ouest-France: 370 journalistes répartis dans soixante
rédactions, sur douze départements... pas simple). Certains utilisent
internet pour la messagerie électronique (courrier interne ou externe,
réception de textes de correspondants à l’étranger, envoi de fichiers
divers) et comme source d’informations. Mais cette pratique demande
encore à s’étendre et à se généraliser. Bien sûr, nous réfléchissons
aussi à tout ce qui touche à l’écriture multimédia et à sa rétro-action
sur l’écriture imprimée, aux changements d’habitudes de nos lecteurs,
etc. (...)

Internet est à la fois une menace et une chance. Menace sur l’imprimé,
très certainement (captation de la pub et des petites annonces,
changement de réflexes des lecteurs, perte du goût de l’imprimé,
concurrence d’un média gratuit, que chacun peut utiliser pour diffuser
sa propre info, etc.). Mais c’est aussi l’occasion de relever tous ces
défis, de rajeunir la presse imprimée.»

Tous sujets que l'on retrouve quelques années plus tard dans les débuts
du livre numérique: rapport accru de l'auteur avec ses lecteurs,
version payante et/ou version gratuite, version numérique et/ou version
imprimée, etc.



1995 > AMAZON, PIONNIER DU CYBERCOMMERCE


[Résumé]
Un nouveau type de librairie naît sur l’internet, avec un site web
comme vitrine et des transactions uniquement en ligne, la plus connue
étant Amazon. Amazon.com est lancé en juillet 1995 par Jeff Bezos à
Seattle, sur la côte ouest des États-Unis. La librairie en ligne débute
avec dix salariés et trois millions d'articles. Les vitrines de la
librairie sont ses pages web, et toutes les transactions se font via
l’internet. Les livres sont stockés dans de gigantesques hangars avant
d’être directement envoyés aux clients par courrier postal. En novembre
2000, Amazon compte 7.500 salariés, 28 millions d'articles, 23 millions
de clients et quatre filiales au Royaume-Uni (filiale ouverte en
octobre 1998), en Allemagne (octobre 1998), en France (août 2000) et au
Japon (novembre 2000). Une cinquième filiale est ouverte au Canada
(juin 2002), suivie d'une sixième filiale, Joyo, en Chine (septembre
2004). Présent dans sept pays et devenu une référence mondiale du
commerce en ligne (avec eBay), Amazon compte 9.000 salariés et 41
millions de clients en juillet 2005.

***

Un nouveau type de librairie naît sur l’internet, avec un site web
comme vitrine et des transactions uniquement en ligne, la plus connue
étant Amazon.

Sous la houlette de Jeff Bezos, Amazon.com ouvre ses portes
«virtuelles» en juillet 1995 avec un catalogue de trois millions de
livres - à savoir l’ensemble de la production imprimée disponible à la
vente aux États-Unis - et dix salariés basés à Seattle, sur la côte
ouest. Les livres sont stockés dans de gigantesques hangars avant
d’être directement envoyés aux clients par courrier postal.

# Les débuts

Quinze mois auparavant, au printemps 1994, Jeff Bezos fait une étude de
marché pour décider du meilleur produit à vendre sur l’internet. Dans
sa liste de vingt produits marchands, qui comprennent entre autres les
vêtements et les instruments de jardinage, les cinq premiers du
classement se trouvent être les livres, les CD, les vidéos, les
logiciels et le matériel informatique.

Jeff Bezos relate en 1997 dans le kit de presse d’Amazon: «J’ai utilisé
tout un ensemble de critères pour évaluer le potentiel de chaque
produit. Le premier critère a été la taille des marchés existants. J’ai
vu que la vente des livres représentait un marché mondial de 82
milliards de dollars US. Le deuxième critère a été la question du prix.
Je voulais un produit bon marché. Mon raisonnement était le suivant:
puisque c’était le premier achat que les gens allaient faire en ligne,
il fallait que la somme à payer soit modique. Le troisième critère a
été la variété dans le choix. Il y avait trois millions de titres pour
les livres alors qu’il n’y avait que 300.000 titres pour les CD, par
exemple.»

# Les «associés»

Au printemps 1997, Amazon.com - que tout le monde appelle désormais
Amazon - décide de s'inspirer du système d'«associés» en ligne lancé
quelques mois plus tôt par l'Internet Bookshop (Royaume-Uni), qui est
la plus grande librairie en ligne européenne.

Tout détenteur d'un site web peut vendre des livres appartenant au
catalogue d'Amazon et toucher un pourcentage de 15% sur les ventes.
L'«associé(e)» sélectionne les titres du catalogue qui l'intéressent,
en fonction de ses centres d'intérêt, et rédige ses propres résumés.
Amazon reçoit les commandes par son intermédiaire, expédie les livres,
rédige les factures et lui envoie un rapport hebdomadaire d'activité
avec le règlement correspondant.

Le réseau d'Amazon compte 30.000 sites affiliés au printemps 1998 et
60.000 sites en juin 1998, qui sont autant de vitrines supplémentaires
pour la librairie en ligne. Les affiliés sont aussi des sociétés telles
que Adobe, InfoBeat, Kemper Funds, PR Newswire, Travelocity, Virtual
Vineyards et Xoom.

# L’expansion

Outre les livres, Amazon propose également des CD, des DVD, des jeux
informatiques, etc. On peut consulter le catalogue à l’écran, lire le
résumé des livres choisis ou même des extraits, puis passer sa commande
en ligne. Le contenu éditorial du site change quotidiennement et se
veut un magazine littéraire en ligne, avec des conseils de lecture, des
articles émanant de journalistes connus (qui travaillaient auparavant
dans la presse imprimée), des entretiens avec des auteurs et des
commentaires de lecteurs. En juillet 1998, Amazon compte 1,5 million de
clients dans 160 pays, le public s’habituant peu à peu aux achats en
ligne.

En novembre 2000, Amazon compte 7.500 salariés, 28 millions d'articles,
23 millions de clients et quatre filiales au Royaume-Uni (filiale
ouverte en octobre 1998), en Allemagne (octobre 1998), en France (août
2000) et au Japon (novembre 2000). Amazon ouvre plus tard une cinquième
filiale en juin 2002, cette fois au Canada, puis une sixième filiale
(dénommée Joyo) en Chine en septembre 2004.

# L’eBookStore

Amazon ouvre son eBookStore en novembre 2000 avec un catalogue de 1.000
livres numériques. Avant ce lancement, la librairie en ligne signe deux
partenariats en août 2000, l’un avec Microsoft pour proposer des livres
lisibles sur le Microsoft Reader, et l’autre avec Adobe pour proposer
des titres lisibles sur l'Acrobat Reader.

Amazon conclut ensuite un deuxième partenariat avec Adobe en avril 2001
pour la mise en vente de 2.000 livres numériques lisibles sur l'Acrobat
eBook Reader (le nouveau logiciel d’Adobe gérant les livres sous
droits). Ces livres sont notamment des titres de grands éditeurs, des
guides de voyages et des livres pour enfants.

Présent dans sept pays et devenu une référence mondiale du commerce en
ligne (avec eBay), Amazon fête ses dix ans d'existence en juillet 2005,
avec 41 millions de clients et 9.000 salariés.

# Barnes & Noble

Le principal concurrent d’Amazon est Barnes & Noble, qui lance sa
librairie en ligne en mai 1997. Celle-ci est financée en partenariat
avec le géant des médias Bertelsmann pendant six ans, avant que Barnes
& Noble ne rachète la part détenue par Bertelsmann (36,8%) en juillet
2003 pour 164 millions de dollars US.

Contrairement à Amazon, librairie uniquement «virtuelle», le site
Barnes & Noble.com s'appuie sur une chaîne de librairies qui, en 1997,
comprend 480 librairies réparties dans 48 des 50 États que compte le
pays. Dès les débuts du site, Barnes & Noble se livre à une guerre des
prix avec Amazon, à la plus grande joie des clients qui profitent de
cette course aux rabais pour faire une économie de 20 à 40% sur
certains titres.

Barnes & Noble.com ouvre ensuite son eBookStore en août 2000, trois
mois avant Amazon, pour y proposer des livres numériques, suite à un
partenariat avec Microsoft en janvier 2000 pour la vente de livres
lisibles sur le Microsoft Reader puis un partenariat avec Adobe en août
2000 pour la vente de livres lisibles sur l'Acrobat Reader et le
Glassbook Reader, Adobe ayant racheté la société Glassbook à la même
date.



1996 > L'INTERNET ARCHIVE, POUR LES GÉNÉRATIONS FUTURES


[Résumé]
Fondée en avril 1996 par Brewster Kahle à San Francisco (Californie),
l'Internet Archive a pour but de constituer, stocker, préserver et
gérer une archive de l'internet en sauvegardant et stockant la totalité
du web tous les deux mois. L'objectif est d'offrir un outil de travail
aux universitaires, chercheurs et historiens, et de préserver un
historique de l'internet pour les générations futures. L’Internet
Archive se présente donc comme «une bibliothèque de l’internet» puis,
dans un deuxième temps, comme «une bibliothèque numérique à but non
lucratif destinée à procurer un accès universel au savoir humain». En
octobre 2001, avec 30 milliards de pages archivées, l'Internet Archive
met ses archives en accès libre sur le web grâce à la Wayback Machine,
qui permet à tout un chacun d’avoir accès à l’historique d’un site. Les
archives du web représentent 65 milliards de pages web (provenant de 50
millions de sites web) en décembre 2006, 85 milliards de pages web en
mai 2007 et 150 milliards de pages web en mars 2010.

***

L’Internet Archive est fondée en avril 1996 par Brewster Kahle à San
Francisco (Californie) pour préserver un historique de l’internet.

L’Internet Archive a pour but de constituer, stocker, préserver et
gérer une archive de l’internet, en sauvegardant et stockant la
totalité du web tous les deux mois, afin d’offrir un outil de travail
aux universitaires, chercheurs et historiens, et de préserver un
historique de l’internet pour les générations présentes et futures.

L’Internet Archive se présente donc comme «une bibliothèque de
l’internet» puis, dans un deuxième temps, comme «une bibliothèque
numérique à but non lucratif destinée à procurer un accès universel au
savoir humain».

# L’importance d’un archivage du web

Comme expliqué à l’époque sur le site, de tout temps les sociétés ont
voulu préserver leur culture et leur héritage pour les générations
présentes et futures. Les bibliothèques ont donc eu pour vocation de
conserver les traces écrites de ces cultures et de ces héritages, et
d’en procurer l’accès au grand public et aux chercheurs. Il paraît donc
essentiel qu’elles étendent leur mission aux nouvelles technologies.
Paradoxalement, le travail de sauvegarde a souvent été mal fait au
début du 20e siècle. Nombreux ont été les premiers films qui ont été
recyclés - et donc définitivement perdus - pour récupérer la couche
d’argent présente sur la pellicule. Nombre d’émissions de radio et de
télévision n’ont pas été conservées. Il importe donc de ne pas
reproduire la même erreur pour l’internet, et particulièrement pour le
web, un nouveau médium dont la portée, immense, est encore méconnue en
1996. C’est la raison d’être de l’Internet Archive.

# La Wayback Machine

En octobre 2001, avec 30 milliards de pages archivées, l'Internet
Archive met ses archives en accès libre grâce à la Wayback Machine, qui
permet à tout un chacun de voir l'historique d'un site web - à savoir
la présentation et le contenu d'un site web donné - théoriquement tous
les deux mois depuis avril 1996, date de la création de l'Internet
Archive.

En 2004, les archives du web représentent plus de 300 To (téraoctets)
de données, avec une croissance de 12 To par mois. Le nombre de pages
web visibles avec la Wayback Machine est de 65 milliards (provenant de
50 millions de sites web) en décembre 2006, 85 milliards de pages web
en mai 2007 et 150 milliards de pages web en mars 2010.

# Des collections numériques

En 2000, l’Internet Archive débute la constitution de collections
numériques, en hébergeant notamment une partie du Million Book Project
(10.520 livres en avril 2005), tout comme des archives de films de la
période 1903-1973, des archives de concerts live récents, des archives
de logiciels, des archives d’images et de vidéos, les sites relatifs au
11 septembre (2001), les sites relatifs aux élections de 2000
(présidentielles) et 2002 (élection du Congrès et des gouverneurs des
États), les sites relatifs aux pionniers du web, etc. Toutes ces
collections sont en consultation libre.

Qu’est-ce exactement que le Million Book Project? Lancé en janvier 2000
par la Carnegie Mellon University (Pennsylvanie, États-Unis), le
Million Book Project - appelé aussi Universal Library ou Universal
Digital Library (UDL) - a pour but de numériser un million de livres
dans un grand nombre de langues, en axant ses efforts sur les livres
disponibles en Inde et en Chine. En 2007, un million de livres sont
disponibles sur le site de l'université, sous forme de fichiers image
aux formats DjVu et TIFF, avec trois sites miroirs (Inde, Chine du
Nord, Chine du Sud).

Par ailleurs, en réaction au projet Google Books, l’Internet Archive
pense qu'une bibliothèque numérique à vocation mondiale ne doit pas
être liée à des enjeux commerciaux. En octobre 2005, elle lance l’Open
Content Alliance (OCA) dans l'optique de fédérer un grand nombre de
partenaires pour créer une bibliothèque planétaire publique
respectueuse du copyright et sur un modèle ouvert, avec des collections
consultables sur tout moteur de recherche.



1996 > CYLIBRIS, ÉDITEUR ÉLECTRONIQUE


[Résumé]
Fondé en août 1996 à Paris par Olivier Gainon, CyLibris (de Cy, cyber
et Libris, livre) est la première maison d'édition à utiliser
l'internet et le numérique pour publier de nouveaux auteurs
littéraires. Vendus uniquement sur le web, les livres sont imprimés à
la commande et envoyés directement au client, ce qui permet d'éviter le
stock et les intermédiaires. Au printemps 2000, CyLibris devient membre
du Syndicat national de l'édition (SNE). En 2001, certains titres sont
également vendus en version imprimée par un réseau de librairies
partenaires, notamment la Fnac, et en version numérique par Mobipocket
et Numilog. En 2003, le catalogue de CyLibris comprend une cinquantaine
de titres. CyLibris met fin à cette belle aventure en 2007.

***

Fondé à Paris en août 1996 par Olivier Gainon, CyLibris (de Cy, cyber
et Libris, livre) est le pionnier francophone de l’édition électronique
commerciale.

CyLibris est en effet la première maison d’édition à utiliser
l’internet et le numérique pour publier de nouveaux auteurs littéraires
et quelques auteurs confirmés, dans divers genres: littérature
générale, policiers, science-fiction, théâtre et poésie.

Vendus uniquement sur le web, les livres sont imprimés à la commande et
envoyés directement au client, ce qui permet d’éviter le stock et les
intermédiaires. Des extraits sont disponibles en téléchargement libre.

# Une maison d’édition spécialisée

Pendant son premier trimestre d’activité, CyLibris signe des contrats
avec treize auteurs. Fin 1999, le site compte 15.000 visites
individuelles et 3.500 livres vendus tous exemplaires confondus, avec
une année financièrement équilibrée.

Olivier Gainon explique en décembre 2000: «CyLibris a été créé d’abord
comme une maison d’édition spécialisée sur un créneau particulier de
l’édition et mal couvert à notre sens par les autres éditeurs: la
publication de premières oeuvres, donc d’auteurs débutants. Nous nous
intéressons finalement à la littérature qui ne peut trouver sa place
dans le circuit traditionnel: non seulement les premières oeuvres, mais
les textes atypiques, inclassables ou en décalage avec la mouvance et
les modes littéraires dominantes. Ce qui est rassurant, c’est que nous
avons déjà eu quelques succès éditoriaux: le grand prix de la SGDL
[Société des gens de lettres] en 1999 pour "La Toile" de Jean-Pierre
Balpe, le prix de la litote pour "Willer ou la trahison" de Jérôme
Olinon en 2000, etc. Ce positionnement de "défricheur" est en soi
original dans le monde de l’édition, mais c’est surtout son mode de
fonctionnement qui fait de CyLibris un éditeur atypique.

Créé dès 1996 autour de l’internet, CyLibris a voulu contourner les
contraintes de l’édition traditionnelle grâce à deux innovations: la
vente directe par l’intermédiaire d’un site de commerce sur internet,
et le couplage de cette vente avec une impression numérique en "flux
tendu". Cela permettait de contourner les deux barrières
traditionnelles dans l’édition: les coûts d’impression (et de stockage)
et les contraintes de distribution. Notre système gérait donc des flux
physiques: commande reçue par internet, impression du livre commandé,
envoi par la poste. Je précise que nous sous-traitons l’impression à
des imprimeurs numériques, ce qui nous permet de vendre des livres de
qualité équivalente à celle de l’offset, et à un prix comparable. Notre
système n’est ni plus cher, ni de moindre qualité, il obéit à une
économie différente qui, à notre sens, devrait se généraliser à terme.»

# Une double activité

En quoi consiste l’activité d’un éditeur électronique? «Je décrirais
mon activité comme double. D’une part celle d’un éditeur traditionnel
dans la sélection des manuscrits et leur re-travail (je m’occupe
directement de la collection science-fiction), mais également le choix
des maquettes, les relations avec les prestataires, etc. D’autre part,
une activité internet très forte qui vise à optimiser le site de
CyLibris et mettre en oeuvre une stratégie de partenariat permettant à
CyLibris d’obtenir la visibilité qui lui fait parfois défaut. Enfin, je
représente CyLibris au sein du SNE [NDLR: Syndicat national de
l’édition, dont CyLibris fait partie depuis le printemps 2000].
CyLibris est aujourd’hui une petite structure. Elle a trouvé sa place
dans l’édition, mais est encore d’une économie fragile sur internet.
Notre objectif est de la rendre pérenne et rentable et nous nous y
employons.»

# Un éditeur présent sur tous les fronts

Le site web se veut aussi un carrefour de la petite édition. Il procure
des informations pratiques aux auteurs en herbe: comment envoyer un
manuscrit à un éditeur, ce que doit comporter un contrat d’édition,
comment protéger ses manuscrits, comment tenter sa chance dans des
revues ou concours littéraires, etc.

En 2001, certains titres sont également vendus en version imprimée par
un réseau de librairies partenaires, notamment la Fnac, et en version
numérique par Mobipocket et Numilog, pour lecture sur ordinateur ou
PDA. En 2003, le catalogue de CyLibris comprend une cinquantaine de
titres.

Par ailleurs, l’équipe de CyLibris lance en mai 1999 CyLibris Infos,
une lettre d’information électronique gratuite dont l’objectif n’est
pas tant de promouvoir les livres de l’éditeur que de présenter
l’actualité de l’édition francophone. Volontairement décalée et souvent
humoristique sinon décapante, la lettre, d’abord mensuelle, paraît deux
fois par mois à compter de février 2000, avec 565 abonnés en octobre
2000. Elle change de nom en février 2001 pour devenir Édition-actu, qui
compte 1.500 abonnés en 2003 avant de laisser place au blog de
CyLibris.

Olivier Gainon met fin à toutes ces activités éditoriales en 2007 pour
passer à d’autres projets.



1996 > VERS UN SAVOIR NUMÉRIQUE


[Résumé]
Vinton Cerf, père de l’internet et fondateur de l’Internet Society
(ISOC),  explique en janvier 1998 lors d’un entretien avec le quotidien
Libération: «Le réseau fait deux choses (...): comme les livres, il
permet d’accumuler de la connaissance. Mais, surtout, il la présente
sous une forme qui la met en relation avec d’autres informations. Alors
que, dans un livre, l’information est maintenue isolée.» De plus,
l’information contenue dans les livres reste la même, alors que
l'internet privilégie les informations récentes et régulièrement
actualisées. Lors d'une conférence organisée en septembre 1996 par
l'IFIP (International Federation of Information Processing), Dale
Spender, professeure et chercheuse, propose une communication sous le
titre «Creativity and the computer education industry» en tentant de
cerner les changements apportés par l'internet dans l'acquisition du
savoir et les méthodes d'enseignement.

***

L’information contenue dans les livres reste la même, alors que
l'internet privilégie les informations récentes et régulièrement
actualisées. Il faut donc complètement repenser notre relation au
savoir.

Vinton Cerf, père de l’internet et fondateur de l’Internet Society
(ISOC),  explique en janvier 1998 lors d’un entretien avec le quotidien
Libération: «Le réseau fait deux choses (...): comme les livres, il
permet d’accumuler de la connaissance. Mais, surtout, il la présente
sous une forme qui la met en relation avec d’autres informations. Alors
que, dans un livre, l’information est maintenue isolée.»

# Une nouvelle manière d’enseigner

Lors d'une conférence organisée en septembre 1996 par l'IFIP
(International Federation of Information Processing), Dale Spender,
professeure et chercheuse, propose une communication sous le titre
«Creativity and the computer education industry» en tentant de cerner
les changements apportés par l'internet dans l'acquisition du savoir et
les méthodes d'enseignement. Voici son argumentation résumée en deux
paragraphes.

Pendant plus de cinq siècles, l'enseignement est principalement basé
sur l'information donnée par les livres. Or les habitudes liées à
l'imprimé ne peuvent être transférées au monde numérique.
L'enseignement en ligne offre des possibilités tellement nouvelles
qu'il n'est guère possible d'effectuer les distinctions traditionnelles
entre enseignant et enseigné. Le passage de la culture imprimée à la
culture numérique exige d'entièrement repenser le processus
d'enseignement, puisque nous avons maintenant l'opportunité sans
précédent de pouvoir influer sur le genre d'enseignement que nous
souhaitons.

Dans la culture imprimée, l'information contenue dans les livres
restait la même pendant un certain temps, ce qui nous a encouragé à
penser que l'information était stable. La nature même de l'imprimé est
liée à la notion de vérité, stable elle aussi. Cette stabilité et
l'ordre qu'elle engendre ont été l’un des fondements de l'âge
industriel et de la révolution scientifique. Les notions de vérité, de
loi, d'objectivité et de preuve ont été les éléments de référence de
nos croyances et de nos cultures. Mais la révolution numérique change
tout ceci. Soudain l'information en ligne supplante l'information
imprimée pour devenir la plus fiable et la plus utile, et l'usager est
prêt à la payer en conséquence. C'est cette transformation radicale
dans la nature de l'information qui doit être au coeur du débat relatif
aux méthodes d'enseignement.

# Trois expériences

En témoigne l'expérience de Patrick Rebollar, professeur de français et
de littérature française à Tokyo (Japon), qui raconte en juillet 1998:
«Mon travail de recherche est différent, mon travail d’enseignant est
différent, mon image en tant qu’enseignant-chercheur de langue et de
littérature est totalement liée à l’ordinateur, ce qui a ses bons et
ses mauvais côtés (surtout vers le haut de la hiérarchie universitaire,
plutôt constituée de gens âgés et technologiquement récalcitrants).
J’ai cessé de m’intéresser à certains collègues proches
géographiquement mais qui n’ont rien de commun avec mes idées, pour
entrer en contact avec des personnes inconnues et réparties dans
différents pays (et que je rencontre parfois, à Paris ou à Tokyo, selon
les vacances ou les colloques des uns ou des autres). La différence est
d’abord un gain de temps, pour tout, puis un changement de méthode de
documentation, puis de méthode d’enseignement privilégiant
l’acquisition des méthodes de recherche par mes étudiants, au détriment
des contenus (mais cela dépend des cours). Progressivement, le
paradigme réticulaire l’emporte sur le paradigme hiérarchique.»

Robert Beard, professeur à la Bucknell University (États-Unis), écrit
en septembre 1998: «En tant que professeur de langue, je pense que le
web présente une pléthore de nouvelles ressources disponibles dans la
langue étudiée, de nouveaux instruments d'apprentissage (exercices
interactifs Java et Shockwave) et de test, qui sont à la disposition
des étudiants quand ceux-ci en ont le temps ou l'envie, 24 heures / 24
et 7 jours / 7. Aussi bien pour mes collègues que pour moi, et bien sûr
pour notre établissement, l'internet nous permet aussi de publier
pratiquement sans limitation. (…) L'internet nous offrira tout le
matériel pédagogique dont nous pouvons rêver, y compris des notes de
lecture, exercices, tests, évaluations et exercices interactifs plus
efficaces que par le passé parce que reposant davantage sur la notion
de communication.»

Russon Wooldridge, professeur au département des études françaises de
l'Université de Toronto (Canada), explique en février 2001: «Mes
activités de recherche, autrefois menées dans une tour d'ivoire, se
font maintenant presque uniquement par des collaborations locales ou à
distance. (...) Tout mon enseignement exploite au maximum les
ressources d'internet (le web et le courriel): les deux lieux communs
d'un cours sont la salle de classe et le site du cours, sur lequel je
mets tous les matériaux des cours. Je mets toutes les données de mes
recherches des vingt dernières années sur le web (réédition de livres,
articles, textes intégraux de dictionnaires anciens en bases de données
interactives, de traités du 16e siècle, etc.). Je publie des actes de
colloques, j'édite un journal, je collabore avec des collègues
français, mettant en ligne à Toronto ce qu'ils ne peuvent pas publier
en ligne chez eux. (…) Je me rends compte que sans internet mes
activités seraient bien moindres, ou du moins très différentes de ce
qu'elles sont actuellement. Donc je ne vois pas l'avenir sans.»



1996 > LE PROJET @FOLIO, BALADEUR DE TEXTES


[Résumé]
Conçu dès octobre 1996 par Pierre Schweitzer, architecte designer à
Strasbourg (Alsace, France), le projet @folio se définit comme un
baladeur de textes ou encore comme un support de lecture nomade
permettant de lire des textes glanés sur l'internet. De petite taille,
il cherche à mimer, sous forme électronique, le dispositif technique du
livre, afin d'offrir une mémoire de fac-similés reliés en hypertexte
pour faciliter le feuilletage. Pierre est aussi l'auteur du logiciel
Mot@mot, un logiciel permettant de découper mot à mot les pages
scannées du livre. Le but est d’obtenir une chaîne d'images-mots
liquide qu'on peut remettre en page aussi facilement qu'une chaîne de
caractères pour lire le texte sur un écran de petite taille. Afin de
développer @folio et Mot@mot, Pierre fait valider un brevet
international en avril 2001 puis crée la start-up française iCodex en
juillet 2002.

***

Conçu dès octobre 1996 par Pierre Schweitzer, le projet @folio se
définit comme un baladeur permettant de lire des textes glanés sur
l’internet.

De petite taille, ce support de lecture nomade cherche à mimer, sous
forme électronique, le dispositif technique du livre, afin d’offrir une
mémoire de fac-similés reliés en hypertexte pour faciliter le
feuilletage.

# Les débuts du projet

Pierre Schweitzer, architecte designer à Strasbourg (Alsace, France),
explique en janvier 2001: «@folio est un baladeur de textes, simple,
léger, autonome, que le lecteur remplit selon ses désirs à partir du
web, pour aller lire n’importe où. Il peut aussi y imprimer des
documents personnels ou professionnels provenant d’un CD-Rom. Les
textes sont mémorisés en faisant: "imprimer", mais c’est beaucoup plus
rapide qu’une imprimante, ça ne consomme ni encre ni papier. Les liens
hypertextes sont maintenus au niveau d’une reliure tactile. (...)

Le projet est né à l’atelier Design de l’École d’architecture de
Strasbourg où j’étais étudiant. Il est développé à l’École nationale
supérieure des arts et industries de Strasbourg avec le soutien de
l’ANVAR-Alsace. Aujourd’hui, je participe avec d’autres à sa
formalisation, les prototypes, design, logiciels, industrialisation,
environnement technique et culturel, etc., pour transformer ce concept
en un objet grand public pertinent.»

# Le logiciel Mot@mot

Pierre est aussi l'auteur du logiciel Mot@mot, un logiciel permettant
de découper mot à mot les pages scannées du livre. Le but est d’obtenir
une chaîne d'images-mots liquide qu'on peut remettre en page aussi
facilement qu'une chaîne de caractères pour lire le texte sur un écran
de petite taille.

Pierre explique à la même date: «La plus grande partie du patrimoine
écrit existant est fixé dans des livres, sur du papier. Pour rendre ces
oeuvres accessibles sur la toile, la numérisation en mode image est un
moyen très efficace. Le projet Gallica en est la preuve. Mais il reste
le problème de l'adaptation des fac-similés d'origine à nos écrans de
lecture aujourd'hui: réduits brutalement à la taille d'un écran, les
fac-similés deviennent illisibles. Sauf à manipuler les barres
d'ascenseur, ce qui nécessite un ordinateur et ne permet pas une
lecture confortable. La solution proposée par Mot@mot consiste à
découper le livre, mot à mot, du début à la fin (enfin, les pages
scannées du livre...). Ces mots restent donc des images, il n'y a pas
de reconnaissance de caractères, donc pas d'erreur possible. On obtient
une chaîne d'images-mots liquide, qu'on peut remettre en page aussi
facilement qu'une chaîne de caractères. Il devient alors possible de
l'adapter à un écran de taille modeste, sans rien perdre de la
lisibilité du texte. La typographie d'origine est conservée, les
illustrations aussi.»

# Le rêve de Pierre Schweitzer

Afin de développer le projet @folio et le logiciel Mot@mot, Pierre
Schweitzer fait valider un brevet international en avril 2001, puis
crée la start-up française iCodex en juillet 2002.

Cinq ans plus tard, en août 2007, Pierre poursuit patiemment sa
croisade pour promouvoir son projet. «Il s’agit d’offrir un support de
lecture efficace aux textes qui n’en ont pas, ceux qui sont accessibles
sur le web. Avec @folio, je reste persuadé qu’un support de lecture
transportable qui serait à la fois simple et léger, annotable et
effaçable, à bas coût, respectueux de la page et de nos traditions
typographiques, pourrait apporter un supplément de confort appréciable
à tous les usagers du texte numérique. Une ardoise dont on pourrait
feuilleter l’hypertexte à main nue, en lieu et place de
l’imprimante...»

En quoi la technologie utilisée est-elle différente de celle des autres
tablettes du marché? «La technologie d'@folio est inspirée du fax et du
classeur à onglets. La mémoire flash est imprimée comme Gutenberg
imprimait ses livres. Ce mode fac-similé ne nécessite aucun format
propriétaire, il est directement lisible à l'oeil nu. Le fac-similé est
un mode de représentation de l'information robuste, pérenne, adaptable
à tout type de contenu (de la musique imprimée aux formules de
mathématique ou de chimie) sans aucune adaptation nécessaire. C'est un
mode de représentation totalement ouvert et accessible à tous: il
supporte l'écriture manuscrite, la calligraphie, les écritures non
alphabétiques, et le dessin à main levée, toutes choses qui sont très
difficiles à faire à l'aide d'un seul outil sur un ordinateur ou un
"ebook" classique. Cette conception technique nouvelle et très
simplifiée permet de recueillir une grande variété de contenus et
surtout, elle permet un prix de vente très raisonnable (100 euros pour
le modèle de base) dans différentes combinaisons de formats (tailles
d'écran) et de mémoire (nombre de pages) adaptées aux différentes
pratiques de lecture.»

Outre cette technologie novatrice, quel serait l'avantage de la lecture
sur @folio? «La simplicité d'usage, l'autonomie, le poids, le prix.
Quoi d'autre? La finesse n'est pas négligeable pour pouvoir être glissé
presque n'importe où. Et l'accès immédiat aux documents - pas de temps
d'attente comme quand on "allume" son ordinateur portable: @folio ne
s'allume jamais et ne s'éteint pas, la dernière page lue reste affichée
et une simple pression sur le bord de l'écran permet de remonter
instantanément au sommaire du document ou aux onglets de classement.»

À la même date, en août 2007, la revue en ligne anglophone TeleRead
fait l'éloge du projet @folio en intitulant l'article «Pierre
Schweitzer's Dream» (Le rêve de Pierre Schweitzer). Plusieurs
spécialistes anglophones, et non des moindres (David Rothman, Mike
Cook, Ellen Hage), rendent hommage à la persévérance de Pierre en
espérant voir son projet commercialisé un jour.



1996 > LES ÉDITIONS DU CHOUCAS SUR LA TOILE


[Résumé]
Basé en Haute-Savoie, le Choucas est une petite maison d'édition
spécialisée dans les romans policiers, la littérature, la photographie
et les livres d'art. Le Choucas voit le jour en 1992 sous la houlette
de Nicolas et Suzanne Pewny. Au prix d'un grand nombre de nuits sans
sommeil, Nicolas Pewny crée lui-même le site web du Choucas en novembre
1996. Les manuscrits affluent par courriel. Les corrections apportées
aux livres, les illustrations et l’envoi des documents à l'imprimeur se
font aussi par courriel. Le Choucas cesse malheureusement ses activités
en mars 2001, une disparition de plus à déplorer chez les petits
éditeurs indépendants. Nicolas Pewny devient ensuite consultant en
édition électronique et met ses compétences au service d'autres
organismes.

***

Né en 1992, le Choucas est une petite maison d’édition haut-savoyarde
spécialisée dans les romans policiers, la littérature, la photographie
et les livres d’art, avec un site web dès 1996.

Le Choucas voit le jour sous la houlette de Nicolas et Suzanne Pewny.
En juin 1998, Nicolas raconte: «Le site des éditions du Choucas a été
créé fin novembre 1996. Lorsque je me suis rendu compte des
possibilités qu’internet pouvait nous offrir, je me suis juré que nous
aurions un site le plus vite possible. Un petit problème: nous n’avions
pas de budget pour le faire réaliser. Alors, au prix d’un grand nombre
de nuits sans sommeil, j’ai créé ce site moi-même et l’ai fait
référencer (ce n’est pas le plus mince travail). Le site a alors évolué
en même temps que mes connaissances (encore relativement modestes) en
la matière et s’est agrandi, et a commencé à être un peu connu même
hors de France et de l’Europe.»

# Un changement considérable

Quels sont les atouts d’un site internet? «Le changement qu’internet a
apporté dans notre vie professionnelle est considérable, explique
Nicolas. Nous sommes une petite maison d’édition installée en province.
Internet nous a fait connaître rapidement sur une échelle que je ne
soupçonnais pas. Même les médias "classiques" nous ont ouvert un peu
leur portes grâce à notre site. Les manuscrits affluent par le courrier
électronique. Ainsi nous avons édité deux auteurs québécois [NDLR:
Fernand Héroux et Liz Morency, auteurs de "Affaire de coeurs", paru en
septembre 1997]. Beaucoup de livres se réalisent (corrections,
illustrations, envoi des documents à l’imprimeur) par ce moyen. Dès le
début du site nous avons reçu des demandes de pays où nous ne sommes
pas (encore) représentés: États-Unis, Japon, Amérique latine, Mexique,
malgré notre volonté de ne pas devenir un site "commercial" mais
d’information et à "connotation culturelle". (Nous n’avons pas de
système de paiement sécurisé, nous avons juste référencé sur une page
les libraires qui vendent en ligne.)»

Comment Nicolas voit-il l'avenir? «J’aurais tendance à répondre par
deux questions: Pouvez-vous me dire comment va évoluer internet?
Comment vont évoluer les utilisateurs? Nous voudrions bien rester aussi
peu "commercial" que possible et augmenter l’interactivité et le
contact avec les visiteurs du site. Y réussirons-nous? Nous avons déjà
reçu des propositions qui vont dans un sens opposé. Nous les avons
mises "en veille". Mais si l’évolution va dans ce sens, pourrons-nous
résister, ou trouver une "voie moyenne"? Honnêtement, je n’en sais
rien.»

# D’éditeur à consultant en édition électronique

Le Choucas cesse malheureusement ses activités en mars 2001, une
disparition de plus à déplorer chez les petits éditeurs indépendants.

Nicolas raconte en juin 2001: «Comme je le prévoyais, notre
distributeur a déposé son bilan. Et malheureusement les éditions du
Choucas (ainsi que d’autres éditeurs) ont cessé leur activité
éditoriale. Je maintiens gracieusement le site web pour témoignage de
mon savoir-faire d’éditeur on- et off-line. (...)

Je ne regrette pas ces dix années de lutte, de satisfactions et de
malheurs passés aux éditions du Choucas. J’ai connu des auteurs
intéressants dont certains sont devenus des amis... Maintenant je fais
des publications et des sites internet pour d’autres. En ce moment pour
une ONG [organisation non gouvernementale] internationale caritative;
je suis ravi de participer (modestement) à leur activité à but non
lucratif. Enfin on ne parle plus de profit ou de manque à gagner, c’est
reposant.»

Fort de son expérience dans le domaine de la librairie, de l'édition,
de l'internet et du numérique, Nicolas Pewny est maintenant consultant
en édition électronique et met ses compétences au service d'autres
organismes.

Il écrit en février 2003: «Je vois le livre numérique du futur comme un
"ouvrage total" réunissant textes, sons, images, vidéo, interactivité:
une nouvelle manière de concevoir et d'écrire et de lire, peut-être sur
un livre unique, sans cesse renouvelable, qui contiendrait tout ce que
l'on a lu, unique et multiple compagnon. Utopique? Invraisemblable?
Peut-être pas tant que cela!»



1997 > LA CONVERGENCE MULTIMÉDIA


[Résumé]
La convergence multimédia est la convergence de tous les secteurs liés
à l'information (imprimerie, édition, presse, conception graphique,
enregistrements sonores, films, radiodiffusion, etc.) suite à
l’utilisation des techniques de numérisation. On peut également la
définir comme la convergence de l’informatique, du téléphone, de la
radio et de la télévision dans une industrie de la communication
utilisant les mêmes canaux de distribution, avec accélération du
processus matériel de production. La convergence multimédia a de
nombreux revers, à savoir des contrats occasionnels et précaires pour
les salariés, l'absence de syndicats pour les télétravailleurs, le
droit d'auteur souvent mis à mal pour les auteurs, etc. La convergence
multimédia amène-t-elle des emplois nouveaux ou bien est-elle source de
chômage? Ce sujet est débattu dès janvier 1997 lors du Colloque sur la
convergence multimédia organisé par l'Organisation internationale du
travail (BIT) à Genève, avec des débats qui se poursuivent les années
suivantes.

***

La convergence multimédia est la convergence de tous les secteurs liés
à l'information (imprimerie, édition, presse, conception graphique,
enregistrements sonores, films, radiodiffusion, etc.) suite à
l’utilisation des techniques de numérisation.

On peut également la définir comme la convergence de l’informatique, du
téléphone, de la radio et de la télévision dans une industrie de la
communication utilisant les mêmes canaux de distribution, souvent
dénommés autoroutes de l'information (ou inforoutes), avec accélération
du processus matériel de production.

# Une approche plus concrète

La numérisation permet de créer, enregistrer, combiner, stocker,
rechercher et transmettre des textes, sons et images par des moyens
simples et rapides. Des procédés similaires permettent le traitement de
l’écriture, de la musique et du cinéma alors que, par le passé, ce
traitement était assuré par des procédés différents sur des supports
différents (papier pour l’écriture, bande magnétique pour la musique,
celluloïd pour le cinéma). De plus, des secteurs distincts comme
l’édition (qui produit des livres) et l’industrie musicale (qui produit
des disques) travaillent désormais de concert pour produire des CD-Rom.

Pour mémoire, ceci n'est pas le premier bouleversement affectant la
chaîne de l’édition, loin de là. Dans les années 1970, l’imprimerie
traditionnelle est d’abord ébranlée par les machines de
photocomposition. Dans les années 1980 et 1990, le coût de l’impression
continue ensuite de baisser avec les photocopieurs, les photocopieurs
couleur, les ateliers de PAO (publication assistée par ordinateur) et
le matériel d’impression numérique.

Tout contenu est désormais systématiquement numérisé pour permettre son
transfert par voie électronique et pour accélérer la processus matériel
de production. Dans l’édition, le rédacteur, le concepteur artistique
et l'infographiste travaillent souvent simultanément au même ouvrage.
Dans la presse, alors qu’auparavant le personnel de production devait
dactylographier les textes du personnel de rédaction, les journalistes
envoient désormais directement leurs textes pour mise en page.

# Un colloque international

Si la convergence multimédia entraîne de nouveaux emplois dans certains
secteurs  - par exemple ceux liés à la production de films ou de
produits audio-visuels - d'autres secteurs sont soumis à d'inquiétantes
restructurations ou même des licenciements en masse. Ces problèmes sont
suffisamment préoccupants pour être débattus lors d’un colloque sur la
convergence multimédia organisé en janvier 1997 par l'Organisation
internationale du travail (OIT) à Genève.

Professeur associé en sciences sociales à l’Université d’Utrecht (Pays-
Bas), Peter Leisink explique que la rédaction des textes et la
correction des épreuves se font désormais à domicile, le plus souvent
par des travailleurs ayant pris le statut d’indépendants à la suite de
licenciements et de délocalisations ou fusions d’entreprises. «Or cette
forme d’emploi tient plus du travail précaire que du travail
indépendant, car ces personnes n’ont que peu d’autonomie et sont
généralement tributaires d’une seule maison d’édition.»

Selon Michel Muller, secrétaire général de la FILPAC (Fédération des
industries du livre, du papier et de la communication), les industries
graphiques françaises ont perdu 20.000 emplois en dix ans, entre 1987
et 1996, avec des effectifs qui sont passés de 110.000 à 90.000
salariés. Les entreprises doivent mettre sur pied des plans sociaux
très coûteux pour favoriser le reclassement des personnes licenciées,
en créant des emplois souvent artificiels, alors qu’il aurait mieux
fallu financer des études fiables sur la manière d’équilibrer créations
et suppressions d’emplois lorsqu’il était encore temps.

Walter Durling, directeur du grand opérateur téléphonique AT&T (États-
Unis), insiste sur le fait que les nouvelles technologies n’apporteront
pas de changements fondamentaux à la situation des salariés au sein de
leur entreprise. L’invention du film n’a pas tué le théâtre et celle de
la télévision n’a pas fait disparaître le cinéma. Les entreprises
devraient créer des emplois liés aux nouvelles technologies et les
proposer à ceux qui sont obligés de quitter d’autres postes devenus
obsolètes. Des arguments bien théoriques alors qu’il s’agit plutôt d’un
problème de pourcentage. Combien de créations de postes pour combien de
licenciements?

# Des suppressions massives d’emplois

À part quelques cas particuliers mis en avant par les organisations
d’employeurs, la convergence multimédia entraîne des suppressions
massives d’emplois. Partout dans le monde, des postes à faible
qualification technique sont remplacés par des postes demandant des
qualifications techniques élevées. Les travailleurs peu qualifiés sont
licenciés. D’autres suivent une formation professionnelle
complémentaire, parfois auto-financée sur leur temps libre, et cette
formation professionnelle ne garantit pas pour autant le réemploi.

Les syndicats préconisent pour leur part la création d’emplois par
l’investissement, l’innovation, la formation aux nouvelles
technologies, la reconversion des travailleurs dont les emplois sont
supprimés, des conventions collectives équitables, la défense du droit
d’auteur, une meilleure protection des travailleurs dans le secteur
artistique, et enfin la défense des télétravailleurs en tant que
travailleurs à part entière.

Malgré tous les efforts des syndicats, la situation deviendra-elle
aussi dramatique que celle décrite dans une note des actes du colloque,
indiquant que «certains craignent un futur dans lequel les individus
seront forcés de lutter pour survivre dans une jungle électronique. Les
mécanismes de survie établis au cours des dernières décennies - tels
que relations de travail relativement stables, conventions collectives,
représentation des salariés, formation professionnelle procurée par les
employeurs et régimes de sécurité sociale cofinancés par employeurs et
employés - risquent d’être mis à rude épreuve dans un monde du travail
qui franchit les frontières à la vitesse de la lumière.»



1997 > UN PORTAIL POUR LES BIBLIOTHÈQUES NATIONALES EUROPÉENNES


[Résumé]
Mis en ligne en janvier 1997, Gabriel - acronyme de «Gateway and Bridge
to Europe’s National Libraries» -  est un portail trilingue offrant un
point d’accès unique aux services internet des bibliothèques nationales
européennes. L’idée d’un tel site naît en 1994 lors de la réunion
annuelle de la CENL (Conference of European National Librarians) à Oslo
(Norvège). En mars 1995, une nouvelle réunion rassemble les
représentants des bibliothèques nationales des Pays-Bas, du Royaume-Uni
et de Finlande, qui dessinent un projet pilote et sont rejoints ensuite
par les bibliothèques nationales d’Allemagne, de France et de Pologne,
suite à quoi un premier site Gabriel est lancé en septembre 1995. Lors
de la réunion annuelle de la CENL en 1996 à Lisbonne (Portugal),
Gabriel devient un site officiel de la CENL, avec un nouveau portail
trilingue (anglais, allemand, français) lancé en janvier 1997.

***

Mis en ligne en janvier 1997, Gabriel est un portail trilingue
(anglais, allemand, français) offrant un point d’accès unique aux
services internet des bibliothèques nationales européennes.

# Le site de Gabriel

Gabriel est l’acronyme de «Gateway and Bridge to Europe’s National
Libraries». On lit sur le site que le choix de ce nom «rappelle
également les travaux de Gabriel Naudé, dont l’"Advis pour dresser une
bibliothèque" (Paris, 1627) est le premier travail théorique en Europe
sur les bibliothèques et qui constitue ainsi un point de départ sur les
bibliothèques de recherche modernes. Le nom Gabriel est aussi employé
dans de nombreuses langues européennes et vient de l'Ancien Testament,
Gabriel étant l'un des archanges, ou messager céleste. Il est également
présent dans le Nouveau Testament et dans le Coran.»

Plus prosaïquement, le site offre en 1998 des liens hypertextes vers
les services internet des 38 bibliothèques nationales participantes
(Allemagne, Autriche, Belgique, Bulgarie, Danemark, Espagne, Estonie,
Finlande, France, Grèce, Hongrie, Irlande, Islande, Italie, Lettonie,
Liechtenstein, Lituanie, Luxembourg, Macédoine, Malte, Norvège, Pays-
Bas, Pologne, Portugal, République slovaque, République tchèque,
Roumanie, Royaume-Uni, San Marino, Suède, Suisse, Turquie et Vatican).

Les services internet sont très divers d’une bibliothèque à l’autre,
avec une liste complète disponible par bibliothèque. Ces services sont
par exemple des catalogues en ligne appelés aussi OPAC (Online Public
Access Catalogues), des bibliographies nationales, des catalogues
collectifs nationaux, des index de périodiques, des serveurs web et des
gophers (à savoir des systèmes d’information à base de menus textuels à
plusieurs niveaux). Une rubrique spécifique informe des projets communs
à plusieurs pays. La recherche sur Gabriel est possible par pays et par
type de services.

#  L’historique de Gabriel

Comment Gabriel voit-il le jour? L’idée d’un projet commun aux
bibliothèques nationales européennes naît lors de la réunion annuelle
de la CENL (Conference of European National Librarians) en 1994 à Oslo
(Norvège). Le projet débute par un tableau d’affichage électronique
commun qui est régulièrement actualisé avec les projets internet en
cours.

En mars 1995, une nouvelle réunion rassemble les représentants des
bibliothèques nationales du Pays-Bas (Koninklijke Bibliotheek), du
Royaume-Uni (British Library) et de Finlande (Helsinki University
Library), qui dessinent un projet pilote et sont rejoints ensuite par
les bibliothèques nationales d’Allemagne (Die Deusche Bibliothek), de
France (Bibliothèque nationale de France) et de Pologne (Biblioteka
Narodowa). Gabriel décrirait leurs services et collections en tentant
d’inciter d’autres bibliothèques nationales à participer au projet.

Lancé en septembre 1995, le premier site Gabriel est géré par la
British Library, qui s’occupe de sa maintenance éditoriale, avec deux
sites miroirs sur les serveurs des bibliothèques nationales des Pays-
Bas et de Finlande.

La seconde étape se déroule entre octobre 1995 et septembre 1996. Les
bibliothèques nationales n’ayant pas participé à la phase pilote sont
invitées à se joindre au projet puisqu’elles ont débuté en parallèle
leur propre site web et leur catalogue en ligne, si bien que le nombre
de bibliothèques utilisant Gabriel en tant que portail commun
s’accroît.

Pendant sa réunion annuelle en septembre 1996 à Lisbonne (Portugal), la
CENL décide de prendre Gabriel sous son ombrelle et de lancer un
portail officiel plus conséquent à compter de janvier 1997. Désormais
trilingue (anglais, allemand, français), Gabriel est maintenu par la
bibliothèque nationale des Pays-Bas (Koninklijke Bibliotheek), avec
quatre sites miroir sur les serveurs des bibliothèques nationales du
Royaume-Uni, de Finlande, d’Allemagne et de Slovénie.

Beaucoup plus tard, en été 2005, Gabriel fusionnera avec le site web de
l’European Library (lancé par la CENL en janvier 2004) pour proposer un
portail commun aux 43 bibliothèques nationales européennes. Europeana
verra le jour trois ans après, en novembre 2008, en tant que
bibliothèque numérique européenne.

# Les bibliothèques publiques

Qu’en est-il des bibliothèques publiques? Sur le site de la Commission
européenne, le document «Internet and the Library Sphere» évalue à
1.000 environ le nombre de bibliothèques publiques disposant d’un site
web en novembre 1998.

Ces bibliothèques sont réparties dans 26 pays. Les pays les plus
représentés sont la Finlande (247 bibliothèques), la Suède (132
bibliothèques), le Royaume-Uni (112 bibliothèques), le Danemark (107
bibliothèques), l'Allemagne (102 bibliothèques), les Pays-Bas (72
bibliothèques), la Lituanie (51 bibliothèques), l'Espagne (56
bibliothèques) et la Norvège (45 bibliothèques). La Russie a un site
commun pour 26 bibliothèques publiques de recherche. Les pays
nouvellement représentés sont la République tchèque (29 bibliothèques)
et le Portugal (3 bibliothèques).

Les sites sont hétérogènes. Certains se contentent de mentionner
l’adresse postale de la bibliothèque et ses heures d’ouverture, tandis
que d’autres proposent toute une gamme de services, y compris un accès
direct à leur catalogue en ligne.



1997 > E INK, TECHNOLOGIE D'ENCRE ÉLECTRONIQUE


[Résumé]
Les recherches sur le papier électronique sont en cours dès 1997, les
deux projets les plus avancés étant ceux des sociétés E Ink et Gyricon
Media. Ce support souple aura une densité comparable au papier
plastifié ou au transparent. Il pourra être utilisé indéfiniment, avec
un contenu changé à volonté via l’internet. Si le concept est
révolutionnaire, le produit lui-même est le résultat d’une fusion entre
trois sciences, la chimie, la physique et l’électronique. En avril
1997, des chercheurs du Media Lab du MIT (Massachusetts Institute of
Technology) créent la société E Ink pour développer une technologie
d'encre électronique. En juillet 2002, E Ink présente le prototype du
premier écran utilisant cette technologie, commercialisé en 2004.
Suivent des écrans pour diverses tablettes de lecture (Librié, Sony
Reader, Cybook, Kindle, Nook, etc.), puis les prototypes des premiers
écrans souples, qui annoncent le papier électronique de demain. Un
deuxième projet est développé par la société Gyricon Media, émanation
du centre Xerox de la Silicon Valley.

***

Les recherches sur le papier électronique sont en cours dès 1997, les
deux projets les plus avancés étant ceux des sociétés E Ink et Gyricon
Media.

Ce support souple aura une densité comparable au papier plastifié ou au
transparent. Il pourra être utilisé indéfiniment, avec un contenu
changé à volonté via l’internet. Si le concept est révolutionnaire, le
produit lui-même est le résultat d’une fusion entre trois sciences, la
chimie, la physique et l’électronique.

# La technologie E Ink

En avril 1997, des chercheurs du Media Lab du MIT (Massachusetts
Institute of Technology) créent la société E Ink pour développer une
technologie d'encre électronique. Très schématiquement, la technologie
est la suivante: prises entre deux feuilles de plastique souple, des
millions de micro-capsules contiennent chacune des particules noires et
blanches en suspension dans un fluide clair. Un champ électrique
positif ou négatif permet de faire apparaître le groupe de particules
souhaité à la surface du support, pour afficher, modifier ou effacer
les données.

En juillet 2002, E Ink présente le prototype du premier écran utilisant
cette technologie, un écran de haute résolution à matrice active
développé en partenariat avec les sociétés Toppan et Philips. Cet écran
est commercialisé en 2004. Suivent des écrans pour diverses tablettes
de lecture (Librié, Sony Reader, Cybook, Kindle, Nook, etc.) puis les
prototypes des premiers écrans souples, qui annoncent le papier
électronique de demain.

La première tablette de lecture disposant d’un écran E Ink de 6 pouces
(au lieu de l’écran LCD habituel) est le Librié, lancé en avril 2004
par Sony au Japon. Suit le Sony Reader lancé en octobre 2006 aux États-
Unis, avec un écran utilisant une technologie E Ink plus avancée, à
savoir «un écran qui donne une excellente expérience de lecture, très
proche de celle du vrai papier, et qui ne fatigue pas les yeux» (Mike
Cook, auteur du site epubBooks.com). Le CyBook Gen3 lancé par Bookeen
en juillet 2007, le Kindle lancé par Amazon en novembre 2007 et le Nook
lancé par Barnes & Noble en novembre 2009 disposent également d’un
écran E Ink.

# La technologie Gyricon

Un autre projet d’encre électronique est développé par Xerox. Le centre
Xerox de la Silicon Valley (Californie), dénommé PARC (Palo Alto
Research Center), travaille depuis 1997 à la mise au point d’une
technique d’affichage dénommée Gyricon.

Le procédé est un peu différent de celui de la société E Ink. Très
schématiquement, la technologie est la suivante: prises entre deux
feuilles de plastique souple, des millions de micro-alvéoles
contiennent des micro-billes bicolores en suspension dans un liquide
clair. Chaque bille est pourvue d'une charge électrique. Une impulsion
électrique extérieure permet la rotation des billes, et donc le
changement de couleur, pour afficher, modifier ou effacer des données.
Dénommé SmartPaper, ce papier électronique est destiné à être produit
en rouleaux, tout comme le papier traditionnel.

En décembre 2000, des chercheurs de PARC créent la société Gyricon
Media dans le but de développer et commercialiser le SmartPaper. Le
marché pressenti est d'abord celui de l'affichage commercial, qui
utilise le système SmartSign, développé par Gyricon Media en complément
du SmartPaper. La vente d’affichettes fonctionnant sur piles débute en
2004. Viennent ensuite les panneaux de signalisation électroniques puis
les premiers prototypes de papier électronique et de journal
électronique. La société Gyricon Media disparaît en 2005, les activités
de recherche et de développement se poursuivant au sein de Xerox.

# Le codex numérique

Christian Vandendorpe, professeur à l’Université d’Ottawa (Canada) et
spécialiste des théories de la lecture, écrit en mai 2001: «Lorsque le
procédé de l’encre électronique sera commercialisé sous la forme d’un
codex numérique plastifié offrant une parfaite lisibilité en lumière
réfléchie, comparable à celle du papier - ce qui devrait être courant
vers 2010 ou 2015 -, il ne fait guère de doute que la part du papier
dans nos activités de lecture quotidienne descendra à une fraction de
ce qu’elle était hier. En effet, ce nouveau support portera à un sommet
l’idéal de portabilité qui est à la base même du concept de livre. Tout
comme le codex avait déplacé le rouleau de papyrus, qui avait lui-même
déplacé la tablette d’argile, le codex numérique déplacera le codex
papier, même si ce dernier continuera à survivre pendant quelques
décennies, grâce notamment au procédé d’impression sur demande qui sera
bientôt accessible dans des librairies spécialisées. Avec sa matrice de
quelques douzaines de pages susceptibles de permettre l’affichage de
millions de livres, de journaux ou de revues, le codex numérique
offrira en effet au lecteur un accès permanent à la bibliothèque
universelle. En plus de cette ubiquité et de cette instantanéité, qui
répondent à un rêve très ancien, le lecteur ne pourra plus se passer de
l’indexabilité totale du texte électronique, qui permet de faire des
recherches plein texte et de trouver immédiatement le passage qui
l’intéresse. Enfin, le codex numérique permettra la fusion des notes
personnelles et de la bibliothèque et accélérera la mutation d’une
culture de la réception vers une culture de l’expression personnelle et
de l’interaction.»



1997 > OVOSITE, ESPACE D’ÉCRITURE HYPERMÉDIA


[Résumé]
Principe de base du web, l’hyperlien relie des textes et des images
entre eux, et les relie aussi à des bandes sonores ou des vidéos. Des
écrivains férus de nouvelles technologies ne tardent pas à en explorer
les possibilités, par exemple dans des sites d’écriture hypermédia ou
des oeuvres d’hyperfiction. Mis en ligne en juin 1997, oVosite est un
espace d’écriture hypermédia conçu par un collectif de six auteurs
issus du département hypermédia de l’Université Paris 8: Chantal
Beaslay, Laure Carlon, Luc Dall’Armellina (qui est aussi le webmestre
d'oVosite), Philippe Meuriot, Anika Mignotte et Claude Rouah. Luc
Dall’Armellina explique en juin 2000: «oVosite est un site web conçu et
réalisé (...) autour d’un symbole primordial et spirituel, celui de
l’oeuf. Le site s’est constitué selon un principe de cellules autonomes
qui visent à exposer et intégrer des sources hétérogènes (littérature,
photo, peinture, vidéo, synthèse) au sein d’une interface unifiante.»

***

Mis en ligne en juin 1997, oVosite est un espace d’écriture hypermédia
conçu par un collectif de six auteurs issus du département hypermédia
de l’Université Paris 8.

Principe de base du web, l’hyperlien relie des textes et des images
entre eux, et les relie aussi à des bandes sonores ou des vidéos. Des
auteurs férus de nouvelles technologies ne tardent pas à en explorer
les possibilités dans des sites d’écriture hypermédia, par exemple
oVosite, dont les auteurs sont Chantal Beaslay, Laure Carlon, Luc
Dall’Armellina (qui est aussi son webmestre), Philippe Meuriot, Anika
Mignotte et Claude Rouah.

Luc Dall’Armellina explique en juin 2000: «oVosite est un site web
conçu et réalisé (...) autour d’un symbole primordial et spirituel,
celui de l’oeuf. Le site s’est constitué selon un principe de cellules
autonomes qui visent à exposer et intégrer des sources hétérogènes
(littérature, photo, peinture, vidéo, synthèse) au sein d’une interface
unifiante.

"Les récits voisins", la première cellule active, met en scène huit
nouvelles originales - métaphores d'éclosion ou de gestation - à
travers des ancrages variant selon trois regards: éléments de
l'environnement (rouge), personnages de l'histoire (violet),
correspondances poétiques (bleu). L'interprétation de ces regards,
c'est-à-dire le choix des liens pour chaque élément de texte, incombe à
chaque auteur et dépend de sa perception individuelle du sujet au sein
de son univers intime. "Les récits voisins" est une oeuvre collective,
un travail d'écriture multimédia qui s'est étendu sur près de six mois.

"Désirs" est une proposition de fragmentation d'un poème anonyme (texte
de 1692) et qui occupe l'espace sur le mode de l'apparition des phrases
reliées à des mots qui s'affichent et disparaissent au gré d'un
aléatoire mesuré.»

# L’hyperlien et l’écriture

Les possibilités offertes par l’hyperlien ont-elles changé son mode
d’écriture? La réponse de Luc est à la fois négative et positive.

Négative d’abord: «Non - parce qu’écrire est de toute façon une affaire
très intime, un mode de relation qu’on entretient avec son monde, ses
proches et son lointain, ses mythes et fantasmes, son quotidien et
enfin, appendus à l’espace du langage, celui de sa langue d’origine.
Pour toutes ces raisons, je ne pense pas que l’hypertexte change
fondamentalement sa manière d’écrire, qu’on procède par touches, par
impressions, associations, quel que soit le support d’inscription, je
crois que l’essentiel se passe un peu à notre insu.»

Positive ensuite: «Oui - parce que l’hypertexte permet sans doute de
commencer l’acte d’écriture plus tôt: devançant l’activité de lecture
(associations, bifurcations, sauts de paragraphes) jusque dans l’acte
d’écrire. L’écriture (ceci est significatif avec des logiciels comme
StorySpace) devient peut-être plus modulaire. On ne vise plus tant la
longue horizontalité du récit, mais la mise en espace de ses fragments,
autonomes. Et le travail devient celui d’un tissage des unités entre
elles. L’autre aspect lié à la modularité est la possibilité
d’écritures croisées, à plusieurs auteurs. Peut-être s’agit-il
d’ailleurs d’une méta-écriture, qui met en relation les unités de sens
(paragraphes ou phrases) entre elles.»

# «Des situations nouvelles»

Luc écrit aussi: «La couverture du réseau autour de la surface du globe
resserre les liens entre les individus distants et inconnus. Ce qui
n’est pas simple puisque nous sommes placés devant des situations
nouvelles: ni vraiment spectateurs, ni vraiment auteurs, ni vraiment
lecteurs, ni vraiment interacteurs. Ces situations créent des nouvelles
postures de rencontre, des postures de "spectacture" ou de "lectacture"
(Jean-Louis Weissberg). Les notions de lieu, d’espace, de temps,
d’actualité sont requestionnées à travers ce médium qui n’offre plus
guère de distance à l’événement mais se situe comme aucun autre dans le
présent en train de se faire. L’écart peut être mince entre l’envoi et
la réponse, parfois immédiat (cas de la génération de textes).

Mais ce qui frappe et se trouve repérable ne doit pas masquer les
aspects encore mal définis tels que les changements radicaux qui
s’opèrent sur le plan symbolique, représentationnel, imaginaire et plus
simplement sur notre mode de relation aux autres. "Plus de proximité"
ne crée pas plus d’engagement dans la relation, de même "plus de liens"
ne créent pas plus de liaisons, ou encore "plus de tuyaux" ne créent
pas plus de partage.

Je rêve d’un internet où nous pourrions écrire à plusieurs sur le même
dispositif, une sorte de lieu d’atelier d’écritures permanent et qui
autoriserait l’écriture personnelle (c’est en voie d’exister), son
partage avec d’autres auteurs, leur mise en relation dans un tissage
d’hypertextes et un espace commun de notes et de commentaires sur le
travail qui se crée.»



1997 > NON, ROMAN MULTIMÉDIA


[Résumé]
Lucie de Boutiny est l’auteure de NON, un roman multimédia débuté en
août 1997 et publié en feuilleton par Synesthésie, revue en ligne d’art
contemporain. Elle raconte en juin 2000: «NON est un roman comique qui
fait la satire de la vie quotidienne d’un couple de jeunes cadres
supposés dynamiques. Bien qu’appartenant à l’élite high-tech d’une
industrie florissante, Monsieur et Madame sont les jouets de la dite
révolution numérique. (...) NON prolonge les expériences du roman post-
moderne (récits tout en digression, polysémie avec jeux sur les
registres - naturaliste, mélo, comique... - et les niveaux de langues,
etc.). Cette hyper-stylisation permet à la narration des développements
inattendus et offre au lecteur l’attrait d’une navigation dans des
récits multiples et multimédias, car l’écrit à l’écran s’apparente à un
jeu et non seulement se lit mais aussi se regarde.»

***

Lucie de Boutiny est l’auteure de NON, un roman multimédia débuté en
août 1997 et publié en feuilleton par Synesthésie, revue en ligne d’art
contemporain.

Elle raconte en juin 2000: «NON prolonge les expériences du roman post-
moderne (récits tout en digression, polysémie avec jeux sur les
registres - naturaliste, mélo, comique... - et les niveaux de langues,
etc.). Cette hyperstylisation permet à la narration des développements
inattendus et offre au lecteur l'attrait d'une navigation dans des
récits multiples et multimédia, car l'écrit à l'écran s'apparente à un
jeu et non seulement se lit mais aussi se regarde.

Quant au sujet: NON est un roman comique qui fait la satire de la vie
quotidienne d'un couple de jeunes cadres supposés dynamiques. Bien
qu'appartenant à l'élite high-tech d'une industrie florissante,
Monsieur et Madame sont les jouets de la dite révolution numérique.
Madame, après quelques années de bons et loyaux services d'audit
expatriée dans les pays asiatiques, vient d'être licenciée. À longueur
de journées inactives, elle se pâme d'extase devant une sitcom
sirupeuse et dépense sans compter l'argent du ménage dans des achats
compulsifs en ligne. Monsieur fait semblant d'aimer son travail de
vendeur de bases de données en ligne. Il cherche un sens à sa vie
d'homme blanc supposé appartenir à une élite sociale: ses attentes sont
calquées sur les valeurs diffusées par la publicité omniprésente. Les
personnages sont des bons produits. Les images et le style graphique
qui accompagnent leur petite vie conventionnelle ne se privent pas de
détourner nombre de vrais bandeaux publicitaires et autres icônes qui
font l'apologie d'une vie bien encadrée par une société de contrôle.»

# Une concentration maximale

Les romans précédents de Lucie de Boutiny sont publiés sous forme
imprimée. Un roman numérique requiert-il une démarche différente?
«D’une manière générale, mon humble expérience d’apprentie auteur m’a
révélé qu’il n’y a pas de différence entre écrire de la fiction pour le
papier ou le pixel: cela demande une concentration maximale, un
isolement à la limite désespéré, une patience obsessionnelle dans le
travail millimétrique avec la phrase, et bien entendu, en plus de la
volonté de faire, il faut avoir quelque chose à dire! Mais avec le
multimédia, le texte est ensuite mis en scène comme s’il n’était qu’un
scénario. Et si, à la base, il n’y a pas un vrai travail sur le langage
des mots, tout le graphisme et les astuces interactives qu’on peut y
mettre fera gadget. Par ailleurs, le support modifie l’appréhension du
texte, et même, il faut le souligner, change l’oeuvre originale. Et
cela ne signifie pas: "the medium is the message" - je vous épargne le
millionième commentaire sur cette citation. Il n'y a pas non plus
dégradation de la littérature mais déplacement...

Par exemple un concert live de jazz ,écouté dans les arènes de Cimiez,
n'est plus le même une fois enregistré, donc compressé, puis écouté
dans une voiture qui file sur l'autoroute. Et pourtant, le mélomane se
satisfait du formatage car ce qui compte est: "j'ai besoin de musique,
je veux l'entendre maintenant". Notre rapport à la littérature évolue
dans ce sens: il y aura de plus en plus d'adaptations, de formats, de
supports, de versions, mais aussi différents prix pour une même oeuvre
littéraire, etc. Comme pour la musique aujourd'hui, il nous faut être
de plus en plus instruits et riches pour posséder les bonnes versions.»

# Des allers-retours papier-pixel

Lucie de Boutiny ajoute aussi: «Mes "conseillers littéraires", des amis
qui n’ont pas ressenti le vent de liberté qui souffle sur le web,
aimeraient que j’y reste, engluée dans la pâte à papier. Appliquant le
principe de demi-désobéissance, je fais des allers-retours papier-
pixel. L’avenir nous dira si j’ai perdu mon temps ou si un nouveau
genre littéraire hypermédia va naître. (...) Si les écrivains français
classiques en sont encore à se demander s’ils ne préfèrent pas le petit
carnet Clairefontaine, le Bic ou le Mont-Blanc fétiche, et un usage
modéré du traitement de texte, plutôt que l’ordinateur connecté, c’est
que l’HTX [HyperTeXt Literature] nécessite un travail d’accouchement
visuel qui n’est pas la vocation originaire de l’écrivain papier. En
plus des préoccupations du langage (syntaxe, registre, ton, style,
histoire...), le techno-écrivain - collons-lui ce label pour le
différencier - doit aussi maîtriser la syntaxe informatique et
participer à l’invention de codes graphiques car lire sur un écran est
aussi regarder.»

# L’intelligence collective virtuelle

Que pense-t-elle de l’internet de l’an 2000? «Comme tous ceux qui ont
surfé avec des modems de 14.4 Ko sur le navigateur Mosaic et son
interface en carton-pâte, je suis déçue par le fait que l'esprit
libertaire ait cédé le pas aux activités libérales décérébrantes. Les
frères ennemis devraient se donner la main comme lors des premiers
jours car le net à son origine n'a jamais été un repaire de "has been"
mélancoliques, mais rien ne peut résister à la force d'inertie de
l'argent. C'était en effet prévu dans le scénario, des stratégies
utopistes avaient été mises en place mais je crains qu'internet ne soit
plus aux mains d'internautes comme c'était le cas.

L'intelligence collective virtuelle pourtant se défend bien dans divers
forums ou listes de discussions, et ça, à défaut d'être souvent
efficace, c'est beau. Dans l'utopie originelle, on aurait aimé profiter
de ce nouveau média, notamment de communication, pour sortir de cette
tarte à la crème qu'on se reçoit chaque jour, merci à la société du
spectacle, et ne pas répéter les erreurs de la télévision qui n'est, du
point de vue de l'art, jamais devenue un média de création ambitieux.

Sinon, les écrivains français, c'est historique, sont dans la majorité
technophobes... Les institutions culturelles et les universitaires
lettrés en revanche soutiennent les démarches hyperlittéraires à force
de colloques et publications diverses. Du côté des plasticiens, je suis
encore plus rassurée, il est acquis que l'art en ligne existe.»

Lucie de Boutiny participe en mai 2001 à la création d’E-critures, une
association d’artistes multimédia, en collaboration avec Gérard Dalmon
et Xavier Malbreil.



1997 > GALLICA, BIBLIOTHÈQUE NUMÉRIQUE


[Résumé]
Bibliothèque numérique de la Bibliothèque nationale de France (BnF),
Gallica est inauguré en octobre 1997 avec des textes et des images du
19e siècle francophone, «siècle de l’édition et de la presse moderne,
siècle du roman mais aussi des grandes synthèses historiques et
philosophiques, siècle scientifique et technique». Gallica  devient
rapidement l'une des plus importantes bibliothèques numériques du
réseau. On y trouve les documents libres de droits du fonds numérisé de
la BnF, qui vont du Moyen-Âge au début du 20e siècle. Pour des raisons
de coût, les documents sont essentiellement numérisés en mode image. En
décembre 2006, les collections de Gallica comprennent 90.000 ouvrages
(fascicules de presse compris), 80.000 images et des dizaines d'heures
de ressources sonores. Gallica débute ensuite la conversion en mode
texte des livres numérisés en mode image pour favoriser l'accès à leur
contenu. En mars 2010, Gallica franchit la barre du million de
documents, dont la plupart sont accessibles gratuitement.

***

Bibliothèque numérique de la Bibliothèque nationale de France (BnF),
Gallica est inauguré en octobre 1997 avec des textes et des images du
19e siècle francophone.

On lit sur le site que ce siècle est le «siècle de l’édition et de la
presse moderne, siècle du roman mais aussi des grandes synthèses
historiques et philosophiques, siècle scientifique et technique».

# Les débuts

À l’époque, le serveur stocke 2.500 livres numérisés en mode image
complétés par les 250 livres numérisés en mode texte de la base
Frantext de l’INaLF (Institut national de la langue française).

Classés par discipline, ces livres sont complétés par une chronologie
du 19e siècle et des synthèses sur les grands courants en histoire,
sciences politiques, droit, économie, littérature, philosophie,
sciences et histoire des sciences.

Le site propose aussi un échantillon de la future iconothèque
numérique, à savoir le fonds du photographe Eugène Atget, une sélection
de documents sur l’écrivain Pierre Loti, une collection d’images de
l’École nationale des ponts et chaussées - ces images ayant trait aux
grands travaux de la révolution industrielle en France - et enfin un
choix de livres illustrés de la bibliothèque du Musée de l’Homme.

Fin 1997, Gallica se considère moins comme une banque de documents
numérisés que comme un «laboratoire dont l’objet est d’évaluer les
conditions d’accès et de consultation à distance des documents
numériques». Le but est d’expérimenter la navigation dans les
collections, en permettant le libre parcours du chercheur ou du lecteur
curieux.

# En 1998

Début 1998, Gallica annonce 100.000 volumes et 300.000 images pour la
fin 1999. Sur les 100.000 volumes prévus, qui représenteraient 30
millions de pages numérisées, plus du tiers concernerait le 19e siècle.
Quant aux 300.000 images fixes, la moitié viendrait des départements
spécialisés de la BnF (Estampes et photographie, Manuscrits, Arts du
spectacle, Monnaies et médailles, etc.), et l'autre moitié de
collections d’établissements publics (musées et bibliothèques,
Documentation française, École nationale des ponts et chaussées,
Institut Pasteur, Observatoire de Paris, etc.) ou privés (agences de
presse dont Magnum, l’Agence France-Presse, Sygma, Rapho, etc.).

En mai 1998, la BnF revoit ses espérances à la baisse et modifie
quelque peu ses orientations premières. Jérôme Strazzulla, journaliste
au quotidien Le Figaro, explique dans un article du 3 juin 1998 que la
BnF est passée «d’une espérance universaliste, encyclopédique, à la
nécessité de choix éditoriaux pointus». Dans le même article, le
président de la BnF, Jean-Pierre Angremy, rapporte la décision du
comité éditorial de Gallica: «Nous avons décidé d’abandonner l’idée
d’un vaste corpus encyclopédique de cent mille livres, auquel on
pourrait sans cesse reprocher des trous. Nous nous orientons
aujourd’hui vers des corpus thématiques, aussi complets que possibles,
mais plus restreints. (...) Nous cherchons à répondre, en priorité, aux
demandes des chercheurs et des lecteurs.»

# Les années 2000

Cinq ans plus tard, en 2003, Gallica rassemble 70.000 ouvrages et
80.000 images allant du Moyen-Âge au début du 20e siècle, tous
documents libres de droits. Mais la numérisation en mode image
n’autorise pas la recherche textuelle alors que Gallica se trouve être
la plus grande bibliothèque numérique francophone en nombre de titres
disponibles en ligne. La recherche textuelle est toutefois possible
dans les tables des matières, les sommaires et les légendes des corpus
iconographiques, qui sont numérisés en mode texte. Seule une petite
collection de livres (1.120 livres en février 2004) est intégralement
numérisée en mode texte, celle de la base Frantext, intégrée à Gallica.

Tous problèmes auxquels la BnF remédie au fil des mois, avec une
navigation plus aisée et la conversion progressive des livres du mode
image au mode texte grâce à un logiciel OCR, avec possibilité donc de
recherche textuelle.

En février 2005, Gallica compte 76.000 ouvrages. À la même date, la BnF
annonce la mise en ligne prochaine (entre 2006 et 2009) de la presse
française parue entre 1826 et 1944, à savoir 22 titres représentant 3,5
millions de pages. Début 2006, les premiers journaux disponibles en
ligne sont les quotidiens Le Figaro (fondé en 1826), La Croix (fondée
en 1883), L'Humanité (fondée en 1904) et Le Temps (fondé en 1861 et
disparu en 1942).

En mars 2010, Gallica franchit la barre du million de documents -
livres, manuscrits, cartes, images, périodiques (presse et revues),
fichiers sonores (paroles et musiques) et partitions musicales - dont
la plupart sont accessibles gratuitement sur un site dont l'interface
désormais quadrilingue (français, anglais, espagnol, portugais) n'a
cessé de s'améliorer au fil des ans. Si les documents sont en langue
française dans leur très grande majorité, on y trouve aussi des
documents en anglais, en italien, en allemand, en latin ou en grec
selon les disciplines.



1998 > DES LIVRES NUMÉRISÉS EN QUANTITÉ


[Résumé]
En 1998, qui dit livre numérique dit numérisation, la majorité des
livres existant seulement en version imprimée. Pour pouvoir être
consulté à l'écran, un livre peut être numérisé soit en mode image soit
en mode texte, les deux modes de numérisation étant complémentaires. La
numérisation en mode image consiste à scanner le livre, et correspond
donc à la photographie du livre page après page. La présentation
originale étant conservée, on peut feuilleter le livre à l’écran. La
version informatique est le fac-similé numérique de la version
imprimée. Si elle est plus économique, cette méthode ne permet pas la
recherche textuelle. La numérisation en mode texte consiste à scanner
le livre en mode image, puis à le convertir en mode texte grâce à un
logiciel OCR (Optical Character Recognition). La version informatique
du livre ne conserve pas la présentation originale du livre ou de la
page mais elle permet la recherche textuelle.

***

En 1998, qui dit livre numérique dit numérisation, la majorité des
livres existant seulement en version imprimée.

Pour pouvoir être consulté à l'écran, un livre peut être numérisé soit
en mode texte soit en mode image, les deux modes de numérisation étant
complémentaires.

# La numérisation en mode texte

Les premiers temps, la numérisation en mode texte consiste à patiemment
saisir le livre sur un clavier, page après page, solution souvent
adoptée lors de la constitution des premières bibliothèques numériques,
ou alors quand les documents originaux manquent de clarté, pour les
livres anciens par exemple.

Les années passant, la numérisation en mode texte consiste surtout à
scanner le livre en mode image, puis à le convertir en texte grâce à un
logiciel OCR (Optical Character Recognition), avec relecture éventuelle
à l’écran pour corriger le texte obtenu puisqu'un bon logiciel OCR
serait fiable à 99%.

La version informatique du livre ne conserve pas la présentation
originale du livre ou de la page. Le livre devient texte, à savoir un
ensemble de caractères apparaissant en continu à l’écran. À cause du
temps passé au traitement de chaque livre, ce mode de numérisation est
assez long, et donc nettement plus coûteux que la numérisation en mode
image. Dans de nombreux cas, il est toutefois préférable, puisqu’il
permet l’indexation, la recherche textuelle, l’analyse textuelle, une
étude comparative entre plusieurs textes ou plusieurs versions du même
texte, etc.

C’est la méthode utilisée par exemple par le Projet Gutenberg, fondé
dès 1971 et qui propose aujourd'hui la plus grande collection numérique
au format texte, avec des livres relus et corrigés à deux reprises pour
être fiables à 99,95% par rapport à la version imprimée.

# La numérisation en mode image

La numérisation en mode image consiste à scanner le livre, et
correspond donc à la photographie du livre page après page. La
présentation originale étant conservée, on peut feuilleter le livre à
l’écran. La version informatique est le fac-similé numérique de la
version imprimée.

C’est la méthode employée à la fin des années 1990 pour les programmes
de numérisation à grande échelle, par exemple celui de la Bibliothèque
nationale de France (BnF) pour alimenter sa bibliothèque numérique
Gallica. Ne sont numérisés en mode texte que les tables des matières,
les sommaires et les corpus de documents iconographiques, afin de
faciliter la recherche textuelle.

Pourquoi ne pas tout numériser en mode texte? La BnF répond en 2000 sur
le site de Gallica: «Le mode image conserve l’aspect initial de
l’original y compris ses éléments non textuels. Si le mode texte
autorise des recherches riches et précises dans un document et permet
une réduction significative du volume des fichiers manipulés, sa
réalisation, soit par saisie soit par OCR, implique des coûts de
traitement environ dix fois supérieurs à la simple numérisation. Ces
techniques, parfaitement envisageables pour des volumes limités, ne
pouvaient ici être économiquement justifiables au vu des 50.000
documents (représentant presque 15 millions de pages) mis en ligne.»

Dans les années qui suivent, Gallica convertira toutefois nombre de ses
livres du mode image au mode texte pour permettre les recherches
textuelles.

# Chaque mode de numérisation a son utilité

Concepteur de Mot@mot, logiciel de remise en page des fac-similés
numériques, Pierre Schweitzer insiste sur l’utilité des deux modes de
numérisation. Il explique en janvier 2001: «Le mode image permet
d’avancer vite et à très faible coût. C’est important car la tâche de
numérisation du domaine public est immense. Il faut tenir compte aussi
des différentes éditions: la numérisation du patrimoine a pour but de
faciliter l’accès aux oeuvres, il serait paradoxal qu’elle aboutisse à
se focaliser sur une édition et à abandonner l’accès aux autres. Chacun
des deux modes de numérisation s’applique de préférence à un type de
document, ancien et fragile ou plus récent, libre de droit ou non (pour
l’auteur ou pour l’édition), abondamment illustré ou pas. Les deux
modes ont aussi des statuts assez différents: en mode texte ça peut
être une nouvelle édition d’une oeuvre, en mode image c’est une sorte
d’"édition d’édition", grâce à un de ses exemplaires (qui fonctionne
alors comme une fonte d’imprimerie pour du papier). En pratique, le
choix dépend bien sûr de la nature du fonds à numériser, des moyens et
des buts à atteindre. Difficile de se passer d’une des deux façons de
faire.»



1998 > L'ENCYCLOPÉDIE DE DIDEROT EN LIGNE


[Résumé]
Projet commun du Centre national de la recherche scientifique (CNRS,
France) et de l'Université de Chicago (Illinois, États-Unis), le Projet
ARTFL (American and French Research on the Treasury of the French
Language)  met en ligne en 1998 la base de données du premier volume
(1751) de l'Encyclopédie de Diderot. Cette mise en ligne expérimentale
est le prélude à une base de données exhaustive comprenant
l'Encyclopédie (1751-1772) dans son entier, à savoir 17 volumes de
texte et 11 volumes de planches. Destinée à rassembler puis divulguer
les connaissances de l'époque, l'Encyclopédie porte la marque des
courants intellectuels et sociaux du Siècle des Lumières. C'est grâce à
elle que se propagent les idées nouvelles qui inspireront la Révolution
française de 1789. L’ARTFL travaille également à d’autres projets, par
exemple à une base de données exhaustive du «Dictionnaire de l’Académie
française», dont les différentes éditions s’échelonnent entre 1694 et
1935.

***

En 1998, le Projet ARTFL met en ligne la base de données du premier
volume (1751) de l'Encyclopédie de Diderot.

Cette mise en ligne expérimentale est le prélude à une base de données
exhaustive comprenant l'Encyclopédie dans son entier, à savoir 17
volumes de texte et 11 volumes de planches.

L’ARTFL (American and French Research on the Treasury of the French
Language) est un projet commun du CNRS (Centre national de la recherche
scientifique) en France et de l'Université de Chicago dans l’Illinois
(États-Unis).

Monumental ouvrage de référence pour les arts et les sciences, la
première édition (1751-1772) de l'«Encyclopédie ou Dictionnaire
raisonné des sciences, des métiers et des arts» de Diderot et
d'Alembert comprend 72.000 articles rédigés par 140 collaborateurs,
dont Voltaire, Rousseau, d'Alembert, Marmontel, d'Holbach, Turgot, etc.

Destinée à rassembler puis divulguer les connaissances de l'époque,
l'Encyclopédie porte la marque des courants intellectuels et sociaux du
Siècle des Lumières. C'est grâce à elle que se propagent les idées
nouvelles qui inspireront la Révolution française de 1789. Les 17
volumes de texte représentent 18.000 pages et 21,7 millions de mots.
Les 11 volumes de planches présentent des planches techniques de telle
qualité que certaines font toujours référence en 2011.

Dans l’Encyclopédie, Diderot explique lui-même que «le but d’une
encyclopédie est de rassembler les connaissances éparses sur la surface
de la terre, d’en exposer le système général aux hommes avec qui nous
vivons et de le transmettre aux hommes qui viendront après nous, afin
(…) que nos neveux, devenant plus instruits, deviennent en même temps
plus vertueux et plus heureux, et que ne mourions pas sans avoir bien
mérité du genre humain.» Un beau texte qui figure aussi sur le mur de
l’Allée de l’Encyclopédie, l’une des grandes artères de la Bibliothèque
nationale de France (BnF).

La base de données correspondant au premier volume est accessible en
ligne à titre expérimental en 1998. La recherche est possible par mot,
portion de texte, auteur ou catégorie, ou en combinant ces critères
entre eux. On dispose de renvois d'un article à l'autre par le biais de
liens hypertextes permettant d'aller d'une planche au texte ou du texte
au fac-similé des pages originales.

L'automatisation complète des procédures de saisie entraîne des erreurs
typographiques et des erreurs d'identification qui sont corrigées au
fil des mois. La recherche d'images est également possible dans un
deuxième temps.

L'ARTFL travaille également à une base de données exhaustive du
«Dictionnaire de l'Académie française», dont les différentes éditions
s’échelonnent entre 1694 et 1935. La première édition (1694) et la
cinquième édition (1798) du dictionnaire sont les premières à être
disponibles en ligne, avec possibilité de recherche par mot puis par
portion de texte. Les différentes éditions sont ensuite combinées dans
une base de données unique, qui permet de juger de l'évolution d'un
terme en consultant aussi bien une édition donnée que l'ensemble des
éditions.

D’autres projets de l'ARTFL concernent par exemple le «Dictionnaire
historique et critique» de Philippe Bayle dans son édition de 1740, le
«Roget's Thesaurus» de 1911, le «Webster's Revised Unabridged
Dictionary» de 1913, le «Thresor de la langue française» de Jean Nicot
imprimé en 1606, ou encore un projet biblique multilingue comprenant
entre autres «La Bible française» de Louis Segond, publiée en 1910. Il
s’agit là encore de bases de données avec moteur de recherche. La
technologie au service de la littérature, donc.



1998 > 00h00, ÉDITEUR EN LIGNE


[Résumé]
Les éditions 00h00 («zéro heure») sont fondées en mai 1998 par Jean-
Pierre Arbon et Bruno de Sa Moreira, en tant que premier éditeur «en
ligne», à savoir un éditeur fabriquant des livres numériques et les
vendant via l'internet. En 2000, le catalogue comprend 600 titres. Les
versions numériques (au format PDF) représentent 85% des ventes, les
15% restants étant des versions imprimées à la demande du client, un
service que l’éditeur procure en complément. Sur le site au très beau
design, les internautes/lecteurs peuvent créer leur espace personnel
pour y rédiger leurs commentaires, participer à des forums, s'abonner à
la lettre d'information ou regarder les clips littéraires produits par
l'éditeur pour présenter les nouveautés. En septembre 2000, 00h00 est
racheté par Gemstar, société américaine de produits et services
numériques pour les médias. Gemstar met fin à l’ensemble de ses
activités eBook en juin 2003.

***

Les éditions 00h00 font leur apparition en mai 1998, un peu moins de
deux ans après CyLibris, premier éditeur électronique commercial.

Le champ d’action de 00h00 est un peu différent de celui de CyLibris
puisqu’il s’agit non seulement d’un éditeur électronique mais aussi
d’un éditeur «en ligne». Son activité est en effet de fabriquer et
vendre des livres numériques via l'internet, et non des livres imprimés
comme CyLibris.

En 2000, les versions numériques (au format PDF) représentent 85% des
ventes, les 15% restants étant des versions imprimées à la demande du
client, un service que l'éditeur procure en complément.

# Les débuts

00h00 («zéro heure») est fondé par Jean-Pierre Arbon et Bruno de Sa
Moreira, respectivement ancien directeur général de Flammarion et
ancien directeur de Flammarion Multimédia.

Bruno de Sa Moreira explique en juillet 1998: «Aujourd’hui mon activité
professionnelle est 100% basée sur internet. Le changement ne s’est pas
fait radicalement, lui, mais progressivement (audiovisuel puis
multimédia puis internet). (…) La gestation de 00h00 a duré un an:
brainstorming, faisabilité, création de la société et montage
financier, développement technique du site et informatique éditoriale,
mise au point et production des textes et préparation du catalogue à
l’ouverture. (...) Nous faisons un pari, mais l’internet me semble un
média capable d’une très large popularisation, sans doute grâce à des
terminaux plus faciles d’accès que le seul micro-ordinateur.»

# L’internet, «un lieu sans passé»

On lit sur le site web que «la création de 00h00 marque la véritable
naissance de l’édition en ligne. C’est en effet la première fois au
monde que la publication sur internet de textes au format numérique est
envisagée dans le contexte d’un site commercial, et qu’une entreprise
propose aux acteurs traditionnels de l’édition (auteurs et éditeurs)
d’ouvrir avec elle sur le réseau une nouvelle fenêtre d’exploitation
des droits. Les textes offerts par 00h00 sont soit des inédits, soit
des textes du domaine public, soit des textes sous copyright dont les
droits en ligne ont fait l’objet d’un accord avec leurs ayants droit.
(…) Avec l’édition en ligne émerge probablement une première vision de
l’édition au 21e siècle. C’est cette idée d’origine, de nouveau départ
qui s’exprime dans le nom de marque, 00h00. (…)

Internet est un lieu sans passé, où ce que l’on fait ne s’évalue pas
par rapport à une tradition. Il y faut inventer de nouvelles manières
de faire les choses. (...) Le succès de l’édition en ligne ne dépendra
pas seulement des choix éditoriaux: il dépendra aussi de la capacité à
structurer des approches neuves, fondées sur les lecteurs autant que
sur les textes, sur les lectures autant que sur l’écriture, et à rendre
immédiatement perceptible qu’une aventure nouvelle a commencé.»

# Des collections diverses

Les collections sont diverses: inédits, théâtre classique français,
contes et récits fantastiques, contes et récits philosophiques,
souvenirs et mémoires, philosophie classique, réalisme et naturalisme,
cyberculture, romans d’enfance, romans d’amour, nouvelles et romans
d’aventure. Le recherche est possible par auteur, par titre et par
genre. Pour chaque livre, on a un descriptif court, un descriptif
détaillé, la table des matières et une courte présentation de l’auteur.
S’y ajoutent ensuite les commentaires des lecteurs. Pas de stock, pas
de contrainte physique de distribution, mais un lien direct avec le
lecteur et entre les lecteurs. Sur le site, les internautes/lecteurs
peuvent créer leur espace personnel pour y rédiger leurs commentaires,
participer à des forums ou recommander des liens vers d’autres sites.
Ils peuvent aussi s’abonner à la lettre d’information de 00h00 ou
regarder les clips littéraires produits par l’éditeur pour présenter
des nouveautés.

En 2000, le catalogue comprend 600 titres, à savoir une centaine
d’oeuvres originales et des rééditions électroniques de livres publiés
par d’autres éditeurs. Les oeuvres originales sont réparties en
plusieurs rubriques: nouvelles écritures interactives et
hypertextuelles, premiers romans, documents d’actualité, études sur les
NTIC (nouvelles technologies de l’information et de la communication),
co-éditions avec des éditeurs traditionnels ou de grandes institutions.
Le paiement est fait en ligne grâce à un système sécurisé mis en place
par la Banque populaire. Ceux que le paiement en ligne rebute peuvent
régler leur commande par carte bancaire (envoi par fax) ou par chèque
(envoi par courrier postal).

# Le rachat par Gemstar

En septembre 2000, 00h00 est racheté par Gemstar-TV Guide
International, grande société américaine de produits et services
numériques pour les médias. Quelques mois auparavant, en janvier 2000,
Gemstar rachète les deux sociétés californiennes ayant lancé les
premières tablettes de lecture, la société NuvoMedia, créatrice du
Rocket eBook, et la société SoftBook Press, créatrice du SoftBook
Reader.

Selon un communiqué de presse citant Henry Yuen, président de Gemstar,
«les compétences éditoriales dont dispose 00h00 et ses capacités
d’innovation et de créativité sont les atouts nécessaires pour faire de
Gemstar un acteur majeur du nouvel âge de l’édition numérique qui
s’ouvre en Europe.»

La communauté francophone ne voit pas ce rachat d’un très bon oeil, la
mondialisation de l’édition semblant justement peu compatible avec
l’innovation et la créativité. Moins de trois ans plus tard, en juin
2003, 00h00 cesse définitivement ses activités, tout comme la branche
eBook de Gemstar.

Il reste le souvenir d’une belle aventure. En octobre 2006, Jean-Pierre
Arbon, devenu chanteur, raconte sur son site: «J’avais fondé, avec
Bruno de Sa Moreira, une maison d’édition d’un genre nouveau, la
première au monde à tenter à grande échelle l’aventure de l’édition en
ligne. Tout était à faire, à inventer. L’édition numérique était terra
incognita: on explorait, on défrichait.»



1998 > UN PROLONGEMENT SUR LE WEB POUR LES LIVRES


[Résumé]
Murray Suid est l’auteur de livres pédagogiques, de livres pour
enfants, d’oeuvres multimédias et de scénarios. Dès septembre 1998, il
préconise une solution adoptée depuis par de nombreux auteurs, à savoir
compléter ses livres imprimés par une version web, pour pouvoir les
actualiser sans attendre une nouvelle édition imprimée. En octobre
2000, l’intégralité de ses oeuvres multimédias, auparavant disponibles
sur CD-Rom, est sur le réseau. Le matériel pédagogique auquel il
contribue est conçu non plus pour diffusion sur CD-Rom, mais pour
diffusion sur le web. D’entreprise multimédia, EDVantage Software, la
société de logiciels éducatifs qui emploie Murray, est devenue une
entreprise internet qui distribue toutes ses publications en ligne
auprès des étudiants et des enseignants.

***

Murray Suid vit à Palo Alto, dans la Silicon Valley, en Californie. Il
est l’auteur de livres pédagogiques, de livres pour enfants, d’œuvres
multimédias et de scénarios.

En septembre 1998, il préconise une solution choisie depuis par de
nombreux auteurs: «Un livre peut avoir un prolongement sur le web - et
donc vivre en partie dans le cyberespace. L’auteur peut ainsi aisément
l’actualiser et le corriger, alors qu’auparavant il devait attendre
longtemps, jusqu’à l’édition suivante, quand il y en avait une. (...)

Je ne sais pas si je publierai des livres sur le web, au lieu de les
publier en version imprimée. J’utiliserai peut-être ce nouveau support
si les livres deviennent multimédias. Pour le moment, je participe au
développement de matériel pédagogique multimédia. C’est un nouveau type
de matériel qui me plaît beaucoup et qui permet l’interactivité entre
des textes, des films, des bandes sonores et des graphiques qui sont
tous reliés les uns aux autres.»

Un an plus tard, en août 1999, il ajoute: «En plus des livres complétés
par un site web, je suis en train d’adopter la même formule pour mes
oeuvres multimédias - qui sont sur CD-ROM - afin de les actualiser et
d’enrichir leur contenu.»

En octobre 2000, l’intégralité de ses oeuvres multimédias est sur le
réseau. Le matériel pédagogique auquel il contribue est conçu non plus
pour diffusion sur CD-Rom, mais pour diffusion sur le web. D’entreprise
multimédia, EDVantage Software, la société de logiciels éducatifs qui
emploie Murray, est devenue une entreprise internet qui distribue
désormais toutes ses publications en ligne.



1998 > UN DURCISSEMENT DU COPYRIGHT


[Résumé]
Aux États-Unis, un nouvel amendement de la loi sur le copyright réduit
encore un peu plus le domaine public, au grand dam de tous ceux qui
sont en train de constituer des bibliothèques numériques. Cet
amendement est entériné le 27 octobre 1998 par le Congrès pour contrer
le formidable véhicule de diffusion qu'est l'internet. Contradiction
flagrante, les instances politiques n'ont de cesse de parler d'Âge de
l'Information tout en durcissant la réglementation relative à la
diffusion de l’information. Le copyright est passé d'une durée de 30
ans en moyenne en 1909 à une durée de 95 ans en moyenne en 1998. En 90
ans, de 1909 à 1998, le copyright a subi une extension de 65 ans qui
affecte les trois quarts de la production du 20e siècle. Seul un livre
publié avant 1923 peut désormais être considéré avec certitude comme
appartenant au domaine public. Un durcissement similaire affecte
ensuite l’Union Européenne.

***

En 1998, un nouvel amendement de la loi sur le copyright réduit encore
un peu plus le domaine public aux États-Unis. Un durcissement similaire
affecte ensuite l’Union européenne.

Cet amendement est entériné le 27 octobre 1998 par le Congrès pour
contrer le formidable véhicule de diffusion qu'est l'internet, au grand
dam de tous ceux qui sont en train de constituer des bibliothèques
numériques.

Les instances politiques n'ont de cesse de parler d'Âge de
l'Information tout en réduisant l’accès à cette information. La
contradiction est flagrante. Le copyright est passé d'une durée de 30
ans en moyenne en 1909 à une durée de 95 ans en moyenne en 1998. En 90
ans, de 1909 à 1998, le copyright a subi une extension de 65 ans qui
affecte les trois quarts de la production du 20e siècle. Seul un livre
publié avant 1923 peut désormais être considéré avec certitude comme
appartenant au domaine public.

# Une claque pour les bibliothèques numériques

De nombreuses oeuvres censées tomber dans le domaine public restent
finalement sous copyright, au grand dam de Michael Hart, fondateur du
Projet Gutenberg, de John Mark Ockerbloom, créateur de l’Online Books
Page, et de bien d'autres. La législation de 1998 porte un coup très
rude aux bibliothèques numériques, en plein essor avec le développement
du web. Nombre de titres doivent être retirés des collections.

Pour ne prendre qu'un exemple, le classique mondial «Gone with the
wind» (Autant en emporte le vent) de Margaret Mitchell, publié en 1939,
aurait dû tomber dans le domaine public au bout de 56 ans, en 1995,
conformément à la législation de l'époque, libérant ainsi les droits
pour les adaptations en tous genres. Suite aux législations de 1976 et
1998, ce classique ne devrait désormais tomber dans le domaine public
qu'en 2035.

Michael Hart explique en juillet 1999: «Le copyright a été augmenté de
20 ans. Auparavant on devait attendre 75 ans, on est maintenant passé à
95 ans. Bien avant, le copyright durait 28 ans (plus une extension de
28 ans si on la demandait avant l’expiration du délai) et, avant cela,
le copyright durait 14 ans (plus une extension de 14 ans si on la
demandait avant l’expiration du délai). Comme on le voit, on assiste à
une dégradation régulière et constante du domaine public. (…) J’ai été
le principal opposant aux extensions du copyright, mais Hollywood et
les grands éditeurs ont fait en sorte que le Congrès ne mentionne pas
mon action en public. Les débats actuels sont totalement irréalistes.
Ils sont menés par “l’aristocratie terrienne de l’Âge de l’Information”
et servent uniquement ses intérêts. Un Âge de l’Information? Et pour
qui?»

John Mark Ockerbloom explique en août 1999: «À mon avis, il est
important que les internautes comprennent que le copyright est un
contrat social conçu pour le bien public - incluant à la fois les
auteurs et les lecteurs. Ceci signifie que les auteurs doivent avoir le
droit d'utiliser de manière exclusive et pour un temps limité les
oeuvres qu'ils ont créées, comme ceci est spécifié dans la loi actuelle
sur le copyright. Mais ceci signifie également que leurs lecteurs ont
le droit de copier et de réutiliser ce travail autant qu'ils le veulent
à l'expiration de ce copyright. Aux États-Unis, on voit maintenant
diverses tentatives visant à retirer ces droits aux lecteurs, en
limitant les règles relatives à l'utilisation de ces oeuvres, en
prolongeant la durée du copyright (y compris avec certaines
propositions visant à le rendre permanent) et en étendant la propriété
intellectuelle à des travaux distincts des oeuvres de création (comme
on en trouve dans les propositions de copyright pour les bases de
données).»

#  Dans l’Union européenne

Un durcissement similaire touche les pays de l'Union européenne. La
règle générale est désormais un copyright de 70 ans après le décès de
l’auteur, alors qu’il était auparavant de 50 ans, suite aux pressions
exercées par les éditeurs de contenu sous prétexte d’«harmoniser» les
lois nationales régissant le droit d'auteur pour répondre à la
mondialisation du marché.

À ceci s'ajoute la législation sur le copyright des éditions numériques
en application des traités internationaux de l'OMPI (Organisation
mondiale de la propriété intellectuelle). Ces traités sont signés en
1996 dans l'optique de contrôler la gestion des droits numériques, à la
suite de quoi le Digital Millenium Copyright Act (DMCA) est entériné en
octobre 1998 aux États-Unis et la directive EUCD (European Union
Copyright Directive) est entérinée en mai 2001 par la Communauté
européenne.

La directive EUCD s'intitule très précisément «Directive 2001/29/EC du
Parlement européen et du Conseil sur l'harmonisation de certains
aspects du droit d'auteur et des droits voisins dans la société de
l'information». Elle fait suite à la directive de février 1993
(Directive 93/98/EEC) qui visait à harmoniser les législations des
différents pays en matière de protection du droit d'auteur. La
directive EUCD entre peu à peu en vigueur dans tous les pays de l'Union
européenne, avec mise en place de législations nationales, le but
officiel étant de renforcer le respect du droit d'auteur sur l'internet
et de contrer ainsi le piratage. En France, par exemple, la loi DADVSI
(Droit d'auteur et droits voisins dans la société de l'information) est
promulguée en août 2006, et n'est pas sans susciter de nombreux remous.



1998 > LES PREMIÈRES TABLETTES DE LECTURE


[Résumé]
Les premières tablettes de lecture sont développées dans la Silicon
Valley, en Californie. Le Rocket eBook est lancé en 1998 par NuvoMedia,
société financée par la chaîne de librairies Barnes & Noble et le géant
des médias Bertelsmann. Peu après, le SoftBook Reader est lancé par
SoftBook Press, société financée par les deux grandes maisons d'édition
Random House et Simon & Schuster. Ces tablettes de lecture fonctionnent
sur batteries et disposent d'un écran à cristaux liquides (ou écran
LCD: Liquid Cristal Display) noir et blanc, avec une capacité de
stockage d’une dizaine de livres. L’usager se connecte à l’internet
soit par le biais d’un ordinateur (comme le Rocket eBook) soit
directement grâce à un modem intégré (comme le SoftBook Reader) pour
télécharger des livres à partir des librairies numériques présentes sur
les sites des sociétés. D’autres modèles suivent, comme l’EveryBook
Reader de la société EveryBook ou le Millennium eBook de la société
Librius.

***

Développées dans la Silicon Valley, en Californie, et commercialisées
en 1998, les premières tablettes de lecture sont le Rocket eBook et le
SoftBook Reader.

Alors qu’elles étaient jusque-là l’apanage des films de science-
fiction, ces tablettes électroniques dédiées ont la taille d'un (gros)
livre et sont souvent appelées ebooks, livres électroniques, tablettes
de lecture ou même liseuses.

Elles suscitent un engouement certain, même si peu de gens vont jusqu'à
les acheter, vu leur prix prohibitif - plusieurs centaines de dollars -
et un choix de titres restreint, le catalogue de livres numériques
étant encore ridicule par rapport à la production imprimée. Les
éditeurs commencent tout juste à produire des livres en version
numérique et se demandent encore comment les commercialiser, la plupart
étant tétanisés par les risques de piratage.

Ces tablettes fonctionnent sur batteries et disposent d'un écran à
cristaux liquides (écran LCD: Liquid Cristal Display) noir et blanc
rétro-éclairé ou non, avec une capacité de stockage d’une dizaine de
livres. L’usager se connecte à l’internet soit par le biais d’un
ordinateur (comme le Rocket eBook) soit directement grâce à un modem
intégré (comme le SoftBook Reader) pour télécharger des livres à partir
de librairies numériques présentes sur les sites des sociétés.

# Le Rocket eBook

Premier modèle du marché, le Rocket eBook est lancé en 1998 par
NuvoMedia, société créée en 1997 à Palo Alto, dans la Silicon Valley,
et financée par la chaîne de librairies Barnes & Noble et le géant des
médias Bertelsmann. NuvoMedia souhaite devenir «la solution pour
distribuer des livres électroniques en procurant une infrastructure
réseau aux éditeurs, distributeurs et usagers afin de publier,
distribuer, acheter et lire un contenu électronique de manière
sécurisée et efficace sur l’internet» (extrait du site web). La
connexion entre le Rocket eBook et l’ordinateur (PC ou Macintosh) se
fait par le biais du Rocket eBook Cradle, un périphérique à deux
câbles, d’une part un câble pour se connecter à une prise électrique
par le biais d’un adaptateur, d’autre part un câble série pour se
connecter à l’ordinateur.

# Le SoftBook Reader

Deuxième modèle du marché, disponible peu après le Rocket eBook, le
SoftBook Reader est lancé par SoftBook Press, société financée par les
deux grandes maisons d'édition Random House et Simon & Schuster. Le
SoftBook Reader s’appuie sur le SoftBook Network, «un service de
distribution de contenu basé sur l’internet».  Selon son site web,
cette tablette permet aux lecteurs de  «télécharger facilement,
rapidement et de manière sécurisée un large choix de livres et de
revues grâce à sa connexion internet intégrée». Contrairement à
l’ordinateur, le SoftBook Reader possède «une ergonomie conçue pour la
lecture de longs documents et de livres».

# D’autres modèles suivent

D’autres tablettes de lecture sont lancées en 1999, par exemple
l’EveryBook Reader et le Millennium eBook (le nouveau millénaire
approche).

L’EveryBook Reader est un appareil à double écran lancé par la société
EveryBook, ou encore «une bibliothèque vivante dans un simple livre»
pouvant stocker 50 livres numériques, avec un modem intégré permettant
l’accès à l’EveryBook Store, afin de «consulter, acheter et recevoir le
texte intégral de livres, magazines et partitions de musique».

Le Millenium eBook est une tablette de lecture «petite et bon marché»
lancée par la société Librius, «une société de commerce électronique
procurant un service complet». Sur le site de la société, un World
Bookstore propose «des copies numériques de milliers de livres»
disponibles via l’internet.

Toutes ces tablettes pèsent entre 700 grammes et deux kilos. Il faudra
attendre le tournant du millénaire pour voir apparaître de nouveaux
modèles ayant une durée de vie légèrement plus longue, par exemple le
Gemstar eBook lancé en novembre 2000 aux États-Unis et le Cybook
(première génération, celui de Cytale) lancé en janvier 2001 en Europe.



1999 > DU BIBLIOTHÉCAIRE AU CYBERTHÉCAIRE


[Résumé]
Piloter les usagers sur l’internet, filtrer et organiser l’information
à leur intention, créer et gérer un site web, rechercher des documents
dans des bases de données spécialisées ou actualiser des catalogues en
ligne, telles sont désormais les tâches de nombreux bibliothécaires.
Bruno Didier, webmestre de la bibliothèque de l’Institut Pasteur
(Paris), explique en août 1999: «Nous devenons de plus en plus des
médiateurs, et peut- être un peu moins des conservateurs. Mon activité
actuelle est typique de cette nouvelle situation: d’une part dégager
des chemins d’accès rapides à l’information et mettre en place des
moyens de communication efficaces, d’autre part former les utilisateurs
à ces nouveaux outils. Je crois que l’avenir de notre métier passe par
la coopération et l’exploitation des ressources communes. C’est un
vieux projet certainement, mais finalement c’est la première fois qu’on
dispose enfin des moyens de le mettre en place.»

***

Piloter les usagers sur l’internet, filtrer et organiser l’information
à leur intention, créer et gérer un site web, rechercher des documents
dans des bases de données spécialisées ou actualiser des catalogues en
ligne, telles sont désormais les tâches de nombreux bibliothécaires.

C’est le cas par exemple de Peter Raggett à la bibliothèque centrale de
l’OCDE et de Bruno Didier à la bibliothèque de l’Institut Pasteur à
Paris.

# À la bibliothèque centrale de l’OCDE

Peter Raggett est sous-directeur (puis directeur) de la bibliothèque
centrale de l’OCDE (Organisation de coopération et de développement
économiques) à Paris. Réservée aux fonctionnaires de l’organisation, la
bibliothèque comprend 60.000 monographies et 2.500 périodiques imprimés
en 1998, et permet aussi la consultation de microfilms, de CD-Rom et de
bases de données telles que Dialog, Lexis-Nexis et UnCover. La
bibliothèque lance en 1996 ses pages intranet, qui deviennent
rapidement une source d’information majeure pour les chercheurs.

Peter explique en août 1999: «Je dois filtrer l’information pour les
usagers de la bibliothèque, ce qui signifie que je dois bien connaître
les sites et les liens qu’ils proposent. J’ai sélectionné plusieurs
centaines de sites pour en favoriser l’accès à partir de l’intranet de
l’OCDE. Cette sélection fait partie du bureau de référence virtuel
proposé par la bibliothèque à l’ensemble du personnel. Outre de
nombreux liens, ce bureau de référence contient des pages recensant les
articles, monographies et sites web correspondant aux différents
projets de recherche en cours à l’OCDE, l’accès en réseau aux CD-Rom et
une liste mensuelle des nouveaux livres achetés par la bibliothèque.»

En ce qui concerne la recherche d’informations, «l’internet offre aux
chercheurs un stock d’informations considérable. Le problème pour eux
est de trouver ce qu’ils cherchent. Jamais auparavant on n’avait senti
une telle surcharge d’informations, comme on la sent maintenant quand
on tente de trouver un renseignement sur un sujet précis en utilisant
les moteurs de recherche disponibles sur l’internet. Lorsqu’on utilise
un moteur de recherche comme Lycos ou AltaVista ou un répertoire comme
Yahoo!, on voit vite la difficulté de trouver des sites utiles sur un
sujet donné. La recherche fonctionne bien sur un sujet très précis, par
exemple si on veut des informations sur une personne au nom inhabituel,
mais elle donne un trop grand nombre de résultats si on veut des
informations sur un sujet assez vaste. Par exemple, si on lance une
recherche sur le web pour “Russie ET transport”, dans le but de trouver
des statistiques sur l’utilisation des trains, des avions et des bus en
Russie, les premiers résultats qu’on trouve sont les compagnies de
transport de fret qui ont des relations d’affaires avec la Russie.»

Comment Peter voit-il l’avenir de la profession? «À mon avis, les
bibliothécaires auront un rôle important à jouer pour améliorer la
recherche et l’organisation de l’information sur le réseau. Je prévois
aussi une forte expansion de l’internet pour l’enseignement et la
recherche. Les bibliothèques seront amenées à créer des bibliothèques
numériques permettant à un étudiant de suivre un cours proposé par une
institution à l’autre bout du monde. La tâche du bibliothécaire sera de
filtrer les informations pour le public. Personnellement, je me vois de
plus en plus devenir un bibliothécaire virtuel. Je n’aurai pas
l’occasion de rencontrer les usagers, ils me contacteront plutôt par
courriel, par téléphone ou par fax, j’effectuerai la recherche et je
leur enverrai les résultats par voie électronique.»

# À la bibliothèque de l’Institut Pasteur

Bruno Didier est bibliothécaire à l’Institut Pasteur à Paris.
L’Institut Pasteur est une fondation privée spécialisée dans la
prévention et le traitement des maladies infectieuses, avec plusieurs
instituts dans le monde. Séduit par les perspectives qu’offre
l’internet pour la recherche documentaire, Bruno crée le site web de la
bibliothèque en 1996 et devient son webmestre.

Il explique en août 1999: «Le site web de la bibliothèque a pour
vocation principale de servir la communauté pasteurienne. Il est le
support d’applications devenues indispensables à la fonction
documentaire dans un organisme de cette taille: bases de données
bibliographiques, catalogue, commande de documents et bien entendu
accès à des périodiques en ligne (un peu plus d’une centaine
actuellement). C’est également une vitrine pour nos différents
services, en interne mais aussi dans toute la France et à l’étranger.
Il tient notamment une place importante dans la coopération
documentaire avec les instituts du réseau Pasteur à travers le monde.
Enfin j’essaie d’en faire une passerelle adaptée à nos besoins pour la
découverte et l’utilisation d’internet. (...) Je développe et maintiens
les pages du serveur, ce qui s’accompagne d’une activité de veille
régulière. Par ailleurs je suis responsable de la formation des
usagers, ce qui se ressent dans mes pages. Le web est un excellent
support pour la formation, et la plupart des réflexions actuelles sur
la formation des usagers intègrent cet outil.»

Son activité professionnelle a changé de manière radicale, tout comme
celle de ses collègues. «C’est à la fois dans nos rapports avec
l’information et avec les usagers que les changements ont eu lieu,
explique-t-il. Nous devenons de plus en plus des médiateurs, et peut-
être un peu moins des conservateurs. Mon activité actuelle est typique
de cette nouvelle situation: d’une part dégager des chemins d’accès
rapides à l’information et mettre en place des moyens de communication
efficaces, d’autre part former les utilisateurs à ces nouveaux outils.
Je crois que l’avenir de notre métier passe par la coopération et
l’exploitation des ressources communes. C’est un vieux projet
certainement, mais finalement c’est la première fois qu’on dispose
enfin des moyens de le mettre en place.»



1999 > LA LIBRAIRIE ULYSSE SUR LE WEB


[Résumé]
Fondée par Catherine Domain en 1971 au coeur de Paris, dans l’île
Saint-Louis, la librairie Ulysse est la plus ancienne librairie au
monde uniquement consacrée au voyage, avec plus de 20.000 livres,
cartes et revues, neufs et anciens, qui recèlent des trésors
introuvables ailleurs. En 1999, Catherine crée elle-même le site de sa
librairie, en s’initiant en même temps à l’informatique. Elle raconte
en novembre 2000: «Mon site est embryonnaire et en construction. Il se
veut à l'image de ma librairie, un lieu de rencontre avant d'être un
lieu commercial. Il sera toujours en perpétuel devenir! Internet me
prend la tête, me bouffe mon temps et ne me rapporte presque rien mais
cela ne m'ennuie pas... Internet tue les librairies spécialisées. En
attendant d'être dévorée, je l'utilise comme un moyen d'attirer les
clients chez moi, et aussi de trouver des livres pour que ceux qui
n'ont pas encore internet chez eux! Mais j'ai peu d'espoir...» Dix ans
plus tard, en avril 2010, elle est beaucoup plus optimiste puisque
l’internet lui permet d’être éditrice de livres de voyages.

***

Fondée par Catherine Domain en 1971 au coeur de Paris, la librairie
Ulysse est la plus ancienne librairie au monde uniquement consacrée au
voyage. Ulysse crée son site web en 1999 et sa maison d’édition en
2010.

Nichée dans l’île Saint-Louis, entourée par la Seine, la librairie
propose 20.000 livres, cartes et revues neufs et d’occasion, qui
recèlent des trésors introuvables ailleurs.

# Les débuts

Catherine raconte sur le site de la librairie: «Au terme de dix années
de voyages sur tous les continents, je me suis arrêtée et me suis dit:
"Que vais-je bien pouvoir faire pour vivre?" Consciente de la nécessité
de  m'insérer dans une société d'une façon ou d'une autre, j'ai procédé
à un choix par déduction et par le refus d'avoir patron et employé.

Me souvenant de mes grands-pères, l'un navigateur au long cours,
l'autre libraire en Périgord, et constatant que j'étais obligée de
visiter une quinzaine de librairies avant de trouver la moindre
documentation sur un pays aussi proche que la Grèce, une "librairie de
voyage" s'est imposée à mon esprit entre Colombo et Surabaya, au cours
d'un tour du monde.

De retour à Paris - j'habitais déjà l'Île Saint-Louis - je cherche un
local, me renseigne sur le métier de libraire, fais des stages, prépare
des fiches et cherche un nom pour cette future entreprise.

Un matin, en allant chercher le journal, je lève le nez sur la
librairie "Ulysse", référence à Joyce, au 35 de la rue Saint-Louis-en-
l'Île.  "Voilà un nom!", me dis -je. Je gravis les deux marches et
entre dans cette boutique de 16m2 à poutre unique. Quatre types jouent
au poker. "Elle est mignonne votre librairie, " [dis-je]. "Elle est à
vendre", me rétorque l’un des joueurs sans lever le nez. 48 heures
après, j'étais libraire. C'était en septembre 1971. La première
librairie spécialisée dans les voyages au monde était née.

Vingt ans plus tard, victime de la promotion immobilière comme
beaucoup, j'ai dû déménager. Fort heureusement, mon côté entêté - je
suis taureau ascendant taureau - m'a permis de transporter la librairie
à quelques mètres dans un local plus vaste, 26 rue Saint-Louis-en-
l'Île,  dans un immeuble peu anodin puisque c'est non seulement là où
j'ai commencé par habiter dans l'Île Saint-Louis mais aussi parce que,
anciennement succursale de banque, ce local fut le théâtre du très
célèbre casse de Spaggiari.»

# En 1999

À la fois libraire et grande voyageuse - elle continue de voyager tous
les étés pendant que son compagnon tient la librairie - Catherine est
membre du Syndicat national de la librairie ancienne et moderne (SLAM),
du Club des explorateurs et du Club international des grands voyageurs.
Elle navigue souvent sur la Méditerranée, l’Atlantique ou le Pacifique.

Début 1999, elle décide de se lancer dans un voyage autrement plus
ingrat, virtuel cette fois, à savoir la réalisation d’un site web en
autodidacte alors que ses connaissances en informatique sont très
sommaires.

Elle raconte fin 1999 lors d’un entretien par courriel: «Mon site est
embryonnaire et en construction. Il se veut à l’image de ma librairie,
un lieu de rencontre avant d’être un lieu commercial. Il sera toujours
en perpétuel devenir! Internet me prend la tête, me bouffe mon temps et
ne me rapporte presque rien, mais cela ne m’ennuie pas... »

Elle est toutefois pessimiste sur l’avenir des librairies comme la
sienne. «Internet tue les librairies spécialisées. En attendant d’être
dévorée, je l’utilise comme un moyen d’attirer les clients chez moi, et
aussi de trouver des livres pour ceux qui n’ont pas encore internet
chez eux! Mais j’ai peu d’espoir...»

# En 2005

En 2005, Catherine crée toutefois une deuxième librairie de voyage à
Hendaye, sur la côte sud de l’Atlantique, avec une vue imprenable sur
l’océan. Ouverte du 20 juin au 20 septembre, cette librairie est située
le long de la plage dans un bâtiment mauresque qui se trouve être
l’ancien casino et qui est classé monument historique. À marée haute,
la librairie est «comme un paquebot de livres qui va prendre la mer,
qu'elle prend quelquefois d'ailleurs.»

# En 2010

Dix ans après la mise en ligne de son site web, Catherine est beaucoup
moins critique à l’égard de l’internet puisque le réseau lui permet de
lancer sa propre maison d'édition en avril 2010 pour publier des livres
de voyage.

Elle écrit à la même date: «Internet a pris de plus en plus de place
dans ma vie! Il me permet d'être éditeur grâce à de laborieuses
formations Photoshop, InDesign et autres.

C'est une grande joie de constater que la volonté politique de garder
le pékin devant son ordinateur afin qu'il ne fasse pas la révolution
peut être mise en échec par des apéros géants et spontanés de milliers
de personnes [organisés via Facebook] qui veulent se voir et se parler
en vrai.

Décidément il y aura toujours des rebondissements inattendus aux
inventions, entre autres. Quand j'ai commencé à utiliser l'internet, je
ne m'attendais vraiment pas à devenir éditeur.»



1999 > L’INTERNET, PERSONNAGE DE ROMAN


[Résumé]
Alain Bron est consultant en systèmes d'information et écrivain.
L'internet est l’un des personnages de son deuxième roman, «Sanguine
sur toile», disponible en version imprimée aux éditions du Choucas en
1999, puis en version numérique (au format PDF) aux éditions 00h00 en
2000, et qui reçoit le prix du Lions Club International la même année.
Quel est le thème de ce roman? L’auteur raconte en novembre 1999 : «La
"toile", c'est celle du peintre, c'est aussi l'autre nom d'internet: le
web - la toile d'araignée. "Sanguine" évoque le dessin et la mort
brutale. Mais l'amour des couleurs justifierait-il le meurtre?
"Sanguine sur toile" évoque l'histoire singulière d'un internaute pris
dans la tourmente de son propre ordinateur, manipulé à distance par un
très mystérieux correspondant qui n'a que vengeance en tête.»

***

L'internet est l’un des personnages de «Sanguine sur toile», deuxième
roman d’Alain Bron, publié en version imprimée par le Choucas en 1999,
puis en version numérique par 00h00 en 2000, et qui reçoit le prix du
Lions Club International la même année.

# Au sujet du roman

Quel est le thème de ce roman? L’auteur raconte en novembre 1999: «La
"toile", c'est celle du peintre, c'est aussi l'autre nom d'internet: le
web - la toile d'araignée. "Sanguine" évoque le dessin et la mort
brutale. Mais l'amour des couleurs justifierait-il le meurtre?
"Sanguine sur toile" évoque l'histoire singulière d'un internaute pris
dans la tourmente de son propre ordinateur, manipulé à distance par un
très mystérieux correspondant qui n'a que vengeance en tête. J'ai voulu
emporter le lecteur dans les univers de la peinture et de l'entreprise,
univers qui s'entrelacent, s'échappent, puis se rejoignent dans la
fulgurance des logiciels.

Le lecteur est ainsi invité à prendre l'enquête à son propre compte
pour tenter de démêler les fils tressés par la seule passion. Pour
percer le mystère, il devra répondre à de multiples questions. Le monde
au bout des doigts, l'internaute n'est-il pas pour autant l'être le
plus seul au monde? Compétitivité oblige, jusqu'où l'entreprise
d'aujourd'hui peut-elle aller dans la violence? La peinture tend-elle à
reproduire le monde ou bien à en créer un autre? Enfin, j'ai voulu
montrer que les images ne sont pas si sages. On peut s'en servir pour
agir, voire pour tuer. (...)

Dans le roman, internet est un personnage en soi. Plutôt que de le
décrire dans sa complexité technique, le réseau est montré comme un
être tantôt menaçant, tantôt prévenant, maniant parfois l'humour.
N'oublions pas que l'écran d'ordinateur joue son double rôle: il montre
et il cache. C'est cette ambivalence qui fait l'intrigue du début à la
fin. Dans ce jeu, le grand gagnant est bien sûr celui ou celle qui sait
s'affranchir de l'emprise de l'outil pour mettre l'humanisme et
l'intelligence au-dessus de tout.»

# Au sujet de l’auteur

Alain Bron est consultant en systèmes d’information et écrivain. Il
explique son parcours: «J'ai passé une vingtaine d'années chez Bull.
Là, j'ai participé à toutes les aventures de l'ordinateur et des
télécommunications, j'ai été représentant des industries informatiques
à l'ISO (Organisation internationale de normalisation), et chairman du
groupe réseaux du consortium X/Open. J'ai connu aussi les tout débuts
d'internet avec mes collègues de Honeywell aux Etats-Unis (fin 1978).
Je suis actuellement [fin 1999] consultant en systèmes d'information où
je m'occupe de la bonne marche de grands projets informatiques (…). Et
j'écris. J'écris depuis mon adolescence. Des nouvelles (plus d'une
centaine), des essais psycho-sociologiques ("La gourmandise du tapir"
et "La démocratie de la solitude"), des articles et des romans. C'est à
la fois un besoin et un plaisir jubilatoire.»

En ce qui concerne la finalité du réseau, «ce qui importe avec
internet, c'est la valeur ajoutée de l'humain sur le système. Internet
ne viendra jamais compenser la clairvoyance d'une situation, la prise
de risque ou l'intelligence du coeur. Internet accélère simplement les
processus de décision et réduit l'incertitude par l'information
apportée. Encore faut-il laisser le temps au temps, laisser mûrir les
idées, apporter une touche indispensable d'humanité dans les rapports.
Pour moi, la finalité d'internet est la rencontre et non la
multiplication des échanges électroniques.»

Quel est son meilleur souvenir lié à l'internet? «À la suite de la
parution de mon roman "Sanguine sur toile", j'ai reçu un message d'un
ami que j'avais perdu de vue depuis plus de vingt ans. Il s'était
reconnu dans un personnage du livre. Nous nous sommes revus récemment
autour d'une bouteille de Saint-Joseph et nous avons pu échanger des
souvenirs et fomenter des projets.»



2000 > ENCYCLOPÉDIES ET DICTIONNAIRES EN LIGNE


[Résumé]
Les premières encyclopédies de référence disponibles sur le web émanent
de versions imprimées. C’est aussi le cas des dictionnaires en ligne.
WebEncyclo est mis en ligne par les éditions Atlas en décembre 1999
avec accès libre et gratuit, tout comme l’Encyclopaedia Universalis,
mais avec accès payant. Le site Britannica.com est lui aussi mis en
ligne à la même date pour proposer le contenu des 32 volumes de
l’Encyclopaedia Britannica, d’abord en accès libre puis en accès
payant. Les premiers dictionnaires de référence en ligne émanent eux
aussi de versions imprimées. Le Dictionnaire universel francophone en
ligne des éditions Hachette est disponible en accès libre dès 1997. Les
20 volumes de l'Oxford English Dictionary (OED) sont mis en ligne en
mars 2000 avec accès payant. Conçu directement pour le web, le Grand
dictionnaire terminologique (GDT) est mis en ligne en septembre 2000
avec accès libre et gratuit.

***

Les premières encyclopédies de référence disponibles sur le web émanent
de versions imprimées. C’est aussi le cas des dictionnaires en ligne.

# Les encyclopédies

WebEncyclo (aujourd'hui disparu), publié par les éditions Atlas, est la
première grande encyclopédie francophone en accès libre, avec mise en
ligne en décembre 1999. La recherche est possible par mots-clés,
thèmes, médias (à savoir les cartes, liens internet, photos et
illustrations) et idées. Un appel à contribution incite les
spécialistes d'un sujet donné à envoyer des articles, qui sont
regroupés dans la section «WebEncyclo contributif». Après avoir été
libre, l'accès est ensuite soumis à une inscription préalable gratuite.

La version web de l'Encyclopaedia Universalis est elle aussi mise en
ligne en décembre 1999, ce qui représente un ensemble de 28.000
articles signés de 4.000 auteurs. Si la consultation est payante sur la
base d'un abonnement annuel, de nombreux articles sont également en
accès libre.

Le site Britannica.com est mis en ligne à la même date, en tant que
première grande encyclopédie anglophone en accès libre. Le site propose
l'équivalent numérique des 32 volumes de l'Encyclopaedia Britannica
(15e édition), en complément de la version imprimée et de la version
CD-Rom, toutes deux payantes. Le site web offre aussi une sélection
d'articles issus de 70 magazines, un guide des meilleurs sites, un
choix de livres, etc., le tout étant accessible à partir d'un moteur de
recherche unique. En septembre 2000, le site fait partie des cent sites
les plus visités du web. En juillet 2001, la consultation devient
payante sur la base d'un abonnement annuel ou mensuel. Beaucoup plus
tard, en 2009, Britannica.com ouvre son site à des contributeurs
externes, avec inscription obligatoire pour écrire et modifier des
articles.

# Les dictionnaires

Le premier grand dictionnaire de langue française en accès libre est le
Dictionnaire universel francophone en ligne, qui répertorie 45.000 mots
et 116.000 définitions tout en présentant «sur un pied d'égalité, le
français dit "standard" et les mots et expressions en français tel
qu'on le parle sur les cinq continents». Issu de la collaboration entre
Hachette et l'AUPELF-UREF (devenu depuis l'AUF - Agence universitaire
de la Francophonie), il est mis en ligne dès 1997 et correspond à la
partie «noms communs» du dictionnaire imprimé disponible chez Hachette.

L'équivalent pour la langue anglaise est le site Merriam-Webster
OnLine, qui donne librement accès au Collegiate Dictionary, au
Collegiate Thesaurus et à d’autres outils linguistiques.

En mars 2000, les 20 volumes de l'Oxford English Dictionary (OED) sont
mis en ligne par l'Oxford University Press (OUP). La consultation du
site est payante. Le dictionnaire bénéficie d'une mise à jour
trimestrielle d'environ 1.000 entrées nouvelles ou révisées.

En mars 2002, deux ans après cette première expérience, l'Oxford
University Press lance l'Oxford Reference Online (ORO), une vaste
encyclopédie conçue cette fois directement pour le web et consultable
elle aussi sur abonnement payant. Avec 60.000 pages et un million
d'entrées, elle représente l'équivalent d'une centaine d'ouvrages de
référence.

# Un dictionnaire bilingue

Conçu lui aussi directement pour le web, avec accès libre et gratuit,
le Grand dictionnaire terminologique (GDT) est un dictionnaire bilingue
français-anglais de trois millions de termes appartenant au vocabulaire
industriel, scientifique et commercial. Sa mise en ligne en septembre
2000 est le résultat d'un partenariat entre l'Office québécois de la
langue française (OQLF), auteur du dictionnaire, et Semantix, société
spécialisée dans les solutions logicielles linguistiques.

Dès le premier mois, le GDT est consulté par 1,3 million de personnes,
avec 60.000 requêtes par jour. La gestion du GDT est ensuite assurée
par Convera Canada, avec 3,5 millions de requêtes mensuelles en février
2003. Une nouvelle version du GDT est mise en ligne en mars 2003. La
gestion du dictionnaire est désormais assurée par l'OQLF lui-même, et
non plus par une société prestataire, avec l’ajout du latin comme
troisième langue.



2000 > LES AVENTURES DE STEPHEN KING


[Résumé]
Maître du suspense de renommée mondiale, Stephen King est le premier
auteur de best-sellers à se lancer dans l’aventure numérique. Dans un
premier temps, en mars 2000, il décide de distribuer sa nouvelle
«Riding The Bullet» uniquement en version numérique, avec 400.000
exemplaires téléchargés dans les premières vingt-quatre heures. Suite à
ce succès à la fois médiatique et financier, l’auteur crée son propre
site web en juillet 2000 pour auto-publier son roman épistolaire inédit
«The Plant» en plusieurs épisodes. Les chapitres paraissent à
intervalles réguliers et sont téléchargeables dans plusieurs formats
(PDF, OeB, HTML, TXT). En décembre 2000, après la parution du sixième
chapitre, l'auteur décide d'interrompre cette expérience, le nombre de
téléchargements et de paiements ayant régulièrement baissé au fil des
chapitres. L’auteur poursuit d’autres expériences numériques dans les
années qui suivent, mais cette fois en partenariat avec son éditeur.

***

Stephen King est le premier auteur de best-sellers à se lancer dans
l’aventure numérique, malgré les risques commerciaux encourus, en
tentant d’auto-publier un roman épistolaire sur le web indépendamment
de son éditeur.

# Mars 2000

En mars 2000, Stephen King, maître du suspense de renommée mondiale,
commence tout d’abord par distribuer uniquement sur l’internet sa
nouvelle «Riding the Bullet», assez volumineuse puisqu’elle comprend 66
pages. Du fait de la notoriété de l’auteur et de la couverture
médiatique de ce scoop, la publication de cette nouvelle sur le web est
un succès immédiat, avec 400.000 exemplaires téléchargés lors des
premières vingt-quatre heures dans les librairies en ligne qui la
vendent au prix de 2,5 dollars US.

# Juillet 2000

En juillet 2000, fort de cette expérience prometteuse, Stephen King
décide de se passer des services de Simon & Schuster, son éditeur
habituel. Il crée son propre site web pour débuter l’auto-publication
en épisodes de «The Plant», un roman épistolaire inédit. Ce roman
raconte l’histoire d’une plante carnivore s’emparant d’une maison
d’édition et lui promettant le succès commercial en échange de
sacrifices humains. Le premier chapitre est téléchargeable dans
plusieurs formats - PDF, OeB, HTML, TXT - pour la modeste somme d’un
dollar, avec paiement différé ou paiement immédiat sur le site
d’Amazon.

Dans une lettre aux lecteurs publiée sur son site à la même date,
l’auteur raconte que la création du site, le design et la publicité lui
ont coûté la somme de 124.150 dollars, sans compter sa prestation en
tant qu’écrivain ni la rémunération de son assistante. Il précise aussi
que la publication des chapitres suivants est liée au paiement du
premier chapitre par au moins 75% des internautes.

«Mes amis, vous avez l’occasion de devenir le pire cauchemar des
éditeurs, déclare-t-il dans sa lettre. Comme vous le voyez, c’est
simple. Pas de cryptage assommant! Vous voulez imprimer l’histoire et
en faire profiter un(e) ami(e)? Allez-y. Une seule condition: tout
repose sur la confiance, tout simplement. C’est la seule solution. Je
compte sur deux facteurs. Le premier est l’honnêteté. Prenez ce que bon
vous semble et payez pour cela, dit le proverbe. Le second est que vous
aimerez suffisamment l’histoire pour vouloir en lire davantage. Si vous
le souhaitez vraiment, vous devez payer. Rappelez-vous: payez, et
l’histoire continue; volez, et l’histoire s’arrête.»

Une semaine après la mise en ligne du premier chapitre, on compte
152.132 téléchargements, avec paiement par 76% des lecteurs. Certains
paient davantage que le dollar demandé, allant parfois jusqu’à 10 ou 20
dollars pour compenser le manque à gagner de ceux qui ne paieraient
pas, et éviter ainsi que la série ne s’arrête.

La barre des 75% est dépassée de peu, au grand soulagement des fans, si
bien que le deuxième chapitre suit un mois après.

# Août 2000

En août 2000, dans une nouvelle lettre aux lecteurs, Stephen King
annonce un nombre de téléchargements légèrement inférieur à celui du
premier chapitre. Il en attribue la cause à une publicité moindre et à
des problèmes de téléchargement. Si le nombre de téléchargements n’a
que légèrement décru, le nombre de paiements est en nette diminution,
les internautes ne réglant leur dû qu’une seule fois pour plusieurs
téléchargements.

L’auteur s’engage toutefois à publier le troisième chapitre comme
prévu, fin septembre, et à prendre une décision ensuite sur la
poursuite ou non de l’expérience, en fonction du nombre de paiements.
Ses prévisions sont de onze ou douze chapitres en tout, avec un nombre
total de 1,7 million de téléchargements. Le ou les derniers chapitres
seraient gratuits.

Plus volumineux avec 10.000 signes au lieu de 5.000 signes comme les
précédents, les chapitres 4 et 5 passent à deux dollars. Mais le nombre
de téléchargements et de paiements ne cesse de décliner, avec 40.000
téléchargements seulement pour le cinquième chapitre alors que le
premier chapitre avait été téléchargé 120.000 fois, et paiement pour
46% des téléchargements seulement.

# Novembre 2000

Fin novembre, Stephen King annonce l’interruption de la publication
pendant une période indéterminée, après la parution du sixième
chapitre, téléchargeable gratuitement à la mi-décembre. «"The Plant" va
retourner en hibernation afin que je puisse continuer à travailler,
précise-t-il sur son site. Mes agents insistent sur la nécessité
d’observer une pause afin que la traduction et la publication à
l’étranger puissent rattraper la publication en anglais.» Mais cette
décision semble d’abord liée à l’échec commercial de l’expérience.

Cet arrêt suscite de vives critiques. On oublie de reconnaître à
l’auteur au moins un mérite, celui d’avoir été le premier à se lancer
dans l’aventure, avec les risques qu’elle comporte. Entre juillet et
décembre 2000, pendant les six mois qu’elle aura duré, nombreux sont
ceux qui suivent les tribulations de «The Plant», à commencer par les
éditeurs, quelque peu inquiets face à un médium qui pourrait un jour
concurrencer le circuit traditionnel.

Quand Stephen King décide d’arrêter l’expérience, plusieurs
journalistes et critiques littéraires affirment qu’il se ridiculise aux
yeux du monde entier, preuve qu’ils n’ont visiblement pas suivi
l’histoire depuis ses débuts. L’auteur avait d’emblée annoncé la
couleur puisqu’il avait lié la poursuite de la publication à un
pourcentage de paiements satisfaisant.

# 2001 et 2002

Qu’est-il advenu ensuite des expériences numériques de Stephen King?
L’auteur reste très présent dans ce domaine, mais cette fois par le
biais de son éditeur.

En mars 2001, son roman «Dreamcatcher» est le premier roman à être
lancé simultanément en version imprimée par Simon & Schuster et en
version numérique par Palm Digital Media, la librairie numérique de
Palm, pour lecture sur Palm Pilot et sur Pocket PC.

En mars 2002, son recueil de nouvelles «Everything’s Eventual» est lui
aussi publié simultanément en deux versions: en version imprimée par
Scribner, subdivision de Simon & Schuster, et en version numérique par
Palm Digital Media, qui en propose un extrait en téléchargement libre.
Et ainsi de suite, preuve que les éditeurs sont toujours utiles.



2000 > DES AUTEURS DE BEST-SELLERS


[Résumé]
À la suite de Stephen King, Frederick Forsyth, le maître britannique du
thriller, décide lui aussi de tenter l'aventure numérique, avec l'appui
d'Online Originals, un éditeur électronique londonien. En novembre
2000, Online Originals publie «The Veteran», histoire d'un crime
violent commis à Londres et premier volet de «Quintet», une série de
cinq nouvelles électroniques. Arturo Pérez-Reverte, romancier espagnol,
tente une expérience un peu différente. Sa série best-seller relate les
aventures du Capitan Alatriste au 17e siècle. Le nouveau titre à
paraître fin 2000 s'intitule «El Oro del Rey» (L'Or du Roi). En
novembre 2000, en collaboration avec Alfaguara, son éditeur habituel,
l'auteur décide de diffuser ce nouveau titre en version numérique sur
une page spécifique du portail Inicia, en exclusivité pendant un mois,
avant la sortie du livre imprimé en librairie. Paulo Coelho, romancier
brésilien, décide pour sa part de diffuser gratuitement plusieurs
romans au format PDF en mars 2003, avec l’accord de ses éditeurs.

***

En novembre 2000, deux auteurs de best-sellers, Frederick Forsyth et
Arturo Pérez-Reverte, décident de tenter l’aventure numérique, suivis
ensuite par de nombreux auteurs, par exemple Paulo Coelho en mars 2003.

Mais, forts de l’expérience d’auto-publication de Stephen King peut-
être, ils n’ont pas l’intention de se passer d’éditeur. Pour mémoire,
Stephen King lance l’auto-publication numérique de son roman
épistolaire inédit «The Plant» en juillet 2000. Il met fin à cette
expérience quelques mois plus tard, le nombre de paiements étant très
inférieur au nombre de téléchargements.

# Frederick Forsyth

Frederick Forsyth, maître britannique du thriller, aborde la
publication numérique avec l’appui de l’éditeur électronique londonien
Online Originals. En novembre 2000, Online Originals publie «The
Veteran» en tant que premier volet de «Quintet», une série de cinq
nouvelles électroniques annoncées dans l’ordre suivant: «The Veteran»,
«The Miracle», «The Citizen», «The Art of the Matter» et «Draco».

Disponible dans trois formats (PDF, Microsoft Reader et Glassbook
Reader), la nouvelle est vendue au prix de 3,99 pounds (6,60 euros) sur
le site de l’éditeur et dans plusieurs librairies en ligne au Royaume-
Uni (Alphabetstreet, BOL.com, WHSmith)  et aux États-Unis (Barnes &
Noble, Contentville, Glassbook).

Frederick Forsyth déclare à la même date sur le site d’Online
Originals: «La publication en ligne sera essentielle à l’avenir. Elle
crée un lien simple et surtout rapide et direct entre le producteur
original (l’auteur) et le consommateur final (le lecteur), avec très
peu d’intermédiaires. Il est passionnant de participer à cette
expérience. Je ne suis absolument pas un spécialiste des nouvelles
technologies. Je n’ai jamais vu de livre électronique. Mais je n’ai
jamais vu non plus de moteur de Formule 1, ce qui ne m’empêche pas de
constater combien ces voitures de course sont rapides.» Toutefois cette
première expérience ne dure pas, les ventes étant très inférieures aux
prévisions.

# Arturo Pérez-Reverte

La première expérience numérique d’Arturo Pérez-Reverte est un peu
différente. La série best-seller du romancier espagnol relate les
aventures du Capitan Alatriste au 17e siècle. Le nouveau titre à
paraître fin 2000 s’intitule «El Oro del Rey» (L'Or du Roi).

En novembre 2000, en collaboration avec son éditeur Alfaguara, l’auteur
décide de diffuser «El Oro del Rey» en version numérique sur un page
spécifique du portail Inicia, en exclusivité pendant un mois, avant sa
sortie en librairie. Le roman est disponible au format PDF pour 2,90
euros, un prix très inférieur aux 15,10 euros annoncés pour le livre
imprimé.

Résultat de l’expérience, le nombre de téléchargements est très
satisfaisant, mais pas celui des paiements. Un mois après la mise en
ligne du roman, on compte 332.000 téléchargements, avec paiement par
12.000 lecteurs seulement.

À la même date, Marilo Ruiz de Elvira, directrice de contenus du
portail Inicia, explique dans un communiqué: «Pour tout acheteur du
livre numérique, il y avait une clé pour le télécharger en 48 heures
sur le site internet et, surtout au début, beaucoup d’internautes se
sont échangés ce code d’accès dans les forums de chats et ont
téléchargé leur exemplaire sans payer. On a voulu tester et cela
faisait partie du jeu. Arturo Pérez-Reverte voulait surtout qu’on le
lise.»

En 2006, on compte 4 millions d’exemplaires vendus pour les cinq
premiers tomes de cette saga littéraire, devenue un succès planétaire,
surtout sous forme imprimée. La saga donne également naissance au film
Alatriste, une superproduction espagnole de 20 millions d’euros.

# Paulo Coelho

Paulo Coelho, romancier brésilien, devient mondialement célèbre avec
«L'Alchimiste». Début 2003, ses livres, traduits en 56 langues, ont été
vendus en 53 millions d'exemplaires dans 155 pays, dont 6,5 millions
d'exemplaires dans les pays francophones.

En mars 2003, Paulo Coelho décide de distribuer plusieurs de ses romans
gratuitement en version PDF, en diverses langues, avec l'accord de ses
éditeurs respectifs, dont Anne Carrière, son éditrice en France. Trois
romans sont disponibles en français: «Manuel du guerrier de la
lumière», «La cinquième montagne» et «Veronika décide de mourir».

Pourquoi une telle décision? L’auteur déclare à la même date par le
biais de son éditrice: «Comme le français est présent, à plus ou moins
grande échelle, dans le monde entier, je recevais sans cesse des
courriers électroniques d'universités et de personnes habitant loin de
la France, qui ne trouvaient pas mes oeuvres.»

À la question classique relative au préjudice éventuel sur les ventes
futures, l’auteur répond: «Seule une minorité de gens a accès à
l'internet, et le livre au format ebook ne remplacera jamais le livre
papier.» Une remarque très juste en 2003, mais qui n'est peut-être plus
de mise en 2011. Paulo Coehlo réitère toutefois l’expérience au
printemps 2011, pour la plus grande joie de ses lecteurs.



2000 > COTRES.NET, SITE DE LITTÉRATURE NUMÉRIQUE


[Résumé]
«Entoileur» du site cotres.net depuis octobre 1998, Jean-Paul
s'interroge en juin 2000 sur l'apport de l'internet dans son écriture.
«La navigation par hyperliens se fait en rayon (j'ai un centre
d'intérêt et je clique méthodiquement sur tous les liens qui s'y
rapportent) ou en louvoiements (de clic en clic, à mesure qu'ils
apparaissent, au risque de perdre de vue mon sujet). Bien sûr, les deux
sont possibles avec l'imprimé. Mais la différence saute aux yeux:
feuilleter n'est pas cliquer. L'internet n'a donc pas changé ma vie,
mais mon rapport à l'écriture. On n'écrit pas de la même manière pour
un site que pour un scénario, une pièce de théâtre, etc. (...) Depuis,
j'écris (compose, mets en page, en scène) directement à l'écran. L'état
"imprimé" de mon travail n'est pas le stade final, le but; mais une
forme parmi d'autres, qui privilégie la linéarité et l'image, et qui
exclut le son et les images animées. (...) C'est finalement dans la
publication en ligne (l'entoilage?) que j'ai trouvé la mobilité, la
fluidité que je cherchais.»

***

Jean-Paul, «entoileur» du site cotres.net, propose depuis octobre 1998
de beaux parcours littéraires utilisant l’hyperlien.

# Une navigation en rayon

En juin 2000, il s'interroge sur l'apport de l'internet dans son
écriture. «La navigation par hyperliens se fait en rayon (j’ai un
centre d’intérêt et je clique méthodiquement sur tous les liens qui s’y
rapportent) ou en louvoiements (de clic en clic, à mesure qu’ils
apparaissent, au risque de perdre de vue mon sujet). Bien sûr, les deux
sont possibles avec l’imprimé. Mais la différence saute aux yeux:
feuilleter n’est pas cliquer. L’internet a donc changé mon rapport à
l’écriture. On n’écrit pas de la même manière pour un site que pour un
scénario, une pièce de théâtre, etc. (...)

Depuis, j’écris (compose, mets en page, en scène) directement à
l’écran. L’état "imprimé" de mon travail n’est pas le stade final, le
but; mais une forme parmi d’autres, qui privilégie la linéarité et
l’image, et qui exclut le son et les images animées. (…)

C’est finalement dans la publication en ligne (l’entoilage?) que j’ai
trouvé la mobilité, la fluidité que je cherchais. Le maître mot y est
"chantier en cours", sans palissades. Accouchement permanent, à vue,
comme le monde sous nos yeux. Provisoire, comme la vie qui tâtonne, se
cherche, se déprend, se reprend. Avec évidemment le risque souligné par
les gutenbergs, les orphelins de la civilisation du livre: plus rien
n’est sûr. Il n’y a plus de source fiable, elles sont trop nombreuses,
et il devient difficile de distinguer un clerc d’un gourou. Mais c’est
un problème qui concerne le contrôle de l’information. Pas la
transmission des émotions.»

«Canon laser», l’une de ses premières oeuvres, est d’abord une oeuvre
imprimée, en série limitée, aujourd'hui épuisée. Pour ce faire, Jean-
Paul utilise un logiciel de PAO, le premier permettant de jouer
facilement avec la forme des lettres. La version hypermédia apparaît
sur le site des cotres en 2002.  Quel en est le thème? «C'est
l'histoire d'un cobaye humain payé pour jouer à l'audimat: ses yeux
balaient l'écran et l'écran les filme, pour alimenter la base de
données que louent les militaires, les publicitaires, tous ceux qui
font leur soupe de nos visions.»

# Cyber-littérature et technologie

Selon Jean-Paul, l’avenir de la cyber-littérature est tracé par sa
technologie même. «Il est maintenant impossible à un(e) auteur(e)
seul(e) de manier à la fois les mots, leur apparence mouvante et leur
sonorité. Maîtriser aussi bien Director, Photoshop et Cubase, pour ne
citer que les plus connus, c’était possible il y a dix ans, avec les
versions 1. Ça ne l’est plus. Dès demain (matin), il faudra savoir
déléguer les compétences, trouver des partenaires financiers aux reins
autrement plus solides que Gallimard, voir du côté d’Hachette-Matra,
Warner, Hollywood. Au mieux, le statut de... l’écrivaste? du
multimédiaste? sera celui du vidéaste, du metteur en scène, du
directeur de produit: c’est lui qui écope des palmes d’or à Cannes,
mais il n’aurait jamais pu les décrocher seul. Soeur jumelle (et non
pas clone) du cinématographe, la cyber-littérature (= la vidéo + le
lien) sera une industrie, avec quelques artisans isolés dans la
périphérie off-off (aux droits d’auteur négatifs, donc).»

Sept ans plus tard, en janvier 2007, Jean-Paul fait à nouveau le point
sur son activité d’entoileur. «J’ai gagné du temps. J’utilise moins de
logiciels, dont j’intègre le résultat dans Flash. Ce dernier m’assure
de contrôler à 90% le résultat à l’affichage sur les écrans de
réception (au contraire de ceux qui préfèrent présenter des oeuvres
ouvertes, où l’intervention tantôt du hasard tantôt de l’internaute est
recherchée). Je peux maintenant me concentrer sur le coeur de la chose:
l’architecture et le développement du récit.»

Selon lui, «les deux points forts des trois ou quatre ans à venir [en
2007] sont: (a) la généralisation du très haut débit (c’est-à-dire en
fait du débit normal), qui va m’affranchir des limitations purement
techniques, notamment des soucis de poids et d’affichage des fichiers
(mort définitive, enfin, des histogrammes de chargement); (b) le
développement de la 3 D. C’est le récit en hypermédia (= le multimédia
+ le clic) qui m’intéresse. Les pièges que pose un récit en 2 D sont
déjà passionnants. Avec la 3 D, il va falloir chevaucher le tigre pour
éviter la simple prouesse technique et laisser la priorité au récit.»

# Les cotres au présent

En juin 2011, la page d’accueil de cotres.net donne accès à trois
œuvres hypermédias qui sont tout autant de parcours, puisant leur
inspiration dans la région parisienne et sur toute la planète.

«Solstice» (2008) est une «carte de voeux à vocation universelle: on
l'envoie à tout moment, adressée à tous les peuples de l'univers, sans
oublier les autres. Elle est ronde, pour lutter contre les angles
blessants de tous ces rectangles autour de nous, la dictature
universelle du rectangle.»

«Agression93» (2009) est «un roman dépliable, sous l'apparence d'une
nouvelle. Un fait-divers infime, le récit d'une agression minuscule.
Mais on sent bien qu'il y a autre chose. Il va falloir choisir, et
vite: la vengeance ou la justice.»

Ce parcours peut durer entre quatre minutes pour le lecteur pressé (en
suivant uniquement les V et les > situés en bas et à droite de l’écran)
et quinze minutes pour le lecteur averti (en explorant tous les liens,
c’est-à-dire en passant la souris sur ceux-ci puis en cliquant sur
certains).

«Aux jardins de Picpus» (2010) est «une visite guidée muette du petit
jardin de Picpus [dans Paris] et de ses fantômes, ceux pour qui le
jardin n'est pas fait et dont les traces sont partout. Un jardin, c'est
d'abord des murs, ou des grilles, ou une ligne dans la tête, ou le
paradis d'après certains.»



2000 > UN FORMAT STANDARD POUR LE LIVRE NUMÉRIQUE


[Résumé]
Les années 1998-2001 sont marquées par la prolifération des formats,
chacun lançant son propre format de livre numérique dans le cadre d’un
marché naissant promis à une expansion rapide, d’où l’intérêt d’un
format standard. Disponible en septembre 1999 dans sa version 1.0,
l'OeB (Open eBook) est un format de livre numérique basé sur le langage
XML (eXtensible Markup Language) et destiné à normaliser le contenu, la
structure et la présentation des livres numériques. Le format OeB est
défini par l'OeBPS (Open eBook Publication Structure). Fondé en janvier
2000, l'Open eBook Forum (OeBF) est un consortium industriel
international regroupant constructeurs, concepteurs de logiciels,
éditeurs, libraires et spécialistes du numérique (avec 85 participants
en 2002). Le format OeB sert de base à de nombreux formats, par exemple
le format LIT pour le Microsoft Reader et le format PRC pour le
Mobipocket Reader. Le format EPUB succède au format OeB en avril 2005.

***

Les années 1998-2001 sont marquées par la prolifération des formats,
chacun lançant son propre format de livre numérique dans le cadre d’un
marché naissant promis à une expansion rapide, d’où la nécessité d’un
format standard.

Aux formats classiques - formats TXT (texte), DOC (Microsoft Word),
HTML (HyperText Markup Language), XML (eXtensible Markup Language) et
PDF (Portable Document Format) - s’ajoutent des formats propriétaires
créés par plusieurs sociétés pour lecture sur leurs propres logiciels,
qui sont entre autres le Glassbook Reader, le Peanut Reader, le Rocket
eBook Reader (pour le Rocket eBook), le Franklin Reader (pour
l'eBookMan), le logiciel de lecture Cytale (pour le Cybook), le Gemstar
eBook Reader (pour le Gemstar eBook) et le Palm Reader (pour le Palm
Pilot). Ces logiciels correspondent souvent à un appareil donné -
tablette de lecture ou PDA - et ne peuvent donc pas être utilisés sur
d'autres appareils, tous comme les formats qui vont avec.

# Le format OeB (Open eBook)

À l’instigation du NIST (National Institute of Standards & Technology)
aux États-Unis, l’Open eBook Initiative voit le jour en juin 1998 et
constitue un groupe de travail de 25 personnes sous le nom d'Open eBook
Authoring Group. Ce groupe élabore l’OeB (Open eBook), un format de
livre numérique basé sur le langage XML et destiné à normaliser le
contenu, la structure et la présentation des livres numériques.

Le format OeB est défini par l’OeBPS (Open eBook Publication
Structure), dont la version 1.0 est disponible en septembre 1999.
Téléchargeable gratuitement, l’OeBPS dispose d'une version ouverte et
gratuite appartenant au domaine public. La version originale est
destinée aux professionnels de la publication puisqu'elle doit souvent
être associée à une technologie normalisée de gestion des droits
numériques, et donc à un système de DRM (Digital Rights Management)
permettant de contrôler l’accès des livres numériques sous droits.

Fondé en janvier 2000 pour prendre la suite de l’Open eBook Initiative,
l’Open eBook Forum (OeBF) est un consortium industriel international
regroupant constructeurs, concepteurs de logiciels, éditeurs, libraires
et spécialistes du numérique (avec 85 participants en 2002) dans
l'optique de développer le format OeB et l’OeBPS. Le format OeB devient
un standard qui sert lui-même de base à de nombreux formats, par
exemple le format LIT pour le Microsoft Reader ou le format PRC pour le
Mobipocket Reader.

# Le format LIT de Microsoft

Microsoft lance en avril 2000 son propre PDA, le Pocket PC, tout comme
le Microsoft Reader, un logiciel permettant la lecture de livres
numériques au format LIT (abrégé du terme anglais «literature»), lui-
même basé sur le format OeB.

Les caractéristiques du Microsoft Reader sont un affichage utilisant la
technologie ClearType, le choix de la taille des caractères, la
mémorisation des mots-clés pour des recherches ultérieures et l’accès
d’un clic au Merriam-Webster Dictionary.

Quatre mois plus tard, en août 2000, le Microsoft Reader est disponible
pour toute plateforme Windows, et donc aussi bien pour ordinateur que
pour PDA, sans oublier la Tablet PC un peu plus tard, lors de son
lancement en novembre 2002.

Ce logiciel étant téléchargeable gratuitement, Microsoft facture les
éditeurs et distributeurs pour l’utilisation de sa technologie DRM de
gestion des droits numériques par le biais du Microsoft Digital Asset
Server (DAS), et touche une commission sur la vente de chaque titre.

Microsoft passe aussi des partenariats avec les grandes librairies en
ligne Barnes & Noble.com en janvier 2000 et Amazon.com en août 2000
pour que celles-ci vendent des livres numériques lisibles sur le
Microsoft Reader.

Le Windows CE, système d’exploitation du Pocket PC, est remplacé en
octobre 2001 par le Pocket PC 2002 pour permettre la lecture des livres
numériques sous droits.

En 2002, la gamme Pocket PC permet la lecture sur trois logiciels: le
Microsoft Reader bien sûr, le Mobipocket Reader et le Palm Reader, qui
est le logiciel de lecture du Palm Pilot, lancé dès mars 1996 en tant
que premier PDA du marché.

# Le format PRC de Mobipocket

Fondé à Paris en mars 2000 par Thierry Brethes et Nathalie Ting,
Mobipocket se spécialise d’emblée dans la distribution sécurisée de
livres numériques pour PDA. La société est en partie financée par
Viventures, branche de la multinationale française Vivendi.

Mobipocket lance le Mobipocket Reader, un logiciel permettant la
lecture de fichiers au format PRC, lui-même basé sur le format OeB.
Gratuit et disponible dans cinq langues (français, anglais, allemand,
espagnol, italien), ce logiciel est «universel», c’est-à-dire
utilisable sur tout PDA (Palm Pilot, Pocket PC, eBookMan, Psion, etc.).

En octobre 2001, le Mobipocket Reader reçoit l’eBook Technology Award
de la Foire internationale du livre à Francfort (Allemagne). À la même
date, Franklin passe un partenariat avec Mobipocket pour proposer le
Mobipocket Reader par défaut sur l’eBookMan, le PDA multimédia de
Franklin, en plus du Franklin Reader, au lieu du partenariat prévu à
l’origine entre Franklin et Microsoft pour proposer le Microsoft
Reader.

Si le Mobipocket Reader est gratuit, d’autres logiciels Mobipocket sont
payants. Le Mobipocket Web Companion est un logiciel d’extraction
automatique de contenu pour les sites de presse partenaires de la
société. Le Mobipocket Publisher permet aux particuliers (version
privée gratuite ou version standard payante) et aux éditeurs (version
professionnelle payante) de créer des livres numériques sécurisés
utilisant la technologie Mobipocket DRM pour contrôler l’accès aux
livres numériques sous droits. Dans un souci d’ouverture aux autres
formats, le Mobipocket Publisher permet aussi de créer des livres
numériques au format LIT pour lecture sur le Microsoft Reader.

Déjà utilisable sur n’importe quel PDA, le Mobipocket Reader est
également disponible sur ordinateur en avril 2002 puis sur les premiers
smartphones de Nokia et Sony Ericsson au printemps 2003.

À la même date, le nombre de livres lisibles sur le Mobipocket Reader
se chiffre à 6.000 titres disponibles dans quatre langues (français,
anglais, allemand, espagnol), distribués soit sur le site de Mobipocket
soit dans des librairies partenaires. Mobipocket est racheté par Amazon
en avril 2005.

# Le format EPUB

En avril 2005, l'Open eBook Forum devient l'International Digital
Publishing Forum (IDPF), et le format OeB laisse la place au format
EPUB, acronyme de «electronic publication». Ce format est largement
utilisé par les éditeurs parce qu’il facilite la mise en page des
livres sur tout appareil de lecture (ordinateur, téléphone mobile,
smartphone, tablette de lecture) en fonction de la taille de l’écran.
Les fichiers PDF (autre standard du livre numérique) créés avec des
versions récentes du logiciel Adobe Acrobat sont compatibles avec le
format EPUB.



2000 > NUMILOG, LIBRAIRIE NUMÉRIQUE


[Résumé]
Numilog ouvre ses portes «virtuelles» en octobre 2000 pour vendre
exclusivement des livres numériques, par téléchargement et dans
plusieurs formats. Fondée par Denis Zwirn en avril 2000, six mois avant
l'ouverture de la librairie numérique, la société a en fait une triple
activité: librairie en ligne, studio de fabrication et diffuseur. En
2003, le catalogue comprend 3.500 titres (livres et périodiques) en
français et en anglais. En décembre 2006, Numilog propose 35.000 livres
numériques grâce à un partenariat avec soixante éditeurs. Au fil des
ans, Numilog devient la principale librairie francophone de livres
numériques. En janvier 2009, Numilog, devenu filiale du groupe Hachette
Livre (en mai 2008), est un distributeur-diffuseur numérique
représentant une centaine d’éditeurs francophones et anglophones, avec
un catalogue de 50.000 livres numériques et des services spécifiques
pour les bibliothèques et les librairies.

***

Numilog ouvre ses portes «virtuelles» en octobre 2000 pour vendre
exclusivement des livres numériques, par téléchargement et dans
plusieurs formats.

# Une triple activité

Fondée par Denis Zwirn en avril 2000, six mois avant l'ouverture de la
librairie numérique, la société a en fait une triple activité:
librairie en ligne, studio de fabrication et diffuseur.

Denis relate en février 2001: «Dès 1995, j’avais imaginé et dessiné des
modèles de lecteurs électroniques permettant d’emporter sa bibliothèque
avec soi et pesant comme un livre de poche. Début 1999, j’ai repris ce
projet avec un ami spécialiste de la création de sites internet, en
réalisant la formidable synergie possible entre des appareils de
lecture électronique mobiles et le développement d’internet, qui permet
d’acheminer les livres dématérialisés en quelques minutes dans tous les
coins du monde. (…)

Numilog est d’abord une librairie en ligne de livres numériques. Notre
site internet est dédié à la vente en ligne de ces livres, qui sont
envoyés par courrier électronique ou téléchargés après paiement par
carte bancaire. Il permet aussi de vendre des livres par chapitres.

Numilog est également un studio de fabrication de livres numériques:
aujourd’hui [début 2001], les livres numériques n’existent pas chez les
éditeurs, il faut donc d’abord les fabriquer avant de pouvoir les
vendre, dans le cadre de contrats négociés avec les éditeurs détenteurs
des droits. Ce qui signifie les convertir à des formats convenant aux
différents "readers" du marché. (...)

Enfin Numilog devient aussi progressivement un diffuseur. Car, sur
internet, il est important d’être présent en de très nombreux points du
réseau pour faire connaître son offre. Pour les livres en particulier,
il faut les proposer aux différents sites thématiques ou de
communautés, dont les centres d’intérêt correspondent à leur sujet
(sites de fans d’histoire, de management, de science-fiction...).
Numilog facilitera ainsi la mise en oeuvre de multiples "boutiques de
livres numériques" thématiques.»

Les livres sont disponibles en plusieurs formats: le format PDF pour
lecture sur l’Acrobat Reader (devenu l’Adobe Reader en mai 2003), le
format LIT pour lecture sur le Microsoft Reader et le format PRC pour
lecture sur le Mobipocket Reader.

En septembre 2003, le catalogue comprend 3.500 titres (livres et
périodiques) en français et en anglais, grâce à un partenariat avec une
quarantaine d’éditeurs, le but à long terme étant de «permettre à un
public d’internautes de plus en plus large d’avoir progressivement
accès à des bases de livres numériques aussi importantes que celles des
livres papier, mais avec plus de modularité, de richesse d’utilisation
et à moindre prix».

# L’expansion

Au fil des ans, Numilog devient la principale librairie francophone de
livres numériques, suite à des accords avec de nombreux éditeurs:
Gallimard, Albin Michel, Eyrolles, Hermès Science, Pearson Education
France, etc. Numilog propose aussi des livres audionumériques lisibles
sur synthèse vocale. Une librairie anglophone est lancée suite à des
accords de diffusion conclus avec plusieurs éditeurs anglo-saxons:
Springer-Kluwer, Oxford University Press, Taylor & Francis, Kogan Page,
etc. Les différents formats proposés permettent la lecture des livres
sur tout appareil électronique: ordinateur, PDA, téléphone mobile,
smartphone et tablette de lecture.

Si les livres numériques ont une longue vie devant eux, les appareils
de lecture risquent de muer régulièrement. Selon Denis Zwirn,
interviewé à nouveau en février 2003, «l’équipement des individus et
des entreprises en matériel pouvant être utilisé pour la lecture
numérique dans une situation de mobilité va continuer de progresser
très fortement dans les dix prochaines années sous la forme de machines
de plus en plus performantes (en terme d’affichage, de mémoire, de
fonctionnalités, de légèreté...) et de moins en moins chères. Cela
prend dès aujourd’hui la forme de PDA (Pocket PC et Palm Pilot), de
tablettes PC et de smartphones, ou de smart displays (écrans tactiles
sans fil). Trois tendances devraient être observées: la convergence des
usages (téléphone/PDA), la diversification des types et tailles
d’appareils (de la montre-PDA-téléphone à la tablette PC waterproof),
la démocratisation de l’accès aux machines mobiles (des PDA pour
enfants à 15 euros). Si les éditeurs et les libraires numériques savent
en saisir l’opportunité, cette évolution représente un environnement
technologique et culturel au sein duquel les livres numériques, sous
des formes variées, peuvent devenir un mode naturel d’accès à la
lecture pour toute une génération.»

En 2004, Numilog met sur pied un système de bibliothèque en ligne pour
le prêt de livres numériques, avant de proposer aux librairies un
service de vente de livres numériques sur leur propre site. En décembre
2006, le catalogue de Numilog comprend 35.000 livres grâce à un
partenariat avec soixante éditeurs francophones et anglophones.

En mai 2008, Numilog devient une filiale du groupe Hachette Livre. En
janvier 2009, Numilog est désormais un distributeur-diffuseur numérique
représentant une centaine d’éditeurs francophones et anglophones, avec
un catalogue de 50.000 livres numériques distribués auprès des
particuliers et des bibliothèques, tout en offrant aussi des services
spécifiques à destination des libraires.



2000 > LA BIBLE DE GUTENBERG EN LIGNE


[Résumé]
En 2000, le livre numérique a bientôt trente ans puisqu’il voit le jour
en juillet 1971 en tant que premier titre (eText #1) du Projet
Gutenberg. Signe des temps peut-être, la British Library met en ligne
en novembre 2000 une version numérisée de la Bible de Gutenberg, qui
fut le premier livre imprimé. Datant de 1454, cette Bible aurait été
imprimée par Gutenberg en 180 exemplaires dans son atelier de Mayence,
en Allemagne. En 2000, 48 exemplaires, dont certains incomplets,
existeraient toujours. La British Library en possède deux versions
complètes. En mars 2000, dix chercheurs et experts techniques de
l'Université Keio de Tokyo et de NTT (Nippon Telegraph and Telephone
Communications) viennent passer deux semaines sur place pour numériser
les deux versions, légèrement différentes. Ces versions numérisées sont
mises en ligne quelques mois plus tard, en novembre 2000.

***

Bientôt trente ans pour le livre numérique. Signe des temps peut-être,
la British Library met en ligne en novembre 2000 une version numérisée
de la Bible originale de Gutenberg, qui fut le premier livre imprimé.

# Le premier livre imprimé en ligne

En 2000, le livre numérique a bientôt trente ans, puisqu’il voit le
jour en juillet 1971 en tant que premier titre (eText #1) du Projet
Gutenberg.

Le livre imprimé a cinq siècles et demi. Datant de 1454, la première
Bible imprimée aurait été imprimée par Gutenberg en 180 exemplaires
dans son atelier de Mayence, en Allemagne. 48 exemplaires, dont
certains incomplets, existeraient toujours. La British Library en
possède deux versions complètes.

En mars 2000, dix chercheurs et experts techniques de l'Université Keio
de Tokyo et de NTT (Nippon Telegraph and Telephone Communications)
viennent passer deux semaines sur place pour numériser les deux
versions complètes, légèrement différentes.  Ces versions numérisées
sont mises en ligne quelques mois plus tard, en novembre 2000.

D’autres trésors de la British Library sont déjà en ligne, par exemple
Beowulf, premier joyau de la littérature anglaise datant du 11e siècle,
Magna Carta, premier texte constitutionnel anglais signé en 1215, les
Lindisfarne Gospels, une oeuvre datant de 698, le Diamond Sutra, autre
joyau datant de 868, les Sforza Hours, trésor de la Renaissance datant
de 1490-1520, le Codex Arundel, composé de notes de Léonard de Vinci de
1478 à 1518, ou encore le Tyndale New Testament, qui fut le premier
Nouveau Testament en langue anglaise, imprimé en 1526 sur les presses
de Peter Schoeffer à Worms (Allemagne).

# État des lieux à la fin 2000

Fin 2000, outre ces trésors inestimables mis à la disposition de tous,
des milliers d’oeuvres du domaine public sont en accès libre sur le web
dans des bibliothèques numériques.

Les libraires et les éditeurs ont pour la plupart un site web. Certains
naissent  directement sur le web, avec la totalité de leurs
transactions faites via l’internet.

Outre le prêt de documents en tous genres et la gestion d’ouvrages de
référence, les bibliothécaires guident les usagers sur l’internet,
sélectionnent et organisent des informations à leur intention pour leur
éviter de se noyer sur la toile, et créent un site web incluant un
catalogue en ligne et une bibliothèque numérique.

De plus en plus de livres et de revues ne sont disponibles qu’en
version numérique, pour éviter les coûts d’une publication imprimée. De
«statique» dans les livres imprimés, l’information devient «fluide» sur
l’internet, avec possibilité d’actualisation constante.

Nombre d’auteurs s’accordent à reconnaître les bienfaits de l'internet,
que ce soit pour la recherche d’information, la diffusion de leurs
oeuvres, les échanges avec les lecteurs ou la collaboration avec
d’autres créateurs, sans parler d’un prolongement de leurs livres sur
un site web, tout comme la possibilité de s’auto-publier avec succès.

Les éditeurs scientifiques et techniques commencent à repenser leur
travail et à s’orienter vers une diffusion en ligne, les tirages
imprimés restant toujours possibles à titre ponctuel. Certaines
universités diffusent désormais des manuels «sur mesure» composés d’un
choix de chapitres et d’articles sélectionnés dans une base de données,
auxquels s’ajoutent les commentaires des professeurs.

Des auteurs explorent les possibilités offertes par l’hyperlien pour
créer de nouveaux genres tels que roman multimédia et hypermédia,
narration hypertextuelle, oeuvre d’hyperfiction, site web d’écriture
hypermédia, mail-roman, etc. La littérature numérique - appelée aussi
littérature électronique, cyber-littérature ou littérature HTX
(HyperTeXt) - bouscule désormais la littérature traditionnelle en lui
apportant un souffle nouveau, tout en intégrant d’autres formes
artistiques puisque le support numérique favorise la fusion de l’écrit
avec l’image et le son.

L’internet devient indispensable pour se documenter, communiquer, avoir
accès aux documents et élargir ses connaissances. Il n’est plus utile
de courir après l'information dont nous avons besoin. Cette information
est à notre portée, en quantité, y compris pour ceux qui suivent leurs
études par correspondance, qui vivent en rase campagne, qui travaillent
à domicile ou qui sont cloués sur un lit.

Le web devient une gigantesque encyclopédie, une énorme bibliothèque,
une immense librairie et un médium des plus complets.

Certains lisent même un livre sur leur ordinateur, leur PDA ou leur
tablette de lecture (encore hors de prix).



2001 > LE WEB AU SERVICE DES AUTEURS


[Résumé]
Nombre d’auteurs s’accordent à reconnaître les bienfaits de l'internet,
que ce soit pour la diffusion de leurs oeuvres, les échanges avec les
lecteurs ou la collaboration avec d’autres créateurs. Voici quatre
expériences parmi tant d’autres. Silvaine Arabo, poète et peintre,
propose une revue imprimée «Saraswati: revue de poésie, d’art et de
réflexion» en mars 2001 après avoir lancé sa cyber-revue «Poésie d’hier
et d’aujourd’hui» en mai 1997. Raymond Godefroy, écrivain-paysan
normand, publie ses fables sur le site web des éditions du Choucas en
décembre 1999, avec un beau design de Nicolas Pewny, avant d’en auto-
publier une version imprimée en juin 2001. Anne-Bénédicte Joly,
romancière et essayiste, crée un site web en avril 2000 en tant que
vitrine pour diffuser ses romans auto-édités. Michel Benoît, auteur de
nouvelles, utilise l’internet pour élargir ses horizons et pour «abolir
le temps et la distance».

***

Nombre d’auteurs s’accordent à reconnaître les bienfaits de l'internet,
que ce soit pour la diffusion de leurs oeuvres, les échanges avec les
lecteurs ou la collaboration avec d’autres créateurs. Voici quatre
expériences parmi tant d’autres.

# Des poèmes sur deux supports

Poète et plasticienne, Silvaine Arabo vit en France, dans la région
Poitou-Charentes. En mai 1997, elle crée «Poésie d’hier et
d’aujourd’hui», l’un des premiers sites francophones consacrés à la
poésie.

En juin 1998, elle raconte: «Pour ce qui est d’internet, je suis
autodidacte (je n’ai reçu aucune formation informatique quelle qu’elle
soit). J’ai eu l’idée de construire un site littéraire centré sur la
poésie: internet me semble un moyen privilégié pour faire circuler des
idées, pour communiquer ses passions aussi. Je me suis donc mise au
travail, très empiriquement, et ai finalement abouti à ce site sur
lequel j’essaye de mettre en valeur des poètes contemporains de talent,
sans oublier la nécessaire prise de recul (rubrique "Réflexions sur la
poésie") sur l’objet considéré. (...)

Par ailleurs, internet m’a mis en contact avec d’autres poètes, dont
certains fort intéressants. Cela rompt le cercle de la solitude et
permet d’échanger des idées. On se lance des défis aussi. Internet peut
donc pousser à la créativité et relancer les motivations des poètes
puisqu’ils savent qu’ils seront lus et pourront même, dans le meilleur
des cas, correspondre avec leurs lecteurs et avoir les points de vue de
ceux-ci sur leurs textes. Je ne vois personnellement que des aspects
positifs à la promotion de la poésie par internet, tant pour le lecteur
que pour le créateur.»

Très vite, «Poésie d’hier et d’aujourd’hui» prend la forme d’une cyber-
revue. Quatre ans plus tard, en mars 2001, Silvaine Arabo crée une
deuxième revue, «Saraswati: revue de poésie, d’art et de réflexion»,
cette fois sous forme imprimée. Les deux revues «se complètent et sont
vraiment à placer en regard l’une de l’autre».

# Des fables publiées en ligne

Le Choucas est une maison d’édition haut-savoyarde gérée par Nicolas et
Suzanne Pewny. Bien qu’étant d’abord un éditeur à vocation commerciale,
le Choucas tient aussi à avoir des activités non commerciales afin de
faire connaître des auteurs peu diffusés, par exemple Raymond Godefroy,
écrivain-paysan normand, qui désespérait de trouver un éditeur pour son
recueil de fables, intitulé «Fables pour l’an 2000». Quelques jours
avant l'année 2000, Nicolas Pewny publie ce recueil en ligne, avec un
beau design.

Enthousiaste à la vue de ses fables en version numérique, Raymond
Godefroy écrit à la même date: «Internet représente pour moi un
formidable outil de communication qui nous affranchit des
intermédiaires, des barrages doctrinaires et des intérêts des médias en
place. Soumis aux mêmes lois cosmiques, les hommes, pouvant mieux se
connaître, acquerront peu à peu cette conscience du collectif,
d’appartenir à un même monde fragile pour y vivre en harmonie sans le
détruire. Internet est absolument comme la langue d’Ésope, la meilleure
et la pire des choses, selon l’usage qu’on en fait, et j’espère qu’il
me permettra de m’affranchir en partie de l’édition et de la
distribution traditionnelle qui, refermée sur elle-même, souffre d’une
crise d’intolérance pour entrer à reculons dans le prochain
millénaire.»

Très certainement autobiographique, la fable «Le poète et l’éditeur»
(sixième fable de la troisième partie) relate on ne peut mieux les
affres du poète à la recherche d’un éditeur. Raymond Godefroy restant
très attaché au papier, il décide toutefois d’auto-publier la version
imprimée de ses fables en juin 2001, avec un titre légèrement
différent, «Fables pour les années 2000», puisque le cap du 21e siècle
est désormais franchi.

# Le site web d’une romancière

Autre expérience, celle d'Anne-Bénédicte Joly, romancière et essayiste,
qui habite en région parisienne. En avril 2000, elle crée un site web
en tant que vitrine pour diffuser ses livres auto-édités.

Elle relate trois mois plus tard: «Mon site a plusieurs objectifs.
Présenter mes livres (essais, nouvelles et romans auto-édités) à
travers des fiches signalétiques (dont le format est identique à celui
que l’on trouve dans la base de données Électre) et des extraits
choisis, présenter mon parcours (de professeur de lettres et
d’écrivain), permettre de commander mes ouvrages, offrir la possibilité
de laisser des impressions sur un livre d’or, guider le lecteur à
travers des liens vers des sites littéraires. (...)

Créer un site internet me permet d’élargir le cercle de mes lecteurs en
incitant les internautes à découvrir mes écrits. Internet est également
un moyen pour élargir la diffusion de mes ouvrages. (…) Internet devra
me permettre d'aller à la rencontre de lecteurs (d'internautes) que je
n'aurai pas l'occasion en temps ordinaire de côtoyer. Je pense à des
pays francophones tels que le Canada qui semble réserver une place
importante à la littérature francaise.»

Pourquoi ce choix d’auto-éditer ses oeuvres? «Après avoir rencontré de
nombreuses fins de non-recevoir auprès des maisons d'édition et ne
souhaitant pas opter pour des éditions à compte d'auteur, j'ai choisi,
parce que l'on écrit avant tout pour être lu (!), d'avoir recours à
l'auto-édition. Je suis donc un écrivain-éditeur et j'assume
l'intégralité des étapes de la chaîne littéraire, depuis l'écriture
jusqu'à la commercialisation, en passant par la saisie, la mise en
page, l'impression, le dépôt légal et la diffusion de mes livres. Mes
livres sont en règle générale édités à 250 exemplaires et je parviens
systématiquement à couvrir mes frais fixes.»

# Une «ouverture sur le monde»

Auteur de nouvelles (polars, récits noirs, histoires fantastiques),
Michel Benoît utilise l’internet pour élargir ses horizons et pour
«abolir le temps et la distance» entre Montréal, où il habite, et de
nombreux lieux de la planète.

Michel relate en juin 2000: «L’internet s’est imposé à moi comme outil
de recherche et de communication, essentiellement. Non, pas
essentiellement. Ouverture sur le monde aussi. Si l’on pense
“recherche”, on pense “information”. Voyez-vous, si l’on pense
“écriture”, “réflexion”, on pense “connaissance”, “recherche”. Donc on
va sur la toile pour tout, pour une idée, une image, une explication.
Un discours prononcé il y a vingt ans, une peinture exposée dans un
musée à l’autre bout du monde. On peut donner une idée à quelqu’un
qu’on n’a jamais vu, et en recevoir de même. La toile, c’est le monde
au clic de la souris. On pourrait penser que c’est un beau cliché.
Peut-être bien, à moins de prendre conscience de toutes les
implications de la chose. L’instantanéité, l’information tout de suite,
maintenant. Plus besoin de fouiller, de se taper des heures de
recherche. On est en train de faire, de produire. On a besoin d’une
information. On va la chercher, immédiatement. De plus, on a accès aux
plus grandes bibliothèques, aux plus importants journaux, aux musées
les plus prestigieux. (...)

Mon avenir professionnel en inter-relation avec le net, je le vois
exploser. Plus rapide, plus complet, plus productif. Je me vois faire
en une semaine ce qui m’aurait pris des mois. Plus beau, plus
esthétique. Je me vois réussir des travaux plus raffinés, d’une facture
plus professionnelle, même et surtout dans des domaines connexes à mon
travail, comme la typographie, où je n’ai aucune compétence. La
présentation, le transport de textes, par exemple. Le travail simultané
de plusieurs personnes qui seront sur des continents différents.
Arriver à un consensus en quelques heures sur un projet, alors qu’avant
le net, il aurait fallu plusieurs semaines, parlons de mois entre les
francophones. Plus le net ira se complexifiant, plus l’utilisation du
net deviendra profitable, nécessaire, essentielle.»



2001 > DE NOUVEAUX GENRES LITTÉRAIRES


[Résumé]
Des auteurs tentent de nouvelles expériences littéraires. Voici deux
expériences parmi tant d’autres, d’une part un cyber-polar et d’autre
part un mail-roman. Après avoir écrit un roman policier sous forme de
feuilleton sur le web en 1998 et 1999, Anne-Cécile Brandenbourger le
publie en version numérique chez 00h00 en février 2000 puis en version
imprimée en août 2000. Jean-Pierre Balpe diffuse quotidiennement par
courriel pendant cent jours (du 11 avril au 19 juillet 2001) un
chapitre de son mail-roman «Rien n'est sans dire» auprès de cinq cents
personnes - sa famille, ses amis, ses collègues, etc. - en y intégrant
les réponses et les réactions de ses lecteurs. Cette idée d'un mail-
roman est issue de deux sources différentes: «d'une part en se
demandant ce qu'internet peut apporter sur le plan de la forme à la
littérature, d'autre part en lisant la littérature épistolaire du 18e
siècle, ces fameux romans par lettres». Selon l’auteur, «les
technologies numériques sont une chance extraordinaire du
renouvellement du littéraire».

***

Des auteurs tentent de nouvelles expériences littéraires. Voici deux
expériences parmi tant d’autres, d’une part un cyber-polar d’Anne-
Cécile Brandenbourger et d’autre part un mail-roman de Jean-Pierre
Balpe.

# Un cyber-polar

Né en Belgique sous la plume d’Anne-Cécile Brandenbourger, «Apparitions
inquiétantes» est «une longue histoire à lire dans tous les sens, un
labyrinthe de crimes, de mauvaises pensées et de plaisirs ambigus», qui
se développe d’abord sous la forme d’un feuilleton en ligne sur le site
d’Anacoluthe, pendant deux ans, en collaboration avec Olivier Lefèvre.

En février 2000, le roman est publié par 00h00 en version numérique (au
format PDF) en tant que premier titre de la Collection 2003, consacrée
aux écritures numériques, avec version imprimée à la demande.

Sur son site, 00h00 présente l'ouvrage comme «un cyber-polar fait de
récits hypertextuels imbriqués en gigogne. Entre personnages de
feuilleton américain et intrigue policière, le lecteur est -
hypertextuellement - mené par le bout du nez dans cette saga aux
allures borgésiennes. (...) C’est une histoire de meurtre et une
enquête policière; des textes écrits court et montés serrés; une balade
dans l’imaginaire des séries télé; une déstructuration (organisée) du
récit dans une transposition littéraire du zapping; et par conséquent,
des sensations de lecture radicalement neuves.»

Anne-Cécile relate en juin 2000: «Les possibilités offertes par
l’hypertexte m’ont permis de développer et de donner libre cours à des
tendances que j’avais déjà auparavant. J’ai toujours adoré écrire et
lire des textes éclatés et inclassables (comme par exemple "La vie mode
d’emploi" de Perec ou "Si par une nuit d’hiver un voyageur" de Calvino)
et l’hypermédia m’a donné l’occasion de me plonger dans ces formes
narratives en toute liberté. Car, pour créer des histoires non
linéaires et des réseaux de textes qui s’imbriquent les uns dans les
autres, l’hypertexte est évidemment plus approprié que le papier. Je
crois qu’au fil des jours, mon travail hypertextuel a rendu mon
écriture de plus en plus intuitive. Plus "intérieure" aussi peut-être,
plus proche des associations d’idées et des mouvements désordonnés qui
caractérisent la pensée lorsqu’elle se laisse aller à la rêverie. Cela
s’explique par la nature de la navigation hypertextuelle, le fait que
presque chaque mot qu’on écrit peut être un lien, une porte qui s’ouvre
sur une histoire.»

Suite au succès du livre, les éditions Florent Massot publient en août
2000 une deuxième version imprimée (la première étant celle de 00h00,
imprimée uniquement à la demande), sous le nouveau titre «La
malédiction du parasol», avec une couverture en 3D et une maquette
d’Olivier Lefèvre restituant le rythme de la version originale.

# Un mail-roman

Le premier mail-roman francophone est lancé en 2001 par Jean-Pierre
Balpe, chercheur, écrivain et directeur du département hypermédia de
l’Université Paris 8. Pendant très exactement cent jours, entre le 11
avril et le 19 juillet 2001, il diffuse quotidiennement par courriel un
chapitre de «Rien n’est sans dire» auprès de cinq cents personnes - sa
famille, ses amis, ses collègues, etc. - en y intégrant les réponses et
les réactions de ses lecteurs.

Racontée par un narrateur, l’histoire est celle de Stanislas et Zita,
qui vivent une passion tragique déchirée par une sombre histoire
politique. L’auteur relate en février 2002: «Cette idée d’un mail-roman
m’est venue tout naturellement. D’une part en me demandant depuis
quelque temps déjà ce qu’internet peut apporter sur le plan de la forme
à la littérature (...) et d’autre part en lisant de la littérature
"épistolaire" du 18e  siècle, ces fameux "romans par lettres". Il
suffit alors de transposer: que peut être le "roman par lettres"
aujourd’hui?»

Le déroulement de ce mail-roman a-t-il correspondu à ses attentes? «Oui
ET non: au départ je n'avais pas d'attente, donc oui... De plus, si je
n'avais pas d'attentes (…) je savais jusqu'où j'étais prêt à aller. Par
exemple, je proposais aux lecteurs de participer au roman mais je n'ai
jamais proposé qu'ils me remplacent: je voulais rester le maître (ah
mais...). Ce qui m'amusait, c'était d'intégrer, dans une trame et une
visée que je m'étais à peu près données, les propositions, y compris
les plus farfelues, sans qu'elles paraissent comme telles et sans que
je "vende mon âme au diable".

NON car j'ai quand même été un peu surpris du "classicisme" des
propositions de lecteurs: on y retrouvait assez massivement les lieux
communs les plus éculés (pardon pour le jeu de mot...) des feuilletons
télévisés. Si je me laissais faire, nous n'étions pas loin du Loft.
D'ailleurs, significativement, parce que c'était la période de
diffusion de cette émission, plusieurs lecteurs y font référence dans
leurs envois et essaient de m'entraîner sur ce terrain. Autrement dit,
le plus surprenant peut-être est que des lecteurs qui s'inscrivaient
volontairement à une expérience "littéraire" n'avaient de cesse de
regarder du côté de la non-littérature, de la banalité et du lieu
commun...»

Jean-Pierre Balpe tire plusieurs conclusions de cette expérience:
«D’abord c’est un "genre": depuis, plusieurs personnes m’ont dit lancer
aussi un mail-roman. Ensuite j’ai aperçu quantité de possibilités que
je n’ai pas exploitées et que je me réserve pour un éventuel travail
ultérieur. La contrainte du temps est ainsi très intéressante à
exploiter: le temps de l’écriture bien sûr, mais aussi celui de la
lecture: ce n’est pas rien de mettre quelqu’un devant la nécessité de
lire, chaque jour, une page de roman. Ce "pacte" a quelque chose de
diabolique. Et enfin le renforcement de ma conviction que les
technologies numériques sont une chance extraordinaire du
renouvellement du littéraire.»



2001 > WIKIPÉDIA, ENCYCLOPÉDIE COLLABORATIVE


[Résumé]
Fondée en janvier 2001 à l'initiative de Jimmy Wales et Larry Sanger
(Larry quitte plus tard l'équipe), Wikipédia est une encyclopédie
gratuite en ligne écrite collectivement et dont le contenu est
librement réutilisable. Sans publicité et financée par des dons, elle
est rédigée par des milliers de volontaires, avec possibilité pour tout
un chacun d’écrire, corriger ou compléter les articles, aussi bien les
siens que ceux d’autres contributeurs. Les articles restent la
propriété de leurs auteurs et leur libre utilisation est régie par la
licence GFDL (GNU Free Documentation License) ou la licence Creative
Commons. En décembre 2006, Wikipédia est l’un des dix sites les plus
visités du web, avec 6 millions d'articles dans 250 langues. En 2009,
Wikipédia est l'un des cinq sites les plus visités du web, le français
étant la troisième langue de l’encyclopédie, après l’anglais et
l’allemand. En janvier 2011, Wikipédia fête ses dix ans d’existence
avec 17 millions d’articles dans 270 langues et 400 millions de
visiteurs par mois pour l’ensemble de ses sites.

***

Lancée en janvier 2001, Wikipédia est une encyclopédie gratuite en
ligne écrite collectivement et dont le contenu est librement
réutilisable.

Qu’est-ce qu’un wiki? Un wiki (terme hawaïen signifiant «vite») est un
site web permettant à plusieurs utilisateurs de collaborer
simultanément en ligne, en rédigeant le contenu du wiki, en le
modifiant et en l'enrichissant en permanence. Le wiki est utilisé par
exemple pour créer et gérer des sites d’information, des dictionnaires
et des encyclopédies. Le programme présent derrière l'interface d'un
wiki est plus ou moins élaboré. Un programme simple gère des textes et
des hyperliens. Un programme élaboré permet d'inclure des images, des
graphiques, des tableaux, etc.

Fondée en janvier 2001 à l’initiative de Jimmy Wales et Larry Sanger
(Larry Sanger quitte plus tard l'équipe), Wikipédia est immédiatement
très populaire. Sans publicité et financée par des dons, elle est
rédigée par des milliers de volontaires - qui s'inscrivent sous un
pseudonyme - avec possibilité pour tout un chacun d’écrire, corriger et
compléter les articles, aussi bien les siens que ceux d'autres
contributeurs. Les articles restent la propriété de leurs auteurs et
leur libre utilisation est régie par la licence Creative Commons ou la
licence GFDL (GNU Free Documentation License).

Créée en juin 2003, la Wikimedia Foundation gère non seulement
Wikipédia mais aussi Wiktionary, un dictionnaire et thésaurus
multilingue lancé en décembre 2002, Wikibooks (livres et manuels en
cours de rédaction) lancé en juin 2003, auxquels s'ajoutent ensuite
Wikiquote (répertoire de citations), Wikisource (textes appartenant au
domaine public), Wikimedia Commons (sources multimédia), Wikispecies
(répertoire d'espèces animales et végétales), Wikinews (site
d'actualités) et enfin Wikiversity (matériel d'enseignement), lancé en
août 2006.

En décembre 2004, Wikipédia compte 1,3 million d’articles rédigés par
13.000 contributeurs dans une centaine de langues. En décembre 2006,
Wikipédia compte 6 millions d’articles dans 250 langues et devient l'un
de dix sites les plus visités du web. En mai 2007, 7 millions
d'articles sont disponibles dans 192 langues, dont 1,8 million
d’articles en anglais, 589.000 articles en allemand, 500.000 articles
en français, 260.000 articles en portugais et 236.000 articles en
espagnol. En 2009, l’encyclopédie est l’un des cinq sites les plus
visités du web. En septembre 2010, Wikipédia compte 14 millions
d'articles dans 272 langues, dont 3,4 millions d’articles en anglais,
1,1 million d’articles en allemand et 1 million d’articles en français,
qui est toujours la troisième langue de l'encyclopédie.

Wikipédia fête ses dix ans en janvier 2011 avec 17 millions d’articles
dans 270 langues et 400 millions de visiteurs par mois pour l’ensemble
de ses sites.

De plus, Wikipédia inspire bien d’autres projets au fil des ans, par
exemple Citizendium, lancé en mars 2007 par Larry Sanger en tant
qu’encyclopédie collaborative expérimentale au contenu vérifié par des
experts, ou encore l’Encyclopedia of Life, un projet global qui voit le
jour en mai 2007 pour recenser toutes les espèces animales et végétales
connues.



2001 > D’AUTRES TABLETTES DE LECTURE


[Résumé]
Après le Rocket eBook et le SoftBook Reader, apparus en 1998, les
expériences se poursuivent avec le lancement d’autres tablettes de
lecture au début des années 2000, les plus connues étant le Gemstar
eBook aux États-Unis et le Cybook en Europe. Grande société américaine
spécialisée dans les produits et services numériques pour les médias,
Gemstar lance le Gemstar eBook en novembre 2000 après avoir racheté les
deux sociétés californiennes auteures des deux premières tablettes de
lecture du marché, à savoir Nuvomedia (auteur du Rocket eBook) et
SoftBook Press (auteur du SoftBook Reader). Première tablette de
lecture européenne, le Cybook est lancé en janvier 2001 par la société
française Cytale. Sa mémoire - 32 Mo de mémoire SDRAM (Synchronous
Dynamic Random Access Memory) et 16 Mo de mémoire flash - permet de
stocker 15.000 pages de texte, soit 30 livres de 500 pages, dans un
appareil de 21 x 16 cm pesant un kilo.

***

Après le Rocket eBook et le SoftBook Reader, apparus en 1998, d’autres
tablettes de lecture voient le jour au début des années 2000, les plus
connues étant le Gemstar eBook aux États-Unis et le Cybook en Europe.

# Le Gemstar eBook

Grande société américaine spécialisée dans les produits et services
numériques pour les médias, Gemstar lance le Gemstar eBook en novembre
2000 aux États-Unis après avoir racheté les deux sociétés
californiennes auteures des premières tablettes de lecture du marché, à
savoir Nuvomedia (auteur du Rocket eBook) et SoftBook Press (auteur du
SoftBook Reader).

Le Gemstar eBook se décline en deux modèles - le REB 1100 (écran noir
et blanc, successeur du Rocket eBook) et le REB 1200 (écran couleur,
successeur du SoftBook Reader) - construits et vendus sous le label
RCA, appartenant à Thomson Multimedia. Le système d’exploitation, le
navigateur et le logiciel de lecture sont spécifiques à l'appareil,
tout comme le format de lecture, basé sur le format OeB (Open eBook).
Les deux modèles sont vendus respectivement 300 et 699 dollars US par
la chaîne de magasins SkyMall.

Les ventes sont très inférieures aux pronostics. En avril 2002, un
article du New York Times annonce l’arrêt de la fabrication de ces
tablettes par RCA. Lancés à l’automne 2002, les modèles suivants - le
GEB 1150 et le GEB 2150 - sont produits cette fois sous le label
Gemstar et vendus par SkyMall à un prix beaucoup plus compétitif, avec
ou sans abonnement annuel ou bisannuel à la librairie numérique du
Gemstar eBook. Le GEB 1150 coûte 199 dollars sans abonnement, et 99
dollars avec abonnement annuel (facturé 20 dollars par mois). Le GEB
2150 coûte 349 dollars sans abonnement, et 199 dollars avec abonnement
bisannuel (facturé lui aussi 20 dollars par mois).

Mais les ventes restent peu concluantes - faute d'un marché mûr pour ce
genre d'appareil - et Gemstar décide de mettre fin à ses activités
eBook. La société cesse la vente de ses tablettes de lecture en juin
2003 et la vente de ses livres numériques le mois suivant.

# Le Cybook

Première tablette de lecture européenne, le Cybook est lancé en janvier
2001 par la société française Cytale.

Sa mémoire - 32 Mo de mémoire SDRAM (Synchronous Dynamic Random Access
Memory) et 16 Mo de mémoire flash - permet de stocker 15.000 pages de
texte, soit 30 livres de 500 pages, dans un appareil de 21 x 16 cm
pesant un kilo.

Olivier Pujol, PDG de Cytale, écrit en décembre 2000: «J’ai croisé il y
a deux ans le chemin balbutiant d’un projet extraordinaire, le livre
électronique. Depuis ce jour, je suis devenu le promoteur impénitent de
ce nouveau mode d’accès à l’écrit, à la lecture, et au bonheur de lire.
La lecture numérique se développe enfin, grâce à cet objet merveilleux:
bibliothèque, librairie nomade, livre "adaptable", et aussi moyen
d’accès à tous les sites littéraires (ou non), et à toutes les
nouvelles formes de la littérature, car c’est également une fenêtre sur
le web.»

Quelle est exactement l’activité de Cytale? «Conception et
commercialisation d'un livre électronique, conception, développement et
gestion d'un site internet de diffusion de livres numériques,
préparation et formatage de livres numériques.»

Quelles sont les perspectives? «L'utilisation d'internet pour le
transport de contenu est un secteur de développement majeur. La société
a pour vocation de développer une base de contenu en provenance
d'éditeurs, et de les diffuser vers des supports de lecture sécurisés.»

Plus généralement, «le livre électronique, permettant la lecture
numérique, ne concurrence pas le papier. C'est un complément de
lecture, qui ouvre de nouvelles perspectives pour la diffusion de
l'écrit et des oeuvres mêlant le mot et d'autres médias (image, son,
image animée...). Les projections montrent une stabilité de l'usage du
papier pour la lecture, mais une croissance de l'industrie de
l'édition, tirée par la lecture numérique, et le livre électronique. De
la même façon que la musique numérique a permis aux mélomanes d'accéder
plus facilement à la musique, la lecture numérique supprime, pour les
jeunes générations commme pour les autres, beaucoup de freins à l'accès
à l'écrit.»

Les ventes du Cybook sont très inférieures aux pronostics et forcent la
société à cesser ses activités en juillet 2002. La commercialisation du
Cybook est reprise par la société Bookeen, créée en 2003 à l’initiative
de Michael Dahan et Laurent Picard, deux ingénieurs de Cytale. Le
Cybook 2e génération est lancé en avril 2004. Bookeen dévoile en
juillet 2007 une nouvelle version de sa tablette, baptisée Cybook Gen3
(3e génération), avec un écran utilisant la technologie E Ink.



2001 > UNE MEILLEURE BANDE PASSANTE


[Résumé]
Henk Slettenhaar est spécialiste des systèmes de communication, avec
une longue carrière à Genève et en Californie.  En 1992, il crée la
Silicon Valley Association (SVA), une association suisse qui organise
des voyages d'étude dans des pôles de haute technologie, à commencer
par la Silicon Valley et San Francisco. Henk mentionne en juillet 2001
«le changement considérable apporté par le fait que j'ai maintenant une
connexion à débit rapide chez moi. Le fait d'être constamment connecté
est totalement différent du fait de se connecter de temps à autre par
la ligne téléphonique. Je reçois maintenant mes messages dès leur
arrivée dans ma messagerie. Je peux écouter mes stations radio
préférées où qu'elles soient. Je peux écouter les actualités quand je
veux. Et aussi écouter la musique que j'aime à longueur de journée. (…)
La seule chose qui manque est une vidéo de bonne qualité en temps réel.
La largeur de bande est encore insuffisante pour cela.» Dix ans plus
tard, Henk peut visionner des vidéos en temps réel et lire des livres
numériques sur le Kindle et l’iPad.

***

Henk Slettenhaar est spécialiste des systèmes de communication, avec
une longue carrière à Genève et en Californie.

En 1958, Henk rejoint le CERN (Laboratoire européen pour la physique
des particules) à Genève pour travailler sur le premier ordinateur
numérique et participer au développement des premiers réseaux
numériques.

Son expérience californienne débute en 1966 lorsqu’il rejoint pendant
dix-huit mois une équipe du SLAC (Stanford Linear Accelerator Center)
pour créer un numérisateur de film. De retour au SLAC en 1983, il
conçoit un système numérique de contrôle qui sera utilisé pendant dix
ans.

Henk est ensuite professeur en technologies de communication à la
Webster University de Genève pendant 25 ans. Il est l'ancien directeur
du Telecom Management Program créé à l'automne 2000. Il travaille
également en tant que consultant auprès de nombreuses organisations
internationales.

# En 1992

En 1992, fort de son expérience en Suisse et en Californie, Henk crée
la Silicon Valley Association (SVA), une association suisse qui
organise des voyages d'étude dans des pôles de haute technologie, à
commencer par la Silicon Valley et San Francisco. Outre des visites de
sociétés, de start-up, d’universités et de centres de recherche, ces
voyages comprennent des conférences, des présentations et des
discussions sur les technologies de l'information (internet,
multimédia, télécommunications, etc.), les derniers développements de
la recherche et de ses applications, et les méthodes les plus récentes
en matière de stratégie commerciale et de création d'entreprise.

# En 1998

Henk raconte en décembre 1998: «Je ne peux pas imaginer ma vie
professionnelle sans l'internet. Cela fait vingt ans que j'utilise le
courrier électronique. Les premières années, c'était le plus souvent
pour communiquer avec mes collègues dans un secteur géographique très
limité. Depuis l'explosion de l'internet et l'avènement du web, je
communique principalement par courriel, mes conférences sont en grande
partie sur le web et mes cours ont tous un prolongement sur le web. En
ce qui concerne les visites que j'organise dans la Silicon Valley,
toutes les informations sont disponibles sur le web, et je ne pourrais
pas organiser ces visites sans utiliser l'internet. De plus, l'internet
est pour moi une fantastique base de données disponible en quelques
clics de souris.»

# En 2000

Quoi de neuf en août 2000? «L'explosion de la technologie du mobile. Le
téléphone mobile est devenu pour beaucoup de gens, moi y compris, le
moyen de communication personnel vous permettant d'être joignable à
tout moment où que vous soyiez. Toutefois l'internet mobile est encore
du domaine du rêve. Les nouveaux services offerts par les téléphones
GSM sont extrêmement primitifs et très chers, si bien que le WAP a reçu
le sobriquet de "Wait And Pay".»

# En 2001

Quoi de neuf un an après, en juillet 2001? «Ce qui me vient à l'esprit
est le changement considérable apporté par le fait que j'ai maintenant
une connexion à débit rapide chez moi. Le fait d'être constamment
connecté est totalement différent du fait de se connecter de temps à
autre par la ligne téléphonique. Je reçois maintenant mes messages dès
leur arrivée dans ma messagerie. Je peux écouter mes stations radio
préférées où qu'elles soient. Je peux écouter les actualités quand je
veux. Et aussi écouter la musique que j'aime à longueur de journée. (…)
La seule chose qui manque est une vidéo de bonne qualité en temps réel.
La largeur de bande est encore insuffisante pour cela.

Mon domicile est maintenant équipé d'un réseau local avec et sans fil.
Je peux utiliser mon ordinateur portable partout à l'intérieur et à
l'extérieur de la maison, et même chez les voisins, tout en restant
connecté. La même technologie me permet maintenant d'utiliser la carte
de réseau local sans fil de mon ordinateur lorsque je voyage. Par
exemple, lors de mon dernier voyage à Stockholm, je pouvais être
connecté à l'hôtel, au centre de conférences, à l'aéroport et même au
pub irlandais!»

# En 2011

Dix ans plus tard, en juin 2011, Henk raconte: «J'ai toujours suivi le
développement des ebooks avec beaucoup d'intérêt, étant professeur de
systèmes de communication et organisateur de voyages dans la Silicon
Valley. Mon utilisation était très limitée pendant près de quarante
ans, à cause du manque de progrès des appareils de lecture. Je n'ai
jamais aimé lire un livre sur un ordinateur ou sur un PDA. Maintenant,
avec l’arrivée de tablettes comme le Kindle et l’iPad, je suis
finalement devenu un lecteur de livres numériques. Je vois un expansion
énorme avec l'arrivée de tablettes faciles a utiliser et avec un choix
considérable de livres grâce au commerce électronique et à des sociétés
comme Amazon.»

Sur quoi travaille-t-il ces temps-ci? «Je suis un "serial entrepreneur"
actuellement en train de créer une start-up dans le domaine de la
mobilité. J'utilise l'internet tout le temps pour trouver des
partenaires et des idées. Nous utilisons également des livres en ligne
pour apprendre l'art de l'innovation!»



2001 > CREATIVE COMMONS, LE COPYRIGHT REVISITÉ


[Résumé]
Créée en 2001 à l'initiative de Lawrence «Larry» Lessig, professeur de
droit à la Stanford Law School, en Californie, la licence Creative
Commons a pour but de favoriser la diffusion d'oeuvres numériques tout
en protégeant le droit d'auteur. Des contrats flexibles de droit
d'auteur compatibles avec une diffusion sur l'internet sont proposés
pour tout type de création: texte, film, photo, musique, site web, etc.
L'auteur peut par exemple choisir d'autoriser ou non la reproduction et
la rediffusion de ses oeuvres. Finalisée en février 2007, la version
3.0 de la Creative Commons instaure une licence internationale et la
compatibilité avec d'autres licences similaires, dont le copyleft et la
GPL (General Public License). La Creative Commons est utilisée pour un
million d'oeuvres en 2003, 4,7 millions d'oeuvres en 2004, 20 millions
d'oeuvres en 2005, 50 millions d'oeuvres en 2006, 90 millions d'oeuvres
en 2007, 130 millions d'oeuvres en 2008 et 350 millions d'oeuvres en
avril 2010.

***

Créée en 2001, la licence Creative Commons a pour but de favoriser la
diffusion d'oeuvres numériques tout en protégeant le droit d'auteur.

Des créateurs souhaitent en effet respecter la vocation première de
l’internet, réseau de diffusion à l’échelon mondial. De ce fait, les
adeptes de contrats flexibles sont de plus en plus nombreux, à
commencer par le copyleft, qui apparaît bien avant la licence Creative
Commons.

# Le copyleft et la GPL

L'idée du copyleft est lancée dès 1984 par Richard Stallman, ingénieur
en informatique et défenseur du mouvement Open Source au sein de la
Free Software Foundation (FSF). Conçu à l’origine pour les logiciels,
le copyleft est formalisé par la GPL (General Public License) et étendu
par la suite à toute oeuvre de création. Il contient la déclaration
normale du copyright affirmant le droit d'auteur, mais son originalité
est de donner au lecteur le droit de librement redistribuer le document
et de le modifier. Le lecteur s’engage toutefois à ne revendiquer ni le
travail original, ni les changements faits par d’autres personnes. De
plus, tous les travaux dérivés de l’oeuvre originale sont eux-mêmes
soumis au copyleft.

# La Creative Commons

Lancée en 2001 à l'initiative de Lawrence «Larry» Lessig, professeur de
droit à la Stanford Law School, en Californie, la licence Creative
Commons a elle aussi pour but de favoriser la diffusion d'œuvres
numériques tout en protégeant le droit d'auteur. L'organisme du même
nom propose des licences-type, qui sont des contrats flexibles de droit
d'auteur compatibles avec une diffusion sur l'internet. Simplement
rédigées, ces autorisations non exclusives permettent aux titulaires
des droits d'autoriser le public à utiliser leurs créations tout en
ayant la possibilité de restreindre les exploitations commerciales et
les oeuvres dérivées et d’autoriser ou non la reproduction et la
rediffusion de leurs œuvres, par exemple. Ces contrats peuvent être
utilisés pour tout type de création: texte, film, photo, musique, site
web, etc. Finalisée en février 2007, la version 3.0 de la Creative
Commons instaure une licence internationale et la compatibilité avec
d'autres licences similaires, dont le copyleft et la GPL.

# Qui utilise la licence Creative Commons?

O’Reilly Media par exemple. Fondé par Tim O’Reilly en 1978, O’Reilly
Media est un éditeur réputé de manuels informatiques et de livres sur
les technologies de pointe. L'éditeur offre d’abord une formule de
«copyright ouvert» pour les auteurs qui le souhaitent ou pour des
projets collectifs. En 2003, il privilégie le Creative Commons
Founders’ Copyright permettant d’offrir des contrats flexibles de droit
d’auteur à ceux qui veulent également diffuser leurs oeuvres sur le
web.

La Public Library of Science (PLoS) utilise la licence Creative Commons
pour les articles de ses périodiques scientifiques et médicaux en ligne
gratuits, lancés à partir de 2003. Les articles peuvent être librement
diffusés et réutilisés ailleurs, y compris pour des traductions, la
seule contrainte étant la mention des auteurs et de la source.

Wikipédia utilise une licence Creative Commons pour les articles de
cette grande encyclopédie collaborative en ligne lancée en 2001 et
rédigée par des milliers de contributeurs.

Une licence Creative Commons est utilisée pour un million d'oeuvres en
2003, 4,7 millions d'oeuvres en 2004, 20 millions d'oeuvres en 2005, 50
millions d'oeuvres en 2006, 90 millions d'oeuvres en 2007, 130 millions
d'oeuvres en 2008 et 350 millions d'oeuvres en avril 2010.



2003 > LA PUBLIC LIBRARY OF SCIENCE


[Résumé]
Fondée en octobre 2000, la Public Library of Science (PLoS) devient en
janvier 2003 un éditeur de périodiques scientifiques et médicaux en
ligne gratuits de haut niveau. Une équipe éditoriale est constituée
pour lancer les deux premiers titres - PLoS Biology en octobre 2003
puis PLoS Medicine en 2004 - selon un nouveau modèle d'édition en ligne
basé sur la diffusion libre du savoir. Trois nouveaux titres voient le
jour en 2005: PLoS Genetics, PLoS Computational Biology et PLoS
Pathogens. PLoS Clinical Trials est lancé en mai 2006. PLoS Neglected
Tropical Diseases voit le jour à l’automne 2007 en tant que première
publication scientifique consacrée aux maladies tropicales négligées.
Tous les articles peuvent être diffusés et réutilisés ailleurs, y
compris pour des traductions, selon les termes de la licence Creative
Commons, la seule contrainte étant la mention des auteurs et de la
source.

***

Fondée en octobre 2000, la Public Library of Science (PLoS) devient en
janvier 2003 un éditeur de périodiques scientifiques et médicaux en
ligne gratuits de haut niveau.

Quel est le constat de départ? À l’heure de l’internet, il paraît assez
scandaleux que le résultat de travaux de recherche - travaux originaux
et demandant de longues années d’efforts - soit «détourné» par des
éditeurs spécialisés s’appropriant ce travail et le monnayant au prix
fort. L’activité des chercheurs est souvent financée par les deniers
publics, et de manière substantielle en Amérique du Nord. Il semblerait
donc normal que la communauté scientifique et le grand public puissent
bénéficier librement du résultat de ces recherches. Dans le domaine
scientifique et médical par exemple, 1.000 nouveaux articles sont
publiés quotidiennement, en ne comptant que les articles révisés par
les pairs.

# PLoS en tant qu’«agitateur»

Se basant sur ce constat, la Public Library of Science (PLoS) est
fondée en octobre 2000 à San Francisco (Californie) à l’initiative de
Harold Varmus, Patrick Brown et Michael Eisen, chercheurs dans les
Universités de Stanford et de Berkeley. Le but est de contrer les
pratiques de l’édition spécialisée en regroupant tous les articles
scientifiques et médicaux au sein d’archives en ligne en accès libre.
Au lieu d’une information disséminée dans des milliers de périodiques
en ligne ayant chacun des conditions d’accès différentes, un point
d’accès unique permettrait de lire le contenu intégral de ces articles,
avec moteur de recherche multicritères et système d’hyperliens entre
les articles.

Pour ce faire, PLoS fait circuler une lettre ouverte demandant que les
articles publiés par les éditeurs spécialisés soient distribués
librement dans un service d’archives en ligne, et incitant les
signataires de cette lettre à promouvoir les éditeurs prêts à soutenir
ce projet. La réponse de la communauté scientifique internationale est
remarquable. Au cours des deux années suivantes, la lettre ouverte est
signée par 30.000 chercheurs dans 180 pays. Bien que la réponse des
éditeurs soit nettement moins enthousiaste, plusieurs éditeurs donnent
leur accord pour une distribution immédiate des articles publiés par
leurs soins, ou alors une distribution dans un délai de six mois. Mais,
dans la pratique, même les éditeurs ayant donné leur accord formulent
nombre d’objections au nouveau modèle proposé, si bien que le projet
d’archives en ligne ne voit finalement pas le jour.

# PLoS en tant qu’éditeur

Un autre objectif de la Public Library of Science est de devenir elle-
même éditeur. PLoS fonde donc une maison d’édition scientifique non
commerciale qui reçoit en décembre 2002 une subvention de 9 millions de
dollars US de la part de la Moore Foundation. Une équipe éditoriale de
haut niveau est constituée en janvier 2003 pour lancer des périodiques
de qualité selon un nouveau modèle d’édition en ligne basé sur la
diffusion libre du savoir.

Le premier numéro de PLoS Biology est disponible en octobre 2003, avec
une version en ligne gratuite et une version imprimée au prix coûtant
(couvrant uniquement les frais de fabrication et de distribution). PLoS
Medicine est lancé en octobre 2004. Trois nouveaux titres voient le
jour en 2005: PLoS Genetics, PLoS Computational Biology et PLoS
Pathogens. PLoS Clinical Trials est lancé en 2006. PLoS Neglected
Tropical Diseases voit le jour à l’automne 2007 en tant que première
publication scientifique consacrée aux maladies tropicales négligées.

Tous les articles de ces périodiques sont librement accessibles en
ligne, sur le site de PLoS et dans PubMed Central, le service
d’archives en ligne public et gratuit de la National Library of
Medicine (États-Unis), avec moteur de recherche multicritères. Les
versions imprimées sont abandonnées en 2006 pour laisser place à un
service d’impression à la demande géré par la société Odyssey Press.
Les articles peuvent être librement diffusés et réutilisés ailleurs, y
compris pour des traductions, selon les termes de la licence Creative
Commons, la seule contrainte étant la mention des auteurs et de la
source. PLoS lance aussi PLoS ONE, un forum en ligne permettant la
publication d’articles sur tout sujet scientifique et médical.

Le succès est total. Trois ans après les débuts de la Public Library of
Science en tant qu’éditeur, PLoS Biology et PLos Medicine ont la même
réputation d’excellence que les grandes revues Nature, Science ou The
New England Journal of Medicine. PLoS reçoit le soutien financier de
plusieurs fondations tout en mettant sur pied un modèle économique
viable, avec des revenus émanant des frais de publication payés par les
auteurs, et émanant aussi de la publicité, des sponsors et des
activités destinées aux membres de PLoS. PLoS souhaite en outre que ce
modèle économique d’un genre nouveau inspire d’autres éditeurs pour
créer des revues du même type ou pour mettre des revues existantes en
accès libre.



2003 > HANDICAPZÉRO, L'INTERNET POUR TOUS


[Résumé]
Un enjeu important est l’information accessible à tous. Mis en ligne en
septembre 2000, le site Handicapzéro devient en février 2003 un portail
généraliste offrant un accès adapté à l’information pour les
francophones ayant un problème visuel, à savoir plus de 10% de la
population. Les aveugles peuvent accéder au site au moyen d’une plage
braille ou d’une synthèse vocale. Les malvoyants peuvent paramétrer sur
la page d’accueil la taille et la police des caractères ainsi que la
couleur du fond d’écran pour une navigation confortable. Les voyants
peuvent correspondre en braille avec des aveugles par le biais du site.
Plus de 2 millions de visiteurs utilisent les services du portail au
cours de l'année 2006. Handicapzéro entend ainsi démontrer «que, sous
réserve du respect de certaines règles élémentaires, l’internet peut
devenir enfin un espace de liberté pour tous.»

***

Un enjeu important est l’information accessible à tous. Le site
Handicapzéro devient en février 2003 un portail généraliste offrant un
accès adapté à l’information pour les francophones ayant un problème
visuel.

Depuis sa création en 1987, l’association Handicapzéro a pour but
d’améliorer l’autonomie de ces personnes, à savoir plus de 10% de la
population francophone. En France par exemple, une personne sur mille
est aveugle, une personne sur cent est malvoyante et une personne sur
deux a des problèmes de vue.

Mis en ligne en septembre 2000, le premier site web de l’association
devient rapidement le site adapté le plus visité de France, avec 10.000
requêtes mensuelles.

En février 2003, l'association lance un portail offrant en accès libre
l’information nationale et internationale en temps réel (en partenariat
avec l’Agence France-Presse), l’actualité sportive (avec L’Équipe), les
programmes de télévision (avec Télérama), la météo (avec Météo France)
et un moteur de recherche (avec Google), ainsi que toute une gamme de
services dans les domaines de la santé, de l’emploi, de la
consommation, des loisirs, des sports et de la téléphonie.

Les aveugles peuvent accéder au site au moyen d’une plage braille ou
d’une synthèse vocale. Les malvoyants peuvent paramétrer sur la page
d’accueil la taille et la police des caractères ainsi que la couleur du
fond d’écran pour une navigation confortable, en créant puis modifiant
leur profil selon leur potentiel visuel. Ce profil est utilisable aussi
pour la lecture de n’importe quel texte situé sur le web, en faisant un
copier-coller dans la fenêtre prévue à cet effet. Les voyants peuvent
correspondre en braille avec des aveugles par le biais du site,
Handicapzéro assurant gratuitement la transcription et l’impression
braille des courriers ainsi que leur expédition par voie postale en
Europe.

Plus de 2 millions de visiteurs utilisent les services du portail au
cours de l'année 2006. Handicapzéro entend ainsi démontrer «que, sous
réserve du respect de certaines règles élémentaires, l’internet peut
devenir enfin un espace de liberté pour tous.»

Qu’en est-il pour l’accès aux livres? Patrice Cailleaud, directeur de
la communication de Handicapzéro, explique en janvier 2001 que, si le
livre numérique est «une nouvelle solution complémentaire aux problèmes
des personnes aveugles et malvoyantes, (...) les droits et
autorisations d’auteurs demeurent des freins pour l’adaptation en
braille ou caractères agrandis d’ouvrage. Les démarches sont
saupoudrées, longues et n’aboutissent que trop rarement.»

D’où la nécessité impérieuse de lois nationales suite à une loi
internationale du droit d’auteur pour les personnes atteintes de
déficience visuelle. Dans l’Union européenne, la directive 2001/29/CE
de mai 2001 sur «l’harmonisation de certains aspects du droit d’auteur
et des droits voisins dans la société de l’information» insiste dans
son article 43 sur la nécessité pour les États membres d’adopter
«toutes les mesures qui conviennent pour favoriser l’accès aux oeuvres
pour les personnes souffrant d’un handicap qui les empêche d’utiliser
les oeuvres elles-mêmes, en tenant plus particulièrement compte des
formats accessibles». Il reste à appliquer cet article à large échelle.



2003 > LE MATÉRIEL D'ENSEIGNEMENT DU MIT


[Résumé]
En septembre 2003, le MIT (Massachusetts Institute of Technology) lance
officiellement le MIT OpenCourseWare pour mettre le matériel
d'enseignement de ses cours à la disposition de tous, avec accès libre
et gratuit. Ce matériel comprend des textes de conférences, des travaux
pratiques, des exercices et corrigés, des bibliographies, des documents
audio et vidéo, etc. Le site donne accès au matériel d’une centaine de
cours en septembre 2003, 500 cours en mars 2004, 1.400 cours en mai
2006 et 1.800 cours en novembre 2007, à savoir la totalité des cours
dispensés par le MIT. Le matériel de ces cours est ensuite
régulièrement actualisé. Certains cours sont traduits en espagnol, en
portugais et en chinois avec l’aide d’autres organismes. En décembre
2005 est lancé en parallèle l'OpenCourseWare Consortium (OCW
Consortium) pour proposer le matériel d’enseignement d’autres
universités, avec cent universités participantes un an plus tard.

***

Le MIT (Massachusetts Institute of Technology) décide de mettre le
matériel d’enseignement de ses cours à la disposition de tous dans un
OpenCourseWare, avec accès libre et gratuit.

Qu’est-ce qu’un OpenCourseWare? Un OpenCourseWare peut être défini
comme la publication électronique en accès libre du matériel
d’enseignement d'un ensemble de cours.

Professeur à l’Université d’Ottawa (Canada), Christian Vandendorpe
salue dès mai 2001 «la décision du MIT de placer tout le contenu de ses
cours sur le web d’ici dix ans, en le mettant gratuitement à la
disposition de tous. Entre les tendances à la privatisation du savoir
et celles du partage et de l’ouverture à tous, je crois en fin de
compte que c’est cette dernière qui va l’emporter.»

Mise en ligne en septembre 2002, la version pilote du MIT
OpenCourseWare (MIT OCW) offre en accès libre le matériel
d’enseignement de 32 cours représentatifs des cinq facultés du MIT. Ce
matériel d’enseignement comprend des textes de conférences, des travaux
pratiques, des exercices et corrigés, des bibliographies, des documents
audio et vidéo, etc. Cette initiative est menée avec le soutien
financier de la Hewlett Foundation et de la Mellon Foundation.

Le lancement officiel du site a lieu un an plus tard, en septembre
2003, avec accès au matériel d’une centaine de cours. 500 cours sont
disponibles en mars 2004 et  1.400 cours en mai 2006. Le matériel de la
totalité des 1.800 cours dispensés par le MIT est en ligne en novembre
2007. Il est ensuite régulièrement actualisé. Certains cours sont
traduits en espagnol, en portugais et en chinois avec l’aide d’autres
organismes.

Le MIT espère que cette expérience de publication électronique - la
première du genre - va permettre de définir un standard et une méthode
de publication, et va inciter d’autres universités à créer un
OpenCourseWare pour la mise à disposition gratuite du matériel de leurs
propres cours. A cet effet, le MIT lance l’OpenCourseWare Consortium
(OCW Consortium) en décembre 2005, avec accès libre et gratuit au
matériel d’enseignement de cent universités dans le monde un an plus
tard.



2004 > LE WEB 2.0, COMMUNAUTÉ ET PARTAGE


[Résumé]
Le terme «web 2.0» émane d'un éditeur de livres informatiques, Tim
O'Reilly, qui l’utilise pour la première fois en 2004 en tant que titre
d’une série de conférences qu'il est en train d’organiser. Le web 2.0
est caractérisé par les notions de communauté et de partage, avec une
flopée de sites dont le contenu est alimenté par les utilisateurs, par
exemple les blogs, les wikis, les sites sociaux et les encyclopédies
collaboratives. Wikipédia, Facebook et Twitter bien sûr, mais aussi des
dizaines de milliers d'autres. Le web 2.0 tente de répondre au rêve
formulé par Tim Berners-Lee, inventeur du web en 1990, qui écrit dans
un essai daté d’avril 1998: «Le rêve derrière le web est un espace
d'information commun dans lequel nous communiquons en partageant
l'information. Son universalité est essentielle, à savoir le fait qu'un
lien hypertexte puisse pointer sur quoi que ce soit, quelque chose de
personnel, de local ou de global, aussi bien une ébauche qu'une
réalisation très sophistiquée.»

***

Le terme «web 2.0» émane d'un éditeur de livres informatiques, Tim
O'Reilly, qui l’utilise pour la première fois en 2004 en tant que titre
d’une série de conférences qu'il est en train d’organiser.

Le web 2.0 est caractérisé par les notions de communauté et de partage,
avec une flopée de sites dont le contenu est alimenté par les
utilisateurs, par exemple les blogs, les wikis, les sites sociaux et
les encyclopédies collaboratives. Wikipédia, Facebook et Twitter bien
sûr, mais aussi des dizaines de milliers d'autres.

# Les blogs envahissent la toile

Un blog (ou blogue) est un journal en ligne tenu par une personne ou un
groupe. Ce journal est le plus souvent présenté par ordre chronologique
inversé (du plus récent au plus ancien) et il est actualisé d'heure en
heure ou bien une fois par mois. Le premier blog apparaît en 1997. En
2004, Le Monde.fr, site du quotidien Le Monde, lance ses propres blogs,
«un formidable format d'expression journalistique qui permet un
dialogue quasi-instantané avec son lecteur», selon Yann Chapellon,
directeur du Monde interactif. En juillet 2005, il y aurait 14 millions
de blogs dans le monde, avec 80.000 nouveaux blogs par jour. En
décembre 2006, Technorati, moteur de recherche pour blogs puis site
spécialisé, recense 65 millions de blogs, avec 175.000 nouveaux blogs
par jour. Certains blogs sont consacrés aux photos (photoblogs), à la
musique (audioblogs ou podcasts) et aux vidéos (vidéoblogs ou vlogs).

# Les wikis, sites collaboratifs

Un wiki (terme hawaïen signifiant «vite») est un site web permettant à
plusieurs utilisateurs de collaborer en ligne sur un même projet. Le
concept du wiki devient très populaire en 2000, avec possibilité pour
les participants de contribuer à la rédaction du contenu, de modifier
ce contenu et de l'enrichir en permanence. Le wiki est utilisé par
exemple pour créer et gérer des sites d’information, des dictionnaires
et des encyclopédies. Le programme présent derrière l'interface d'un
wiki est plus ou moins élaboré. Un programme simple gère des textes et
des hyperliens. Un programme élaboré permet d'inclure des images, des
graphiques, des tableaux, etc. L'encyclopédie wiki la plus connue est
Wikipédia.

# Facebook, réseau social

Facebook est un réseau social fondé en février 2004 par Mark Zuckerberg
et ses collègues étudiants. Destiné à l'origine aux étudiants de
l'Université de Harvard, puis aux étudiants de toutes les universités
américaines, le réseau social s’ouvre au monde en septembre 2006 afin
de connecter entre eux des personnes proches (famille, amis, collègues)
ou des personnes partageant les mêmes centres d'intérêt. En juin 2010,
Facebook devient le deuxième site mondial en nombre de visites, après
Google, et fête ses 500 millions d'usagers tout en suscitant des débats
sur le respect de la vie privée.

# Twitter, l'information en 140 caractères

Lancé en 2006 par Jack Dorsey et Biz Stone, Twitter est un outil de
réseau social et de micro-blogging permettant à l'utilisateur d'envoyer
gratuitement des tweets (messages brefs au format texte) de 140
caractères maximum, par messagerie instantanée, par SMS ou via
l’internet. Parfois décrit comme le SMS de l'internet, Twitter gagne
rapidement une popularité mondiale, avec 106 millions d'usagers en
avril 2010 et 300.000 nouveaux usagers par jour. Quant aux tweets, on
compte 5.000 tweets quotidiens en 2007, 300.000 en 2008, 2,5 millions
en 2009, 50 millions en janvier 2010 et 55 millions en avril 2010, avec
un archivage systématique des tweets à usage public par la Bibliothèque
du Congrès en tant que reflet des tendances de notre époque.

# Le rêve de Tim Berners-Lee

Comme on le voit, le web 2.0 tente de répondre au rêve formulé par Tim
Berners-Lee, inventeur du web en 1990, qui écrit dans un essai daté
d’avril 1998: «Le rêve derrière le web est un espace d'information
commun dans lequel nous communiquons en partageant l'information. Son
universalité est essentielle, à savoir le fait qu'un lien hypertexte
puisse pointer sur quoi que ce soit, quelque chose de personnel, de
local ou de global, aussi bien une ébauche qu'une réalisation très
sophistiquée. Deuxième partie de ce rêve, le web deviendrait d'une
utilisation tellement courante qu'il serait un miroir réaliste (sinon
la principale incarnation) de la manière dont nous travaillons, jouons
et nouons des relations sociales. Une fois que ces interactions
seraient en ligne, nous pourrions utiliser nos ordinateurs pour nous
aider à les analyser, donner un sens à ce que nous faisons, et voir
comment chacun trouve sa place et comment nous pouvons mieux travailler
ensemble.» (extrait de «The World Wide Web: A very short personal
history»)



2005 > DU PDA AU SMARTPHONE


[Résumé]
En avril 2001, on compte 17 millions de PDA dans le monde pour
seulement 100.000 tablettes de lecture, d'après un Seybold Report
disponible en ligne. 13,2 millions de PDA sont vendus en 2001. Le
premier PDA du marché est le Palm Pilot, lancé en mars 1996, avec 23
millions de Palm Pilot vendus entre 1996 et 2002. Suit le Pocket PC de
Microsoft en mars 2000 avec son logiciel de lecture Microsoft Reader,
disponible ensuite pour toute plateforme Windows. En 2002, la gamme
Palm Pilot est toujours le leader du marché (36,8% des PDA vendus),
suivi de la gamme Pocket PC de Microsoft et des modèles de Hewlett-
Packard, Sony, Handspring, Toshiba et Casio. Les systèmes
d'exploitation utilisés sont essentiellement le Palm OS (pour 55% des
PDA) et le Pocket PC (pour 25,7% des PDA). Le PDA laisse ensuite
progressivement la place au smartphone, du modèle précurseur Nokia 9210
lancé en 2001 à l’iPhone d’Apple lancé en juin 2007.

***

En avril 2001, on compte 17 millions de PDA dans le monde pour
seulement 100.000 tablettes de lecture, d'après un Seybold Report
disponible en ligne. En 2005, le PDA laisse progressivement la place au
smartphone.

# Le Palm Pilot

Basée en Californie, la société Palm lance en mars 1996 le Palm Pilot,
premier PDA du marché, et vend 23 millions de Palm Pilot entre 1996 et
2002. Son système d'exploitation est le Palm OS et son logiciel de
lecture le Palm Reader. En mars 2001, les usagers peuvent également
lire des livres sur le Mobipocket Reader. À la même date, Palm rachète
Peanutpress.com, éditeur et distributeur de livres numériques pour PDA,
ainsi que son logiciel de lecture Peanut Reader et sa collection de
2.000 titres numériques, transférée dans Palm Digital Media, la
librairie numérique de Palm. En juillet 2002, le Palm Reader, jusque-là
utilisable sur Palm Pilot et sur Pocket PC, est également utilisable
sur ordinateur. À la même date, Palm Digital Media (renommée plus tard
Palm eBook Store) distribue 5.500 titres dans plusieurs langues. En
2003, le catalogue approche les 10.000 titres.

Un changement considérable donc pour les adeptes du livre numérique
qui, avant l’avènement du Palm Pilot, ne pouvaient lire les livres que
sur l’écran de leur ordinateur, portable ou non. Si certains
professionnels du livre s'inquiètent de la petitesse de l'écran, les
adeptes de la lecture sur PDA sont enthousiastes à l’idée de lire sur
un appareil mobile multitâches et ne se plaignent guère de la taille de
l'écran.

# L’eBookMan

Lancé par la société Franklin en 2000, l'eBookMan est un PDA multimédia
permettant de lire des livres numériques sur le Franklin Reader. Il
reçoit l’eBook Technology Award de la Foire internationale de Francfort
(Allemagne) en octobre 2000. Trois modèles (EBM-900, EBM-901 et EBM-
911) sont disponibles début 2001, avec une mémoire vive de 8 ou 16 Mo
et un écran LCD rétro-éclairé ou non. L’écran est nettement plus grand
que celui des autres PDA du marché, mais n'existe qu'en noir et blanc,
contrairement à la gamme Pocket PC de Microsoft ou à certains modèles
du Palm Pilot avec écran couleur.

L'eBookMan permet l'écoute de livres audionumériques et de fichiers
musicaux au format MP3. Le Mobipocket Reader est ajouté au Franklin
Reader en octobre 2001. Le Franklin Reader est également proposé par
défaut sur les gammes de PDA Psion, Palm et Pocket PC et sur le premier
smartphone, lancé la même année par Nokia. Franklin développe aussi une
librairie numérique sur son site et passe des partenariats avec
plusieurs sociétés, notamment avec Audible.com pour avoir accès à sa
collection de 4.500 livres audionumériques.

# D’autres modèles

En avril 2001, on compte 17 millions de PDA dans le monde pour
seulement 100.000 tablettes de lecture, d'après un Seybold Report
disponible en ligne. 13,2 millions de PDA sont vendus en 2001.

En 2002, la gamme Palm Pilot est toujours le leader du marché (36,8%
des PDA vendus), suivi de la gamme Pocket PC de Microsoft lancée en
mars 2000 et des modèles de Hewlett-Packard, Sony, Handspring, Toshiba
et Casio. Les systèmes d'exploitation utilisés sont essentiellement le
Palm OS (pour 55% des PDA) et le Pocket PC (pour 25,7% des PDA).

Pour mémoire, les grands logiciels de lecture sont le Mobipocket Reader
(disponible depuis mars 2000), le Microsoft Reader (disponible depuis
avril 2000), le Palm Reader (disponible depuis mars 2001), l’Acrobat
Reader (disponible depuis mai 2001 pour le Palm Pilot et décembre 2001
pour le Pocket PC) et l’Adobe Reader (lancé en mai 2003 pour remplacer
l’Acrobat Reader).

En 2003, des centaines de nouveautés sont vendues en version numérique
sur Amazon.com, Barnes & Noble.com, Yahoo! eBook Store ou sur des sites
d’éditeurs comme Random House ou PerfectBound. Le catalogue de Palm
Digital Media approche les 10.000 titres lisibles sur PDA, avec 15 à 20
nouveaux titres par jour et 1.000 nouveaux clients par semaine.
Mobipocket distribue 6.000 titres numériques dans plusieurs langues,
soit sur son site soit dans des librairies partenaires. Numilog
distribue 3.500 titres numériques (livres et périodiques) en français
et en anglais.

En 2004, les trois principaux fabricants de PDA sont Palm, Sony et
Hewlett-Packard. Suivent Handspring, Toshiba, Casio et d'autres. Mais
le PDA est de plus en plus concurrencé par le smartphone, qui est un
téléphone portable doublé d'un PDA, et les ventes commencent à baisser.
En février 2005, Sony décide de se retirer du marché des PDA.

# Les smartphones

Le premier smartphone est le Nokia 9210, modèle précurseur lancé en
2001 par la société finlandaise Nokia, grand fabricant mondial de
téléphones portables, avec une plateforme Symbian OS. Apparaissent
ensuite le Nokia Series 60, le Sony Ericsson P800, puis les modèles de
Motorola et de Siemens. Ces différents modèles permettent de lire des
livres numériques sur le Mobipocket Reader.

Appelé aussi téléphone multimédia, téléphone multifonctions ou encore
téléphone intelligent, le smartphone dispose d’un écran couleur, du son
polyphonique et de la fonction appareil photo, qui viennent s'ajouter
aux fonctions habituelles du PDA (agenda, dictaphone, lecteur de livres
numériques, lecteur de musique, etc.)

Les smartphones représentent 3,7% des ventes de téléphones mobiles en
2004 et 9% des ventes en 2006, à savoir 90 millions de smartphones pour
un milliard de téléphones portables comptabilisés sur la planète.

# L’iPhone

Présenté en janvier 2007 par Steve Jobs, l'iPhone, le smartphone
d’Apple, est un téléphone mobile multifonctions qui intègre le baladeur
de musique iPod (lancé lui-même en octobre 2001), un appareil photo et
un navigateur web, avec les caractéristiques suivantes: grand écran
tactile (3,5 pouces), synchronisation automatique avec la plateforme
iTunes pour télécharger musique et vidéos, appareil photo de 2
mégapixels, navigateur Safari d'Apple, système d'exploitation Mac OS X,
téléphonie par les réseaux GSM (Global System for Mobile
Telecommunications) et EDGE (Enhanced Data for GSM Evolution),
connexion internet via la WiFi (Wireless Fidelity) et enfin connexion
Bluetooth.

L'iPhone est lancé en juin 2007 aux États-Unis au prix de 499 dollars
US pour le modèle de 4 Go (giga-octets) et 599 dollars pour le modèle
de 8 Go. Son lancement à l’international a lieu fin 2007 en Europe et
en 2008 en Asie. Le dernier modèle en date est l'iPhone 4, disponible
en juin 2010. On attend l’iPhone 5 pour «bientôt» (en juin 2011).

En février 2009, Google Books lance un portail spécifique pour
téléphone mobile et smartphone, par exemple sur l'iPhone 3G d'Apple ou
sur le G1 de T-Mobile. Le catalogue comprend 1,5 million de livres du
domaine public, auxquels s'ajoutent 500.000 autres titres
téléchargeables hors des États-Unis, du fait d'une législation du
copyright moins restrictive dans certains pays.



2005 > DE GOOGLE PRINT À GOOGLE BOOKS


[Résumé]
Google lance Google Print en mai 2005, en partenariat avec des éditeurs
et des bibliothèques. Trois mois plus tard, Google Print est suspendu
pour une durée indéterminée suite au conflit opposant Google aux
associations américaines d'auteurs et d'éditeurs, qui lui reprochent de
numériser les livres sans l'accord préalable des ayants droit. Le
programme reprend en août 2006 sous le nom de Google Books (Google
Livres). La numérisation des fonds de grandes bibliothèques se
poursuit, tout comme les partenariats avec les éditeurs qui le
souhaitent. Le conflit avec les associations d'auteurs et d'éditeurs se
poursuit lui aussi, puisque Google continue de numériser des livres
sous droits sans l'autorisation préalable des ayants droit, en
invoquant le droit de citation pour présenter des extraits sur le web.
Un accord entre les deux parties est proposé par Google en octobre 2008
pour tenter de mettre fin aux actions légales menées à son encontre.

***

Google décide de mettre son expertise au service du livre et lance
Google Print en mai 2005 avant de le rebaptiser Google Books en août
2006.

# Google Print

Le lancement de Google Print en mai 2005 est précédé de deux étapes.

En octobre 2004, Google met sur pied la première partie de son
programme Google Print, établi en partenariat avec les éditeurs pour
pouvoir consulter à l’écran des extraits de livres, puis commander les
livres auprès d’une librairie en ligne.

En décembre 2004, Google met sur pied la deuxième partie de son
programme Google Print, cette fois à destination des bibliothèques. Il
s’agit d’un projet de bibliothèque consistant à numériser les livres
appartenant à plusieurs grandes bibliothèques partenaires, à commencer
par la bibliothèque de l’Université du Michigan (dans sa totalité, à
savoir 7 millions d’ouvrages), les bibliothèques des Universités de
Harvard, de Stanford et d’Oxford, et celle de la ville de New York. Le
coût estimé au départ se situe entre 150 et 200 millions de dollars US,
avec la numérisation de 10 millions de livres sur six ans et un
chantier d'une durée totale de dix ans.

En août 2005, soit trois mois après son lancement, Google Print est
suspendu pour une durée indéterminée suite à un conflit grandissant
avec les associations américaines d'auteurs et d'éditeurs, celles-ci
reprochant à Google de numériser des livres sans l'accord préalable des
ayants droit.

# Google Books

Le programme reprend en août 2006 sous le nom de Google Books (Google
Livres), qui permet de rechercher les livres par date, titre ou
éditeur. La numérisation des fonds de grandes bibliothèques se
poursuit, tout comme des partenariats avec les éditeurs qui le
souhaitent.

Les livres libres de droit sont consultables à l’écran en texte
intégral. Leur contenu est copiable et l’impression est possible page à
page. Ces livres sont téléchargeables sous la forme de fichiers PDF
imprimables dans leur entier. Les liens publicitaires associés aux
pages de livres sont situés en haut et à droite de l’écran, comme
partout dans Google.

Le conflit avec les associations d'auteurs et d'éditeurs se poursuit,
puisque Google continue de numériser des livres sous droits sans
l’autorisation préalable des ayants droit, en invoquant le droit de
citation pour présenter des extraits sur le web. L’Authors Guild et
l’Association of American Publishers (AAP) invoquent pour leur part le
non respect de la législation relative au copyright pour attaquer
Google en justice.

Fin 2006, d'après le buzz médiatique, Google scannerait 3.000 livres
par jour, ce qui représenterait un million de livres par an. Le coût
estimé serait de 30 dollars par livre. Google Books comprendrait déjà 3
millions de livres. Tous chiffres à prendre avec précaution, la société
ne communiquant pas de statistiques à ce sujet.

À l’exception de la New York Public Library, les premières
bibliothèques numérisées sont toutes des bibliothèques universitaires
(Harvard, Stanford, Michigan, Oxford, Californie, Virginie, Wisconsin-
Madison, Complutense de Madrid), auxquelles s'ajoutent début 2007 les
bibliothèques des Universités de Princeton et du Texas à Austin, la
Biblioteca de Catalunya (Catalogne, Espagne) et la Bayerische
Staatbibliothek (Bavière, Allemagne). En mai 2007, Google annonce la
participation de la première bibliothèque francophone, la Bibliothèque
cantonale et universitaire (BCU) de Lausanne (Suisse), pour la
numérisation de 100.000 titres en français, en allemand et en italien
publiés entre le 17e et le 19e siècle. Suit un partenariat avec la
Bibliothèque municipale de Lyon (France) signé en juillet 2008 pour
numériser 500.000 livres.

# Le conflit avec les associations d’auteurs et d’éditeurs

En octobre 2008, après trois ans de conflit, Google tente de mettre fin
aux poursuites émanant des associations d'auteurs et d'éditeurs en
proposant un accord pouvant être effectif les années suivantes.
L’accord serait basé sur un partage des revenus générés par Google
Books ainsi qu'un large accès aux ouvrages épuisés, tout comme le
paiement de 125 millions de dollars US à l'Authors Guild et à
l'Association of American Publishers (AAP) pour clôturer définitivement
ce conflit.

Suite à cet accord, Google pourrait proposer de plus larges extraits de
livres, jusqu'à 20% d'un même ouvrage, avec un lien commercial pour
acheter une copie - numérique ou non - de l'oeuvre. Les ayants droit
auraient la possibilité de participer ou non à Google Books, et donc de
retirer leurs livres des collections. Par ailleurs, les bibliothèques
universitaires et publiques des États-Unis pourraient accéder à un
portail gratuit géré par Google et donnant accès aux textes de millions
de livres épuisés. Un abonnement permettrait aux universités et aux
écoles de consulter les collections des bibliothèques les plus
renommées. À ce jour (juin 2011), suite au rejet de l’accord proposé
par Google, la société planche sur d’autres propositions.

En novembre 2008, Google Books comprend 7 millions d'ouvrages
numérisés, en partenariat avec 24 bibliothèques et 2.000 éditeurs. Les
24 bibliothèques partenaires se situent principalement aux États-Unis
(16), mais aussi en Allemagne (1), en Belgique (1), en Espagne (2), en
France (1), au Japon (1), au Royaume-Uni (1) et en Suisse (1).



2005 > L'OPEN CONTENT ALLIANCE, BIBLIOTHÈQUE PLANÉTAIRE


[Résumé]
L’Internet Archive s’associe d’abord à Yahoo! en janvier 2005 pour
mettre sur pied l’Open Content Alliance (OCA), un projet mondial visant
à créer un répertoire libre et multilingue de livres numérisés et de
documents multimédia consultable sur tout moteur de recherche. Lancée
officiellement en octobre 2005, l’Open Content Alliance souhaite éviter
les travers de Google Books, à savoir la numérisation des livres sous
droits sans l’accord préalable des éditeurs, tout comme la consultation
et le téléchargement impossibles sur un autre moteur de recherche. Le
projet regroupe de nombreux partenaires: des bibliothèques et des
universités bien sûr, mais aussi des organisations gouvernementales,
des associations à but non lucratif, des organismes culturels et des
sociétés informatiques (Adobe, Hewlett Packard, Microsoft, Yahoo!,
Xerox, etc.). Un million de livres sont consultables en décembre 2008
dans l’Internet Archive et deux millions de livres en mars 2010.

***

En 2005, l’Internet Archive met sur pied l’Open Content Alliance (OCA)
pour créer une bibliothèque planétaire publique de livres numérisés et
de documents multimédia.

Fondée en avril 1996 par Brewster Kahle pour archiver l’internet,
l’Internet Archive pense qu'une bibliothèque numérique à vocation
mondiale ne doit pas être liée à des enjeux commerciaux, contrairement
au projet Google Books.  L’Internet Archive s’associe donc à Yahoo! en
janvier 2005 pour mettre sur pied l’Open Content Alliance (OCA), une
initiative visant à créer un répertoire libre et multilingue de livres
numérisés et de documents multimédia à l’échelon mondial.

L’OCA est officiellement lancée en octobre 2005. Son objectif est de
fédérer un grand nombre de partenaires (bibliothèques, universités,
organismes gouvernementaux, associations, organismes culturels,
sociétés informatiques) pour créer une bibliothèque numérique
respectueuse du copyright et sur un modèle ouvert, consultable sur tout
moteur de recherche, en évitant les travers de Google Books, à savoir
la numérisation des livres sous droits sans l’accord préalable des
éditeurs ou des auteurs, et l’utilisation de ces collections uniquement
à partir du moteur de recherche de Google.

Les premiers organismes participants sont les bibliothèques des
Universités de Californie et de Toronto, les Archives européennes, les
Archives nationales du Royaume-Uni, O'Reilly Media et les Prelinger
Archives. Seuls les livres appartenant au domaine public sont
numérisés, pour éviter les problèmes de copyright auxquels se heurte
Google. L'OCA ne numérise les livres sous droits que si les éditeurs ou
les auteurs de ces livres ont donné leur accord. Les collections
numérisées sont progressivement intégrées à la section Text Archive de
l’Internet Archive, avec 100.000 livres numérisés disponibles en
décembre 2006 et 200.000 livres disponibles en mai 2007.

En décembre 2006, l’Internet Archive reçoit une subvention d'un million
de dollars US de la part de la Sloan Foundation pour numériser les
collections du Metropolitan Museum of Art (la totalité des livres et
plusieurs milliers d’images) ainsi que certaines collections de la
Boston Public Library (les 3.800 livres de la bibliothèque personnelle
de John Adams, deuxième président des États-Unis), du Getty Research
Institute (une collection de livres d'art), de la John Hopkins
University (une collection de documents sur le mouvement anti-
esclavagiste) et de l’Université de Californie à Berkeley (une
collection de documents sur la ruée vers l’or).

En décembre 2006, tout en participant à l’OCA, Microsoft met en ligne
aux États-Unis la version bêta de Live Search Books, sa propre
bibliothèque numérique, qui permet une recherche par mots-clés dans sa
collection de livres du domaine public. Ces livres sont numérisés par
Microsoft suite à des accords passés avec de grandes bibliothèques, les
premières étant la British Library et les bibliothèques des Universités
de Californie et de Toronto, suivies en janvier 2007 par la New York
Public Library et la bibliothèque de la Cornell University. Microsoft
compte aussi ajouter des livres sous droits, mais uniquement avec
l'accord préalable des éditeurs. En mai 2007, la société annonce des
accords avec plusieurs grands éditeurs, dont Cambridge University Press
et McGraw Hill. Microsoft met finalement un terme à ce projet en mai
2008. Les 750.000 livres déjà numérisés sont versés dans les
collections de l'OCA disponibles dans l’Internet Archive.

Les collections de l’OCA comptent un million de livres numérisés en
décembre 2008 et deux millions de livres numérisés en mars 2010.



2006 > LE CATALOGUE COLLECTIF WORLDCAT EN LIGNE


[Résumé]
En août 2006, le catalogue collectif mondial WorldCat lance sa version
web bêta en accès libre. Géré depuis nombre d’années par l’association
OCLC (Online Computer Library Center), WorldCat était jusque-là
disponible sur abonnement. Les 73 millions de notices des 10.000
bibliothèques participantes dans 112 pays permettent de localiser un
milliard de documents. La migration de WorldCat sur le web est
progressive puisque la consultation des notices est d'abord possible
via plusieurs moteurs de recherche (Yahoo!, Google et d'autres). Suit
la version web gratuite worldcat.org, qui propose non seulement les
notices des documents mais aussi un accès direct (gratuit ou payant)
aux documents électroniques des bibliothèques membres: livres du
domaine public, articles, photos, vidéos, musique et livres audio.
WorldCat permet de localiser 1,5 milliard de documents en avril 2010.
L’autre grand catalogue mondial, géré par le RLG (Research Libraries
Group), fusionne avec OCLC en décembre 2006.

***

En août 2006, l'association OCLC (Online Computer Library Center) lance
la version web de son catalogue collectif mondial WorldCat en accès
libre.

Qu’est-ce exactement qu’un catalogue collectif? Le but premier d’un
catalogue collectif est d’éviter de cataloguer à nouveau un document
déjà traité par une bibliothèque partenaire. Si le catalogueur trouve
la notice du livre qu’il est censé cataloguer, il la copie pour
l’inclure dans le catalogue de sa propre bibliothèque. S’il ne trouve
pas la notice, il la crée, et cette notice est aussitôt disponible pour
les catalogueurs officiant dans d'autres bibliothèques. De nombreux
catalogues collectifs sont gérés à l’échelon local, régional, national
ou mondial.

Les deux grands catalogues collectifs mondiaux sont lancés dès les
années 1980, respectivement par OCLC (Online Computer Library Center)
et par RLG (Research Libraries Group). Vingt ans plus tard, ces deux
associations gèrent de gigantesques bases bibliographiques alimentées
par leurs adhérents, permettant ainsi aux bibliothèques d’unir leurs
forces par-delà les frontières.

# Le catalogue d’OCLC

Fondée en 1967 dans l’Ohio (États-Unis), l’association OCLC gère
d'abord l’OCLC Online Union Catalog à partir de 1971 pour desservir les
bibliothèques universitaires de l’Ohio avant de s’étendre
progressivement à tout le pays puis au monde entier.

Rebaptisé WorldCat et disponible sur abonnement payant, ce catalogue
comprend 38 millions de notices en 370 langues en 1998, avec
translittération des notices dans les langues JACKPHY, à savoir le
japonais, l'arabe, le chinois, le coréen (Korean en anglais), le
persan, l'hébreu et le yiddish. Deux millions de notices sont ajoutées
au catalogue chaque année. WorldCat utilise huit formats
bibliographiques correspondant aux catégories suivantes: livres,
périodiques, documents visuels, cartes et plans, documents mixtes
(plusieurs supports à la fois), enregistrements sonores, partitions et
enfin documents informatiques.

En 2005, 61 millions de notices bibliographiques produites par 9.000
bibliothèques participantes sont disponibles dans 400 langues. En 2006,
73 millions de notices provenant de 10.000 bibliothèques dans 112 pays
permettent de localiser un milliard de documents. Une notice type
contient la description du document ainsi que des informations sur son
contenu (table des matières, résumé, couverture, illustrations, courte
biographie de l’auteur).

Devenue la plus grande base mondiale de données bibliographiques,
WorldCat migre progressivement sur le web, d’abord en rendant la
consultation des notices possible par le biais de plusieurs moteurs de
recherche (Yahoo!, Google et d’autres), puis en lançant en août 2006
une version web (bêta) de WorldCat en accès libre, qui propose non
seulement les notices des documents mais aussi l'accès direct (gratuit
ou payant) aux documents électroniques des bibliothèques membres:
livres du domaine public, articles, photos, livres audio, musique et
vidéos.

# Le catalogue du RLG

Le deuxième grand catalogue collectif mondial est géré par
l’association RLG (Research Library Group, qui devient plus tard le
Research Libraries Group). Fondé en 1980 en Californie, avec une
antenne à New York, le RLG se donne pour but d’améliorer l’accès à
l’information dans le domaine de l’enseignement et de la recherche et
lance un catalogue collectif dénommé RLIN (Research Libraries
Information Network).

Contrairement à WorldCat qui n'accepte qu'une notice par document, RLIN
accepte plusieurs notices pour le même document. En 1998, RLIN comprend
82 millions de notices dans 365 langues, avec des notices
translittérées pour les documents publiés dans les langues JACKPHY et
en cyrillique. Des centaines de dépôts d’archives, bibliothèques de
musées, bibliothèques universitaires, bibliothèques publiques,
bibliothèques de droit, bibliothèques techniques, bibliothèques
d’entreprise et bibliothèques d’art utilisent RLIN pour le catalogage,
le prêt interbibliothèques et le descriptif de leurs archives et
manuscrits.

Une des spécialités de RLIN est l’histoire de l’art. Alimentée par 65
bibliothèques spécialisées, une section spécifique comprend 100.000
notices de catalogues d’expositions et 168.500 notices de documents
iconographiques (photographies, diapositives, dessins, estampes et
affiches). Cette section inclut aussi les 110.000 notices de la base
bibliographique Scipio, dédiée au catalogues de ventes d'objets d'art.

En 2003, RLIN devient le RLG Union Catalog, qui comprend désormais 126
millions de notices bibliographiques correspondant à 42 millions de
documents (livres, cartes, manuscrits, films, bandes sonores, etc.).

Au printemps 2004, une version web du catalogue est disponible en accès
libre sous le nom de RedLightGreen, suite à une phase pilote lancée à
l’automne 2003. C’est la première fois qu’un catalogue collectif
mondial est en accès libre, trois ans avant WorldCat. Destiné en
priorité aux étudiants du premier cycle universitaire, RedLightGreen
propose 130 millions de notices, avec des informations spécifiques aux
bibliothèques d’un campus donné (cote du document, lien vers sa version
en ligne si celle-ci existe, etc.).

Après trois ans d’activité, le site RedLightGreen ferme en novembre
2006, et le RLG fusionne avec OCLC. Les usagers sont invités à utiliser
WorldCat, qui dispose d’une version web en accès libre depuis août
2006. En avril 2010, WorldCat permet de localiser 1,5 milliard de
documents et d'avoir directement accès à nombre d'entre eux.



2007 > QUEL AVENIR POUR L'EBOOK?


[Résumé]
Suite à une enquête lancée fin 2006 sur l’avenir de l’ebook, voici les
réponses de deux professionnels du livre: Pierre Schweitzer, concepteur
du projet @folio, un lecteur portable de textes, et Denis Zwirn,
fondateur de la bibliothèque numérique Numilog. Selon Pierre, «la
chance qu’on a tous est de vivre là, ici et maintenant cette
transformation fantastique. Quand je suis né en 1963, les ordinateurs
avaient comme mémoire quelques pages de caractères à peine.
Aujourd’hui, mon baladeur de musique pourrait contenir des milliards de
pages, une vraie bibliothèque de quartier. Demain, par l’effet conjugué
de la loi de Moore et de l’omniprésence des réseaux, l’accès instantané
aux oeuvres et aux savoirs sera de mise. Le support de stockage lui-
même n’aura plus beaucoup d’intérêt. Seules importeront les commodités
fonctionnelles d’usage et la poétique de ces objets.»

***

Suite à une enquête lancée fin 2006 sur l’avenir de l’ebook, voici les
réponses de deux professionnels du livre: Pierre Schweitzer, concepteur
du projet @folio, un lecteur portable de textes, et Denis Zwirn,
fondateur de la bibliothèque numérique Numilog.

# «La poétique des objets»

Concepteur du projet @folio, un lecteur portable de textes, Pierre
Schweitzer explique en décembre 2006: «La lecture numérique dépasse de
loin, de très loin même, la seule question du "livre" ou de la presse.
Le livre et le journal restent et resteront encore, pour longtemps, des
supports de lecture techniquement indépassables pour les contenus de
valeur ou pour ceux dépassant un seuil critique de diffusion. Bien que
leur modèle économique puisse encore évoluer (comme pour les "gratuits"
la presse grand public), je ne vois pas de bouleversement radical à
l’échelle d’une seule génération. Au-delà de cette génération, l’avenir
nous le dira. On verra bien. Pour autant, d’autres types de contenus se
développent sur les réseaux. Internet défie l’imprimé sur ce terrain-
là: celui de la diffusion en réseau (dématérialisée = coût marginal
nul) des oeuvres et des savoirs. Là où l’imprimé ne parvient pas à
équilibrer ses coûts. Là où de nouveaux acteurs peuvent venir prendre
leur place.

Or, dans ce domaine nouveau, les équilibres économiques et les logiques
d’adoption sont radicalement différents de ceux que l’on connaît dans
l’empire du papier - voir par exemple l’évolution des systèmes de
validation pour les archives ouvertes dans la publication scientifique
ou les modèles économiques émergents de la presse en ligne. Il est donc
vain, dangereux même, de vouloir transformer au forceps l’écologie du
papier - on la ruinerait à vouloir le faire! À la marge, certains
contenus très spécifiques, certaines niches éditoriales, pourraient
être transformées - l’encyclopédie ou la publication scientifique le
sont déjà. De la même façon, les guides pratiques, les livres
d’actualité quasi-jetables et quelques autres segments qui envahissent
les tables des librairies pourraient l’être, pour le plus grand bonheur
des libraires. Mais il n’y a là rien de massif ou brutal selon moi. Nos
habitudes de lecture ne seront pas bouleversées du jour au lendemain,
elles font partie de nos habitudes culturelles, elles évoluent
lentement, au fur et à mesure de leur adoption (= acceptation) par les
générations nouvelles.»

Selon Pierre, «la chance qu’on a tous est de vivre là, ici et
maintenant cette transformation fantastique. Quand je suis né en 1963,
les ordinateurs avaient comme mémoire quelques pages de caractères à
peine. Aujourd’hui, mon baladeur de musique pourrait contenir des
milliards de pages, une vraie bibliothèque de quartier. Demain, par
l’effet conjugué de la loi de Moore et de l’omniprésence des réseaux,
l’accès instantané aux oeuvres et aux savoirs sera de mise. Le support
de stockage lui-même n’aura plus beaucoup d’intérêt. Seules importeront
les commodités fonctionnelles d’usage et la poétique de ces objets.»

# «Un produit commercial»

Selon Denis Zwirn, fondateur de la librairie numérique Numilog,
interviewé en août 2007, on peu noter «un premier point d'inflexion
dans la courbe de croissance du marché des livres numériques. Plusieurs
facteurs sont réunis pour cela:

(1) le développement de vastes catalogues en ligne utilisant pleinement
les fonctionnalités de la recherche plein texte dans les livres
numérisés, comme les catalogues de la future Bibliothèque numérique
européenne, de VollTextSuche Online, de Google et d'Amazon. Une fois le
contenu trouvé dans un des ouvrages ainsi "sondé" par ce type de
recherche révolutionnaire pour le grand public, il est naturel de
vouloir accéder à la totalité de l'ouvrage... dans sa version
numérique.

(2) Des progrès techniques cruciaux tels que la proposition commerciale
d'appareils de lecture à base d'encre électronique améliorant
radicalement l'expérience de lecture finale pour l'usager en la
rapprochant de celle du papier. Par exemple l'iLiad d'Irex ou le Sony
Reader, mais bien d'autres appareils s'annoncent. Le progrès concerne
toutefois tout autant le développement des nouveaux smartphones
multifonctions comme les BlackBerry ou l'iPhone, ou la proposition de
logiciels de lecture à l'interface fortement améliorée et pensée pour
les ebooks sur PC, comme Adobe Digital Editions.

(3) Enfin, le changement important d'attitude de la part des
professionnels du secteur, éditeurs, et probablement bientôt aussi
libraires. Les éditeurs anglo-saxons universitaires ont massivement
tracé une route que tous les autres sont en train de suivre, en tout
cas aux États-Unis, en Europe du Nord et en France: proposer une
version numérique de tous les ouvrages. Même pour les plus réticents
encore il y a quelques années, ce n'est plus une question de
"pourquoi?", c'est simplement devenu une question de "comment?". Les
libraires ne vont pas tarder à considérer que vendre un livre numérique
fait partie de leur métier normal.»

Selon Denis, «le livre numérique n'est plus une question de colloque,
de définition conceptuelle ou de divination par certains "experts":
c'est un produit commercial et un outil au service de la lecture. Il
n'est pas besoin d'attendre je ne sais quel nouveau mode de lecture
hypermoderne et hypertextuel enrichi de multimédia orchestrant
savamment sa spécificité par rapport au papier, il suffit de proposer
des textes lisibles facilement sur les supports de lecture électronique
variés qu'utilisent les gens, l'encre électronique pouvant
progressivement envahir tous ces supports. Et de les proposer de
manière industrielle. Ce n'est pas et ne sera jamais un produit de
niche (les dictionnaires, les guides de voyage, les livres pour les non
voyants...): c'est en train de devenir un produit de masse, riche de
formes multiples comme l'est le livre traditionnel.»



2007 > CITIZENDIUM, ENCYCLOPÉDIE EXPÉRIMENTALE


[Résumé]
Citizendium - acronyme de «The Citizens' Compendium» - est une
encyclopédie collaborative expérimentale lancée en mars 2007 (en
version bêta) par Larry Sanger, auparavant co-fondateur de Wikipédia
avec Jimmy Wales. Citizendium est une encyclopédie coopérative
gratuite, tout comme Wikipédia, mais, d’après Larry, sans ses travers,
à savoir le vandalisme, le manque de rigueur et l'utilisation d'un
pseudonyme pour y contribuer. Les auteurs signent leurs articles de
leur vrai nom et ces articles sont relus et corrigés par des «editors»,
âgés d'au moins 25 ans et titulaires d'une licence universitaire. De
plus, des «constables» sont chargés de la bonne marche du projet et du
respect du règlement. Le jour de son lancement le 25 mars 2007,
Citizendium comptabilise 1.100 articles, 820 auteurs et 180 experts.
L’encyclopédie comprend 11.800 articles en juillet 2009 et 15.000
articles en septembre 2010.

***

Citizendium est une encyclopédie collaborative expérimentale lancée en
mars 2007 par Larry Sanger, co-fondateur de Wikipédia.

Acronyme de «The Citizens’ Compendium», Citizendium est une
encyclopédie coopérative et gratuite, tout comme Wikipédia, mais,
d’après Larry, sans ses travers, à savoir le vandalisme, le manque de
rigueur et l'utilisation d'un pseudonyme pour y participer. Les auteurs
signent leurs articles de leur vrai nom et ces articles sont relus et
corrigés par des «editors» âgés d'au moins 25 ans et titulaires d'une
licence universitaire. De plus, des «constables» sont chargés de la
bonne marche du projet et du respect du règlement. Le jour de son
lancement le 25 mars 2007, suite à une gestation débutée en novembre
2006, Citizendium comptabilise 1.100 articles, 820 auteurs et 180
experts. L’encyclopédie comprend 9.800 articles en janvier 2009 et
15.000 articles en septembre 2010.

Dans «Why make room for experts in web 2.0?» (Pourquoi accorder une
place aux experts dans le web 2.0?), un article en ligne daté d'octobre
2006 et actualisé en mars 2007, Larry Sanger voit dans Citizendium
l'émergence d'un nouveau modèle de collaboration massive de dizaines de
milliers d'intellectuels et scientifiques, non seulement pour les
encyclopédies, mais aussi pour les manuels d'enseignement, les ouvrages
de référence, le multimédia et les applications en 3D. Cette
collaboration serait basée sur le partage des connaissances, dans la
lignée du web 2.0, un concept lancé en 2004 pour caractériser les
notions de communauté et de partage et qui se manifeste d'abord par une
floraison de wikis, de blogs et de sites sociaux. D'après Larry, on
pourrait également créer des structures de type web 2.0 pour des
collaborations scientifiques, et Citizendium pourrait servir de
prototype dans ce domaine.



2007 > L'ENCYCLOPEDIA OF LIFE, PROJET GLOBAL


[Résumé]
L’Encyclopedia of Life (EOL) débute en mai 2007 en tant que projet
global visant à regrouper les connaissances existantes sur les espèces
animales et végétales. Les espèces connues seraient au nombre de 1,8
million, y compris les espèces en voie d’extinction, avec l’ajout de
nouvelles espèces au fur et à mesure de leur identification, ce qui
représenterait entre 8 et 10 millions d'espèces en tout. Outil
d’apprentissage et d’enseignement pour une meilleure connaissance de
notre planète, cette encyclopédie collaborative multimédia sera à
destination de tous: scientifiques, enseignants, étudiants, scolaires,
médias, décideurs et grand public, qui pourront y contribuer
directement, le contenu étant ensuite validé ou non par des experts.
L’encyclopédie devrait être pleinement opérationnelle en 2012 et
complète - c'est-à-dire à jour - en 2017. Des versions en plusieurs
langues seront gérées par des organismes partenaires.

***

L’Encyclopedia of Life (EOL) débute en mai 2007 en tant que projet
global visant à regrouper les connaissances existantes sur les espèces
animales et végétales.

Les espèces connues seraient au nombre de 1,8 million, y compris les
espèces en voie d’extinction, avec l’ajout de nouvelles espèces au fur
et à mesure de leur identification, ce qui représenterait entre 8 et 10
millions d'espèces en tout.

Ce projet collaboratif est mené par plusieurs grandes institutions:
Field Museum of Natural History, Harvard University, Marine Biological
Laboratory, Missouri Botanical Garden, Smithsonian Institution et
Biodiversity Heritage Library.

Le financement initial de l'Encyclopedia of Life est assuré par la
MacArthur Foundation avec 10 millions de dollars US et la Sloan
Foundation avec 2,5 millions de dollars. Un financement total de 100
millions de dollars serait nécessaire sur dix ans, avant que
l'encyclopédie ne puisse s'autofinancer.

Le directeur honoraire du projet est Edward Wilson, professeur émérite
à l’Université de Harvard, qui, dans un essai daté de 2002, fut le
premier à émettre le voeu d’une telle encyclopédie. Cinq ans plus tard,
en 2007, c'est désormais chose possible grâce aux avancées
technologiques récentes: outils logiciels permettant l’agrégation de
contenu, mash-up (à savoir le fait de rassembler un contenu donné à
partir de nombreuses sources différentes), wikis de grande taille et
gestion de contenus à vaste échelle.

Cette encyclopédie collaborative multimédia permettra de rassembler
textes, photos, cartes, bandes sonores et vidéos, avec une page web par
espèce, en offrant un portail commun à des millions de documents épars,
en ligne et hors ligne. Outil d’apprentissage et d’enseignement pour
une meilleure connaissance de notre planète, l'encyclopédie sera à
destination de tous: scientifiques, enseignants, étudiants, scolaires,
médias, décideurs et grand public, qui pourront y contribuer
directement, le contenu étant ensuite validé ou non par des experts.

En qualité de consortium des dix plus grandes bibliothèques des
sciences de la vie, qui s’ouvrira ensuite à d’autres bibliothèques, la
Biodiversity Heritage Library entreprend la numérisation de 2 millions
de documents dont les dates de publication s’étalent sur deux cents
ans, pour intégration progressive dans l'Encyclopedia of Life. En mai
2007, à la date du lancement officiel du projet, on compte déjà 1,25
million de pages traitées dans les centres de numérisation de Londres,
Boston et Washington, D.C. Les documents numérisés sont disponibles au
fur et à mesure dans l’Internet Archive.

Si la réalisation des pages web débute courant 2007, l’encyclopédie
fait ses réels débuts sur la toile à la mi-2008. Elle devrait être
pleinement opérationnelle en 2012 et complète - c'est-à-dire à jour -
en 2017.

L'encyclopédie sera un «macroscope» permettant de déceler les grandes
tendances à partir d’un stock d’informations considérable, à la
différence du microscope permettant l’étude de détail. La version
initiale sera d’abord en anglais avant d’être traduite en plusieurs
langues par de futurs organismes partenaires.



2009 > INDISCRIPTS, LABORATOIRE DE SCRIPTS INDESIGN


[Résumé]
Marc Autret, infographiste, a derrière lui dix ans de journalisme
multi-tâches et de formation en ligne dans les domaines de l’édition,
du multimédia et du droit d’auteur. Il explique en décembre 2006: «Je
suis un "artisan" de l’information et je travaille essentiellement avec
des éditeurs. Ils sont tellement en retard, tellement étrangers à la
révolution numérique, que j’ai du pain sur la planche pour pas mal
d’années. Aujourd’hui je me concentre sur le conseil, l’infographie, la
typographie, le pré-presse et le webdesign, mais je sens que la part du
logiciel va grandir. Des secteurs comme l’animation 3D,
l’automatisation des tâches de production, l’intégration multi-
supports, la base de données et toutes les technologies issues de XML
[eXtensible Markup Language] vont s’ouvrir naturellement. Les éditeurs
ont besoin de ces outils, soit pour mieux produire, soit pour mieux
communiquer. C’est là que je vois l’évolution, ou plutôt
l’intensification, de mon travail.» En mai 2009, Marc crée le site
Indiscripts, laboratoire de scripts InDesign.

***

Marc Autret, infographiste, a derrière lui dix ans de journalisme
multi-tâches et de formation en ligne dans les domaines de l’édition,
du multimédia et du droit d’auteur. Il explique en décembre 2006:
«C’est un "socle" irremplaçable pour mes activités d’aujourd’hui, qui
en sont le prolongement technique. Je suis un "artisan" de
l’information et je travaille essentiellement avec des éditeurs. Ils
sont tellement en retard, tellement étrangers à la révolution
numérique, que j’ai du pain sur la planche pour pas mal d’années.
Aujourd’hui je me concentre sur le conseil, l’infographie, la
typographie, le pré-presse et le webdesign, mais je sens que la part du
logiciel va grandir. Des secteurs comme l’animation 3D,
l’automatisation des tâches de production, l’intégration multi-
supports, la base de données et toutes les technologies issues de XML
[eXtensible Markup Language] vont s’ouvrir naturellement. Les éditeurs
ont besoin de ces outils, soit pour mieux produire, soit pour mieux
communiquer. C’est là que je vois l’évolution, ou plutôt
l’intensification, de mon travail.»

Comment Marc voit-il l'avenir de l'ebook? «Sans vouloir faire dans la
divination, je suis convaincu que l’e-book (ou "ebook": impossible de
trancher!) a un grand avenir dans tous les secteurs de la non-fiction.
Je parle ici de livre numérique en termes de "logiciel", pas en terme
de support physique dédié (les conjectures étant plus incertaines sur
ce dernier point). Les éditeurs de guides, d’encyclopédies et
d’ouvrages informatifs en général considèrent encore l’e-book comme une
déclinaison très secondaire du livre imprimé, sans doute parce que le
modèle commercial et la sécurité de cette exploitation ne leur semblent
pas tout à fait stabilisés aujourd’hui. Mais c’est une question de
temps. Les e-books non commerciaux émergent déjà un peu partout et
opèrent d’une certaine façon un défrichage des possibles. Il y a au
moins deux axes qui émergent: (a) une interface de lecture/consultation
de plus en plus attractive et fonctionnelle (navigation, recherche,
restructuration à la volée, annotations de l’utilisateur, quizz
interactif, etc.); (b) une intégration multimédia (vidéo, son,
infographie animée, base de données, etc.) désormais fortement couplée
au web. Aucun livre physique n’offre de telles fonctionnalités.
J’imagine donc l’e-book de demain comme une sorte de wiki cristallisé,
empaqueté dans un format. Quelle sera alors sa valeur propre? Celle
d’un livre: l’unité et la qualité du travail éditorial!»

Marc lance en mai 2009 son site Indiscripts, qui est «un laboratoire de
scripts InDesign. On y explore l'automatisation de mise en page, les
techniques de scripting et le développement de plugins dans le contexte
d'Adobe InDesign. Plus largement, notre ambition est d'illustrer les
possibilités offertes par le langage JavaScript au sein des
applications Adobe et d'informer utilement les créateurs de scripts.»

Marc réalise de beaux livres interactifs au format PDF. Quel est son
sentiment sur la «concurrence» entre les formats PDF et EPUB?  Il
répond en juin 2011: «Je déplore que l'émergence de l'EPUB ait provoqué
l'anéantissement pur et simple du PDF comme format de livre numérique.
Le fait que les éléments d'interactivité disponibles au sein du PDF ne
soient pas supportés par les plateformes nomades actuelles a aboli
toute possibilité d'expérimentation dans cette voie, qui m'apparaissait
extrêmement prometteuse. Alors que l'édition imprimée fait la place à
des objets de nature très différentes, entre le livre d'art de très
haute facture et le livre "tout terrain", le marché de l'ebook s'est
développé d'emblée sur un mode totalitaire et ségrégationniste,
comparable en cela à une guerre de systèmes d'exploitation plutôt qu'à
une émulation technique et culturelle. De fait, il existe fort peu de
livres numériques PDF tirant parti des possibilités de ce format.

Dans l'inconscient collectif, le PDF reste une sorte de duplicata
statique de l'ouvrage imprimé et personne ne veut lui voir d'autre
destin. L'EPUB, qui n'est rien d'autre qu'une combinaison XHTML/CSS
(certes avec des perspectives JavaScript), consiste à mettre le livre
numérique "au pas" du Web. C'est une technologie très favorable aux
contenus structurés, mais très défavorable à l'artisanat typographique.
Elle introduit une vision étroite de l'oeuvre numérique, réduite à un
flux d'information. On ne le mesure pas encore, mais la pire
catastrophe culturelle de ces dernières décennies est l'avènement du
XML, ce langage qui précalibre et contamine notre façon de penser les
hiérarchies. Le XML et ses avatars achèvent de nous enfermer dans les
invariants culturels occidentaux.»



2010 > DU LIBRIÉ A L'IPAD


[Résumé]
L'iPad est lancé par Apple en avril 2010 aux États-Unis  en tant que
tablette numérique multifonctions, six ans après le Librié lancé par
Sony en avril 2004 au Japon. Comme on s’en souvient, les premières
tablettes électroniques dédiées à la lecture sont le Rocket eBook
(1998), le SoftBook Reader (1998) et le Gemstar eBook (novembre 2000),
qui ne durent pas. Après une période morose qui voit la montée de la
lecture sur PDA puis sur smartphone, des tablettes plus légères gagnent
en puissance et en qualité d'écran grâce à la technologie E Ink. Ces
nouvelles tablettes sont par exemple le Librié de Sony (avril 2004), le
Cybook 2e génération (juin 2004), le Sony Reader (septembre 2006), le
Kindle d'Amazon (novembre 2007), le Nook de Barnes & Noble (novembre
2009) et l'iPad d'Apple (avril 2010). La compétition est rude sur un
marché prometteur, en attendant les possibilités de lecture multimédia
/ hypermédia et de lecture en 3D sur des supports flexibles.

***

L'iPad est lancé par Apple en avril 2010 aux États-Unis en tant que
tablette numérique multifonctions, six ans après le Librié lancé par
Sony en avril 2004 au Japon.

Comme on s’en souvient, les premières  tablettes électroniques dédiées
à la lecture sont le Rocket eBook (1998), le SoftBook Reader (1998) et
le Gemstar eBook (novembre 2000), qui ne durent pas. Après une période
morose qui voit la montée de la lecture sur PDA puis sur smartphone,
des tablettes plus légères gagnent en puissance et en qualité d'écran,
avec l’introduction de la technologie E Ink. Ces nouvelles tablettes
sont par exemple le Librié de Sony (avril 2004), le Cybook 2e
génération (juin 2004), le Sony Reader (septembre 2006), le Kindle
d'Amazon (novembre 2007), le Nook de Barnes & Noble (novembre 2009) et
l'iPad d'Apple (avril 2010).

# Le Librié (Sony)

En avril 2004, Sony lance au Japon sa première tablette de lecture, le
Librié 1000-EP, produit en partenariat avec les sociétés Philips et E
Ink. Le Librié est la première tablette du marché à utiliser la
technologie d’affichage développée par la société E Ink. L’appareil
pèse 300 grammes (avec piles et protection d’écran) pour une taille de
12,6 x 19 x 1,3 centimètres et fonctionne avec quatre piles alcalines.
Sa mémoire est de 10 Mo - avec possibilité d’extension - et sa capacité
de stockage de 500 livres. Son écran de 6 pouces a une définition de
170 DPI et une résolution de 800 x 600 pixels. Un port USB permet le
téléchargement des livres à partir d’un ordinateur. L’appareil comprend
aussi un clavier, une fonction d'enregistrement et une synthèse vocale.

# Le Cybook (Bookeen)

Suite au lancement du Cybook dès janvier 2001 par la société Cytale en
tant que première tablette de lecture européenne, avec cessation des
activités de Cytale en juillet 2002, la commercialisation du Cybook est
reprise en 2003 par la société Bookeen, créée à l'initiative de Michael
Dahan et Laurent Picard, deux ingénieurs de Cytale. Le Cybook 2e
génération est lancé en juin 2004 et se décline en plusieurs modèles.
En juillet 2007, Bookeen dévoile une nouvelle version de sa tablette,
baptisée Cybook Gen3 (3e génération), avec un écran utilisant pour la
première fois la technologie E Ink.

# Le Sony Reader

Après le Librié lancé en avril 2004 au Japon, Sony lance le Sony Reader
en octobre 2006 aux États-Unis. L'écran de cette tablette, qui utilise
une technologie E Ink plus avancée, est  «un écran qui donne une
excellente expérience de lecture, très proche de celle du vrai papier,
et qui ne fatigue pas les yeux» (Mike Cook, auteur du site
epubBooks.com). Un autre avantage de cette tablette sur ses
concurrentes est la durée de vie de la batterie, avec plus de 7.000
pages consultables, ou deux semaines sans nécessité de la recharger.
Cette tablette est aussi la première à utiliser Adobe Digital Editions,
un logiciel qui adapte le texte du livre à la taille de l’écran. Le
Sony Reader est progressivement disponible au Canada, au Royaume-Uni,
en Allemagne et en France.

# Le Kindle (Amazon)

Amazon lance en novembre 2007 sa propre tablette de lecture, le Kindle,
avec un format livresque (19 x 13 x 1,8 cm), un poids de 289 grammes,
un écran noir et blanc de 6 pouces avec une résolution de 800 x 600
pixels, un clavier, une mémoire de 256 Mo (extensible par carte SD), un
port USB et la possibilité de se connecter à l'internet via la WiFi. Le
Kindle peut contenir jusqu'à 200 livres sur les 80.000 livres
numériques que propose le catalogue d’Amazon. Amazon lance en février
2009 le Kindle 2 pour un prix plus modique, qui continue de baisser
sensiblement dans les mois qui suivent, puis le Kindle DX en mai 2009
avec un écran de 9,7 pouces permettant la lecture de journaux et
magazines. Le catalogue d'Amazon comptabiliserait 450.000 titres
numériques en mars 2010, y compris des livres et revues audionumériques
suite au rachat du catalogue d'Audible.com en janvier 2009.

# Le Nook (Barnes & Noble)

En novembre 2009, la grande chaîne de librairies américaine Barnes &
Noble lance sa propre tablette de lecture, le Nook. La tablette dispose
d’une plateforme Android et d'un écran E Ink de 6 pouces, avec une
connexion WiFi et 3G. En juin 2010, le prix du premier modèle baisse.
Un nouveau modèle plus économique disposant de la seule connexion WiFi
est également lancé à la même date. Par ailleurs, le Nook Color
apparaît en octobre 2010 avec un écran LCD de 7 pouces pour la lecture
de magazines et livres d'images. Un nouveau Nook plus léger est lancé
en mai 2011 sous plateforme Android, avec un écran de 6 pouces
utilisant la technologie E Ink Pearl tactile. Le  catalogue de Barnes &
Noble proposerait 2 millions de livres numériques à la fin 2010.

# L’iPad (Apple)

L'iPad est lancé par Apple le 3 avril 2010 aux États-Unis en tant que
tablette numérique multifonctions, avec un iBookstore de 60.000 livres
numériques qui s'étoffe rapidement. Un lancement mondial suit en juin
2010. Après l'iPod (lancé en octobre 2001) puis l'iPhone (lancé en juin
2007), deux objets cultes auprès de toute une génération, Apple devient
lui aussi un acteur de poids pour le livre numérique. Apple lance
l’iPad 2 en mars 2011 aux États-Unis, avec un lancement deux semaines
plus tard dans d’autres pays.

Cette courte liste de tablettes est loin d’exhaustive, bien entendu. La
compétition est rude sur un marché prometteur, en attendant les
possibilités de lecture multimédia / hypermédia et de lecture en 3D sur
des supports flexibles.



2011 > L’EBOOK EN DIX POINTS


[Résumé]
Dans cette conclusion sous forme de citations, les dates indiquées sont
les dates auxquelles ces textes - extraits d’entretiens par courriel -
ont été écrits et publiés. Les auteurs de ces textes sont Michael Hart
(août 1998), John Mark Ockerbloom (septembre 1998), Robert Beard
(octobre 1998), Jean-Paul (juin 2000), Nicolas Pewny (février 2003),
Marc Autret (décembre 2006), Pierre Schweitzer (janvier 2007), Denis
Zwirn (août 2007), Catherine Domain (avril 2010) et Henk Slettenhaar
(juin 2011).

***

Voici une conclusion sous forme de citations. Les dates indiquées sont
les dates auxquelles ces textes - extraits d’entretiens par courriel -
ont été écrits et publiés.

# Août 1998

«Nous considérons le texte électronique comme un nouveau médium, sans
véritable relation avec le papier. Le seul point commun est que nous
diffusons les mêmes oeuvres, mais je ne vois pas comment le papier peut
concurrencer le texte électronique une fois que les gens y sont
habitués, particulièrement dans les établissements d'enseignement.»
(Michael Hart, fondateur du Projet Gutenberg en 1971)

# Septembre 1998

«Je me suis passionné pour l'énorme potentiel qu'a l'internet de rendre
la littérature accessible au plus grand nombre. (…) Je suis très
intéressé par le développement de l'internet en tant que médium de
communication de masse ces prochaines années. J'aimerais aussi rester
impliqué dans la mise à disposition gratuite de livres sur l'internet,
que ceci fasse partie intégrante de mon activité professionnelle, ou
que ceci soit une activité bénévole menée sur mon temps libre.» (John
Mark Ockerbloom, créateur de l’Online Books Page en 1993)

# Octobre 1998

«Le web sera une encyclopédie du monde faite par le monde pour le
monde. Il n'y aura plus d'informations ni de connaissances utiles qui
ne soient pas disponibles, si bien que l'obstacle principal à la
compréhension internationale et interpersonnelle et au développement
personnel et institutionnel sera levé. Il faudrait une imagination plus
débordante que la mienne pour prédire l'effet de ce développement sur
l'humanité.» (Robert Beard, cofondateur du portail yourDictionary.com
en 2000)

# Juin 2000

«La navigation par hyperliens se fait en rayon (j'ai un centre
d'intérêt et je clique méthodiquement sur tous les liens qui s'y
rapportent) ou en louvoiements (de clic en clic, à mesure qu'ils
apparaissent, au risque de perdre de vue mon sujet). Bien sûr, les deux
sont possibles avec l'imprimé. Mais la différence saute aux yeux:
feuilleter n'est pas cliquer. L'internet a donc changé mon rapport à
l'écriture. (...) C'est finalement dans la publication en ligne
(l'entoilage?) que j'ai trouvé la mobilité, la fluidité que je
cherchais.» (Jean-Paul, créateur du site hypermédia cotres.net en 1998)

# Février 2003

«Je vois le livre numérique du futur comme un "ouvrage total"
réunissant textes, sons, images, vidéo, interactivité: une nouvelle
manière de concevoir et d'écrire et de lire, peut-être sur un livre
unique, sans cesse renouvelable, qui contiendrait tout ce que l'on a
lu, unique et multiple compagnon. Utopique? Invraisemblable? Peut-être
pas tant que cela!» (Nicolas Pewny, fondateur des éditions du Choucas
en 1992)

# Décembre 2006

«Il y a au moins deux axes qui émergent [pour le livre numérique]: (a)
une interface de lecture/consultation de plus en plus attractive et
fonctionnelle (navigation, recherche, restructuration à la volée,
annotations de l’utilisateur, quizz interactif...); (b) une intégration
multimédia (vidéo, son, infographie animée, base de données, etc.)
désormais fortement couplée au web. Aucun livre physique n’offre de
telles fonctionnalités. J’imagine donc l'e-book de demain comme une
sorte de wiki cristallisé, empaqueté dans un format. Quelle sera alors
sa valeur propre ? Celle d'un livre: l'unité et la qualité du travail
éditorial!» (Marc Autret, infographiste et créateur du site Indiscripts
en 2009)

# Janvier 2007

«La chance qu'on a tous est de vivre là, ici et maintenant cette
transformation fantastique. Quand je suis né en 1963, les ordinateurs
avaient comme mémoire quelques pages de caractères à peine.
Aujourd'hui, mon baladeur de musique pourrait contenir des milliards de
pages, une vraie bibliothèque de quartier. Demain, par l'effet conjugué
de la loi de Moore et de l'omniprésence des réseaux, l'accès instantané
aux oeuvres et aux savoirs sera de mise. Le support de stockage lui-
même n'aura plus beaucoup d'intérêt. Seules importeront les commodités
fonctionnelles d'usage et la poétique de ces objets.» (Pierre
Schweitzer, concepteur du projet @folio en 1996)

# Août 2007

«Le livre numérique n'est plus une question de colloque, de définition
conceptuelle ou de divination par certains "experts": c'est un produit
commercial et un outil au service de la lecture. (…) Il suffit de
proposer des textes lisibles facilement sur les supports de lecture
électronique variés qu'utilisent les gens, l'encre électronique pouvant
progressivement envahir tous ces supports. Et de les proposer de
manière industrielle. Ce n'est pas et ne sera jamais un produit de
niche (les dictionnaires, les guides de voyage, les livres pour non
voyants...): c'est en train de devenir un produit de masse, riche de
formes multiples comme l'est le livre traditionnel.» (Denis Zwirn,
fondateur de la librairie numérique Numilog en 2000)

# Avril 2010

«Internet a pris de plus en plus de place dans ma vie! Il me permet
depuis le 1er avril d'être éditeur grâce à de laborieuses formations
Photoshop, InDesign et autres. (…) Décidément il y aura toujours des
rebondissements inattendus aux inventions, entre autres. Quand j'ai
commencé à utiliser l'internet [en 1999], je ne m'attendais vraiment
pas à devenir éditeur. » (Catherine Domain, fondatrice de la librairie
Ulysse en 1971)

# Juin 2011

«Je n'ai jamais aimé lire un livre sur un ordinateur ou sur un PDA.
Maintenant, avec l’arrivée de tablettes comme le Kindle et l’iPad, je
suis finalement devenu un lecteur de livres numériques. Je vois un
expansion énorme avec l'arrivée de tablettes faciles à utiliser et avec
un choix considérable de livres grâce au commerce électronique et à des
sociétés comme Amazon. (…) J'utilise également des livres en ligne pour
apprendre l'art de l'innovation!» (Henk Slettenhaar, fondateur de la
Silicon Valley Association suisse en 1992)

